lr >>> 0.0001230630205016966

dropout >>> 0.0

emsize >>> 128

batch_size >>> 1

nlayers >>> 12

num_features >>> 100

nhead >>> 4

nhid_factor >>> 2

bptt >>> 1152

eval_positions >>> [1094]

seq_len_used >>> 50

sampling >>> mnd

epochs >>> 1

num_steps >>> 1

verbose >>> False

mix_activations >>> True

pre_sample_causes >>> True

multiclass_type >>> variable_balance

nan_prob_unknown_reason_reason_prior >>> 0.5

categorical_feature_p >>> 0

nan_prob_no_reason >>> 0.0

nan_prob_unknown_reason >>> 0.0

nan_prob_a_reason >>> 0.0

max_num_classes >>> 2

num_classes >>> 2

noise_type >>> Gaussian

balanced >>> False

normalize_to_ranking >>> False

set_value_to_nan >>> 0.0

normalize_by_used_features >>> True

num_features_used >>> 100

num_categorical_features_sampler_a >>> -1.0

differentiable_hyperparameters >>> {'prior_bag_exp_weights_1': {'distribution': 'uniform', 'min': 2.0, 'max': 10.0}, 'num_layers': {'distribution': 'meta_gamma', 'max_alpha': 2, 'max_scale': 3, 'round': True, 'lower_bound': 2}, 'prior_mlp_hidden_dim': {'distribution': 'meta_gamma', 'max_alpha': 3, 'max_scale': 100, 'round': True, 'lower_bound': 4}, 'prior_mlp_dropout_prob': {'distribution': 'meta_beta', 'scale': 0.6, 'min': 0.1, 'max': 5.0}, 'noise_std': {'distribution': 'meta_trunc_norm_log_scaled', 'max_mean': 0.3, 'min_mean': 0.0001, 'round': False, 'lower_bound': 0.0}, 'init_std': {'distribution': 'meta_trunc_norm_log_scaled', 'max_mean': 10.0, 'min_mean': 0.01, 'round': False, 'lower_bound': 0.0}, 'num_causes': {'distribution': 'meta_gamma', 'max_alpha': 3, 'max_scale': 7, 'round': True, 'lower_bound': 2}, 'pre_sample_weights': {'distribution': 'meta_choice', 'choice_values': [True, False]}, 'y_is_effect': {'distribution': 'meta_choice', 'choice_values': [True, False]}, 'prior_mlp_activations': {'distribution': 'meta_choice_mixed', 'choice_values': [<class 'torch.nn.modules.activation.Tanh'>, <class 'torch.nn.modules.linear.Identity'>, <class 'torch.nn.modules.activation.ReLU'>]}, 'sort_features': {'distribution': 'meta_choice', 'choice_values': [True, False]}, 'in_clique': {'distribution': 'meta_choice', 'choice_values': [True, False]}, 'outputscale': {'distribution': 'meta_trunc_norm_log_scaled', 'max_mean': 10.0, 'min_mean': 1e-05, 'round': False, 'lower_bound': 0}, 'lengthscale': {'distribution': 'meta_trunc_norm_log_scaled', 'max_mean': 10.0, 'min_mean': 1e-05, 'round': False, 'lower_bound': 0}, 'noise': {'distribution': 'meta_choice', 'choice_values': [1e-05, 0.0001, 0.01]}}

prior_type >>> mlp

recompute_attn >>> True

differentiable >>> True

flexible >>> True

bptt_extra_samples >>> None

output_multiclass_ordered_p >>> 0.0

multiclass_loss_type >>> nono

normalize_with_sqrt >>> False

new_mlp_per_example >>> True

prior_mlp_scale_weights_sqrt >>> True

batch_size_per_gp_sample >>> None

normalize_ignore_label_too >>> False

differentiable_hps_as_style >>> False

max_eval_pos >>> 1000

random_feature_rotation >>> True

rotate_normalized_labels >>> True

n_layers >>> 4

canonical_y_encoder >>> True

aggregate_k_gradients >>> 1

total_available_time_in_s >>> None

train_mixed_precision >>> True

efficient_eval_masking >>> True

is_causal >>> False

num_causes >>> 5

prior_mlp_hidden_dim >>> 50

num_layers >>> 4

noise_std >>> 0.05

init_std >>> 0.05

y_is_effect >>> True

pre_sample_weights >>> True

prior_mlp_dropout_prob >>> 0

prior_mlp_activations >>> ReLU()

block_wise_dropout >>> True

sort_features >>> False

in_clique >>> False

max_features >>> 100

no_encoder >>> False

normalize >>> False

min_features >>> 100

n_samples >>> 1000

max_classes >>> 2

base_size >>> 1000

n_estimators >>> 1

min_depth >>> 5

max_depth >>> 15

categorical_x >>> False

data_sample_func >>> mnd

comp >>> True

microbiome_test >>> True

weight_classes >>> False

run_name >>> time

hist_targets >>> False

y_std >>> 1

align_majority >>> False

limit_imbalance >>> False

mlp_noise >>> True

epoch_frac >>> 0.2

