{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "806ff5d9-0f3a-4fa1-a089-db69fbe1c925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "from tabpfn_new.scripts.transformer_prediction_interface import TabPFNClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from data_prep_utils import *\n",
    "from evaluate import *\n",
    "from load_models import *\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import openml\n",
    "from data_prep_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tabpfn_new import priors, encoders\n",
    "from tabpfn_new.scripts.model_configs import *\n",
    "from tabpfn_new.scripts.model_builder import get_forest_prior_hyperparameters, get_mlp_prior_hyperparameters, get_model\n",
    "from tabpfn.utils import get_uniform_single_eval_pos_sampler\n",
    "from utils import get_dataloader\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99725f1b-38b4-4188-ba15-49b041341222",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reload_config(prior_type='forest', config_type='causal', task_type='binary', longer=0):\n",
    "    config = get_prior_config(config_type=config_type)\n",
    "    \n",
    "    config['prior_type'] = prior_type\n",
    "    \n",
    "    model_string = ''\n",
    "    \n",
    "    config['epochs'] = 12000\n",
    "    config['recompute_attn'] = True\n",
    "\n",
    "    config['max_num_classes'] = 10\n",
    "    config['num_classes'] = uniform_int_sampler_f(2, config['max_num_classes'])\n",
    "    config['balanced'] = False\n",
    "    model_string = model_string + '_multiclass'\n",
    "    \n",
    "    model_string = model_string + '_'+datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "    \n",
    "    return config, model_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ef61611-5810-4471-93f9-82b1f6958f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_from_config(config_sample, prior_hyperparameters):\n",
    "    def eval_pos_seq_len_sampler():\n",
    "        single_eval_pos_gen = get_uniform_single_eval_pos_sampler(config.get('max_eval_pos', config['bptt']), min_len=config.get('min_eval_pos', 0))\n",
    "        single_eval_pos = single_eval_pos_gen()\n",
    "        return single_eval_pos, 1024\n",
    "    dl_class, extra_kwargs = get_dataloader(config_sample, device=\"cpu\", should_train=False, verbose=0)\n",
    "    dl = dl_class(num_steps=1, batch_size=1, \n",
    "                                eval_pos_seq_len_sampler=eval_pos_seq_len_sampler, \n",
    "                                seq_len_maximum=None, device=\"cpu\", **extra_kwargs)\n",
    "    dl.model = None\n",
    "    return dl\n",
    "\n",
    "                                     \n",
    "def get_sample(dl):\n",
    "    (style, x, y), _, _ =  next(iter(dl))\n",
    "    return x, y\n",
    "    \n",
    "def plot_grid(dl, hyperparameters, rows=8, cols=6):\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(20,15))\n",
    "    class_assigner = priors.flexible_categorical.BalancedBinarize()#priors.flexible_categorical.MulticlassRank(hyperparameters['num_classes'], ordered_p=hyperparameters['output_multiclass_ordered_p'])\n",
    "    for ax in axes.flatten():\n",
    "        x2, b = get_sample(dl)\n",
    "        #b = class_assigner(b)\n",
    "        i1, i2 = np.random.choice(x2.shape[2], size=(2,), replace=False)\n",
    "        ax.scatter(x2[:, 0, i1], x2[:, 0, i2], c=b, s=1.5, cmap=\"bwr\")\n",
    "\n",
    "def remove_zero_features(x):\n",
    "    x = x[:,:,torch.where(torch.sum(torch.abs(x), dim=0)>0,1,0)]\n",
    "    return x\n",
    "\n",
    "def plot_feature_combinations(dl, hyperparameters, num_plots=10):\n",
    "    fig = plt.figure(figsize=(20,15))\n",
    "    x2, b = get_sample(dl)\n",
    "    #x2 = remove_zero_features(x2)\n",
    "    #class_assigner = priors.flexible_categorical.BalancedBinarize()#MulticlassRank(hyperparameters['num_classes'], ordered_p=hyperparameters['output_multiclass_ordered_p'])\n",
    "    #b = class_assigner(b)\n",
    "    n_feat = num_plots#hyperparameters[\"max_features\"]\n",
    "    for i in range(n_feat-1):\n",
    "        for j in range(n_feat):\n",
    "            if(j>i) :\n",
    "                ax = plt.subplot2grid((n_feat-1, n_feat-1), (i,j-1))\n",
    "                #ax.xaxis.set_ticklabels([]) \n",
    "                #ax.yaxis.set_ticklabels([])\n",
    "                plt.scatter(x2[:,0,i],x2[:,0,j],c=b, s=1.5, cmap=\"bwr\")\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f7f010d-ebbf-4cf7-ba58-8f98ecb7e01b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prior_type = 'mlp'\n",
    "config, model_string = reload_config(prior_type, longer=1)\n",
    "\n",
    "config['differentiable'] = True\n",
    "config['flexible'] = True\n",
    "config['bptt_extra_samples'] = None\n",
    "\n",
    "# diff\n",
    "config['output_multiclass_ordered_p'] = 0.0\n",
    "del config['differentiable_hyperparameters']['output_multiclass_ordered_p']\n",
    "\n",
    "\n",
    "\n",
    "config['sampling'] = 'normal' # vielleicht schlecht?\n",
    "del config['differentiable_hyperparameters']['sampling']\n",
    "\n",
    "config['pre_sample_causes'] = True\n",
    "# end diff\n",
    "\n",
    "config['multiclass_loss_type'] = 'nono' # 'compatible'\n",
    "config['normalize_to_ranking'] = False # False\n",
    "\n",
    "config['categorical_feature_p'] = 0 # diff: .0\n",
    "\n",
    "# turn this back on in a random search!?\n",
    "config['nan_prob_no_reason'] = .0\n",
    "config['nan_prob_unknown_reason'] = .0 # diff: .0\n",
    "config['set_value_to_nan'] = .0 # diff: 1.\n",
    "\n",
    "config['normalize_with_sqrt'] = False\n",
    "\n",
    "config['new_mlp_per_example'] = True\n",
    "config['prior_mlp_scale_weights_sqrt'] = True\n",
    "config['batch_size_per_gp_sample'] = None\n",
    "\n",
    "config['normalize_ignore_label_too'] = False\n",
    "\n",
    "config['differentiable_hps_as_style'] = False\n",
    "config['max_eval_pos'] = 1000\n",
    "\n",
    "config['random_feature_rotation'] = True\n",
    "config['rotate_normalized_labels'] = True\n",
    "\n",
    "config[\"mix_activations\"] = True # False heisst eig True\n",
    "\n",
    "config['n_layers'] = 4\n",
    "config['emsize'] = 128\n",
    "config['nhead'] = config['emsize'] // 32\n",
    "config['bptt'] = 1024+128\n",
    "config['canonical_y_encoder'] = True\n",
    "\n",
    "    \n",
    "config['aggregate_k_gradients'] = 1\n",
    "config['batch_size'] = 1*config['aggregate_k_gradients']\n",
    "config['num_steps'] = 1//config['aggregate_k_gradients']\n",
    "config['epochs'] = 1\n",
    "\n",
    "config['total_available_time_in_s'] = None #60*60*22 # 22 hours for some safety...\n",
    "\n",
    "config['train_mixed_precision'] = True\n",
    "config['efficient_eval_masking'] = True\n",
    "\n",
    "# mlp params\n",
    "config['is_causal'] = True\n",
    "config['num_causes'] = 5\n",
    "config['prior_mlp_hidden_dim'] = 50\n",
    "config['num_layers'] = 4\n",
    "config['noise_std'] = 0.05\n",
    "config['init_std'] = 0.05\n",
    "config['y_is_effect'] = True\n",
    "config['pre_sample_weights'] = True\n",
    "config['prior_mlp_dropout_prob'] = 0\n",
    "config['pre_sample_causes'] = True\n",
    "config[\"prior_mlp_activations\"] = torch.nn.ReLU()\n",
    "config[\"block_wise_dropout\"] = True\n",
    "config[\"sort_features\"] = False\n",
    "config[\"in_clique\"] = False\n",
    "\n",
    "# general data params\n",
    "config['balanced'] = True\n",
    "config['max_num_classes'] = 2\n",
    "config['max_features'] = 100\n",
    "config['num_features_used'] = 100\n",
    "\n",
    "config['num_classes'] = 2\n",
    "\n",
    "config['no_encoder'] = False\n",
    "config['normalize_to_ranking'] = False # False\n",
    "config['normalize_with_sqrt'] = False\n",
    "config['normalize_ignore_label_too'] = False\n",
    "config[\"normalize\"] = False\n",
    "config['num_classes'] = 2\n",
    "config[\"balanced\"] = False\n",
    "config['multiclass_type'] = 'imbalanced_binarize'\n",
    "\n",
    "\n",
    "config_sample = evaluate_hypers(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30030035-1632-47a4-9d32-a560361a8dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dl = dl_from_config(config_sample, prior_hyperparameters)\n",
    "model = get_model(config_sample, device, should_train=False, verbose=0)\n",
    "dl = model[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8825b69a-4d14-4e8f-95ba-1128f5486b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid(dl,config_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb563131-d329-4157-bb05-31fa6a6f0e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_combinations(dl,config_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b700a2b-c26b-44cc-8530-8d1f7650c97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "class MicroDL(DataLoader):\n",
    "    def __init__(self, path=None):\n",
    "        data, labels = get_microbiome(path)\n",
    "        data = top_non_zero(data)\n",
    "        data, labels = unison_shuffled_copies(data, labels)\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return 100\n",
    "        \n",
    "    def get_perm(self):\n",
    "        indices = np.random.permutation(self.data.shape[0])[:1000]\n",
    "        return (None, np.expand_dims(self.data[indices], axis=1), np.expand_dims(self.labels[indices],axis=1)), None, None\n",
    "        \n",
    "    def __iter__(self):\n",
    "        indices = np.random.permutation(self.data.shape[0])[:1000]\n",
    "        return iter(self.get_perm() for _ in range(100))\n",
    "path = \"datasets/data_all.csv\"\n",
    "dl = MicroDL(path)\n",
    "#plot_grid(dl, config_sample)\n",
    "plot_feature_combinations(dl, config_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47342e30-ae92-451b-824d-6db9a19ea2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['differentiable'] = True\n",
    "config['flexible'] = True\n",
    "\n",
    "# forest params\n",
    "config[\"min_features\"] = 100\n",
    "config[\"max_features\"] = 100\n",
    "config[\"n_samples\"] = 1000\n",
    "config[\"max_classes\"] = 2\n",
    "config[\"base_size\"] = 1000\n",
    "config[\"n_estimators\"] = 1\n",
    "config[\"min_depth\"] = 3\n",
    "config[\"max_depth\"] = 15\n",
    "config[\"categorical_x\"] = False\n",
    "config[\"data_sample_func\"] = \"zinb\"\n",
    "config[\"comp\"] = True\n",
    "\n",
    "config['multiclass_type'] = 'balance'\n",
    "del config['differentiable_hyperparameters']['multiclass_type']\n",
    "\n",
    "config[\"prior_type\"] = \"forest\"\n",
    "config[\"microbiome_test\"] = True\n",
    "config[\"weight_classes\"] = False\n",
    "config[\"run_name\"] = \"time\"\n",
    "\n",
    "config[\"prior_type\"] = \"forest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc90518-93eb-4479-980f-3ee7f749c365",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_sample = evaluate_hypers(config)\n",
    "model = get_model(config_sample, device, should_train=False, verbose=0)\n",
    "dl = model[3]\n",
    "plot_grid(dl, config_sample)\n",
    "plot_feature_combinations(dl, config_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540f17f5-7cc3-4fc7-a60c-cfb409cb3513",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"data_sample_func\"] = \"mnd\"\n",
    "\n",
    "\n",
    "config_sample = evaluate_hypers(config)\n",
    "model = get_model(config_sample, device, should_train=False, verbose=0)\n",
    "dl = model[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4e1075-d1c8-443b-85e6-05216d0367da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid(dl, config_sample)\n",
    "plot_feature_combinations(dl, config_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b2ca11-0225-4a0d-9c95-796d9eb3ee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabpfn import TabPFNClassifier\n",
    "model = TabPFNClassifier(device='cpu', N_ensemble_configurations=1)\n",
    "x, y = get_sample(dl)\n",
    "x, y = torch.squeeze(x), torch.squeeze(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.8, test_size=0.2, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "print(torch.tensor(preds)*y_test+(torch.tensor(preds)-1)*(y_test-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd56e91-d6c5-4163-bf92-a797bb3a8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['differentiable'] = True\n",
    "config['flexible'] = True\n",
    "config[\"prior_type\"] = \"forest\"\n",
    "config[\"sampling\"] = \"mnd\"\n",
    "#del config['differentiable_hyperparameters']['sampling']\n",
    "config[\"data_sample_func\"] = \"mnd\"\n",
    "config[\"is_causal\"] = False\n",
    "#del config['differentiable_hyperparameters']['is_causal']\n",
    "config['multiclass_type'] = 'balance'\n",
    "config['hist_targets'] = False\n",
    "\n",
    "config[\"align_majority\"] = False\n",
    "config[\"limit_imbalance\"] = False\n",
    "\n",
    "config[\"normalize\"] = True\n",
    "config[\"num_classes\"] = 2\n",
    "config[\"max_num_classes\"] = 2\n",
    "\n",
    "config['min_depth'] = 5\n",
    "config['max_depth'] = 15\n",
    "#del config['differentiable_hyperparameters']['multiclass_type']\n",
    "\n",
    "config_sample = evaluate_hypers(config)\n",
    "model = get_model(config_sample, device, should_train=False, verbose=0)\n",
    "dl = model[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae188bf-91f0-42c2-91b5-96714476a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print(f\"\\n\\n\\nRun #{i}\")\n",
    "    x, y = get_sample(dl)\n",
    "    x, y = torch.squeeze(x), torch.squeeze(y)\n",
    "    print(\"Target mean: \", torch.mean(y))\n",
    "    if len(np.unique(y, return_counts=True)[0])>1 and np.min(np.unique(y, return_counts=True)[1])>20:\n",
    "        #plt.hist(y, bins=100)\n",
    "        #plt.show()\n",
    "        cv = 3\n",
    "        n_optim = 20\n",
    "        ft_epochs = 0\n",
    "        sampling = None\n",
    "        strat_split=True\n",
    "        max_samples = 1000\n",
    "        metrics = metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\"]\n",
    "        models = [\n",
    "            #XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "            XGBoostOptim(n_optim=n_optim),\n",
    "            #LogisticRegression(max_iter=500), \n",
    "            #TabPFNClassifier(device='cpu', N_ensemble_configurations=3),\n",
    "            #TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\")\n",
    "        ]\n",
    "        results = pd.DataFrame(np.zeros((len(models), len(metrics))), \n",
    "                               index=[m.__class__.__name__ for m in models],\n",
    "                              columns=metrics)\n",
    "        \n",
    "        for ii, model in enumerate(models):\n",
    "            results.iloc[ii,:] = cross_validate_sample(model, x, y, metrics, strat_split, cv, sampling)\n",
    "        print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b15a5d3-46cc-439c-a324-577e5dda3ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 1.64 M parameters\n",
      "(1152, 0)\n",
      "(1152, 0)\n",
      "(1152, 1)\n",
      "(1152, 1)\n",
      "(1152, 0)\n",
      "(1152, 0)\n",
      "(1152, 0)\n",
      "(1152, 0)\n",
      "(1152, 0)\n",
      "(1152, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config['differentiable'] = True\n",
    "config['flexible'] = True\n",
    "config[\"prior_type\"] = \"mlp\"\n",
    "config[\"sampling\"] = \"mnd\"\n",
    "#del config['differentiable_hyperparameters']['sampling']\n",
    "config[\"data_sample_func\"] = \"mnd\"\n",
    "\n",
    "config['multiclass_type'] = 'balance'\n",
    "config[\"is_causal\"] = False\n",
    "if \"is_causal\" in config['differentiable_hyperparameters']:\n",
    "    del config['differentiable_hyperparameters']['is_causal']\n",
    "if 'multiclass_type' in config['differentiable_hyperparameters']:\n",
    "    del config['differentiable_hyperparameters']['multiclass_type']\n",
    "\n",
    "if \"block_wise_dropout\" in config['differentiable_hyperparameters']:\n",
    "    del config['differentiable_hyperparameters']['block_wise_dropout']\n",
    "config[\"block_wise_dropout\"] = True\n",
    "\n",
    "\n",
    "\n",
    "config['hist_targets'] = False\n",
    "config['y_std'] = 1\n",
    "config['min_depth'] = 5\n",
    "config['max_depth'] = 15\n",
    "config['num_classes'] = 2\n",
    "config[\"max_num_classes\"] = 2\n",
    "\n",
    "\n",
    "config[\"align_majority\"] = False\n",
    "config[\"limit_imbalance\"] = False\n",
    "\n",
    "\n",
    "config[\"mlp_noise\"] = True\n",
    "for noi in [0.3,0.001,0.01,0.1]:\n",
    "    \n",
    "    config['differentiable_hyperparameters']['noise_std'] = {'distribution': 'meta_trunc_norm_log_scaled', 'max_mean': noi, 'min_mean': 0.00001, 'round': False, 'lower_bound': 0.0}\n",
    "    \n",
    "    \n",
    "    #config[\"epoch_frac\"] = e/maxepo\n",
    "    config_sample = evaluate_hypers(config)\n",
    "    model = get_model(config_sample, device, should_train=False, verbose=0)\n",
    "    dl = model[3]\n",
    "    rocs = []\n",
    "    means = []\n",
    "    for i in range(100):\n",
    "        #print(f\"\\n\\n\\nRun #{i}\")\n",
    "        x, y = get_sample(dl)\n",
    "        x, y = torch.squeeze(x).numpy(), torch.squeeze(y).numpy()\n",
    "        print((x[:,np.sum(np.abs(x), axis=0)==0]).shape)\n",
    "        #print(np.unique(y, return_counts=True))\n",
    "        #if len(np.unique(y, return_counts=True)[0])<2: print(\"HERE FUZCKSNDKSHFKHSFG!!!!!!!!!###########################################################################################################################################\\n\\n\")\n",
    "        means.append(np.mean(y))\n",
    "        if len(np.unique(y, return_counts=True)[0])>1 and np.min(np.unique(y, return_counts=True)[1])>1:\n",
    "            #print(\"Target mean: \", torch.mean(y))\n",
    "            #plt.hist(y, bins=100)\n",
    "            #plt.show()\n",
    "            cv = 3\n",
    "            n_optim = 100\n",
    "            ft_epochs = 0\n",
    "            sampling = None\n",
    "            strat_split=True\n",
    "            max_samples = None\n",
    "            metrics = metrics = [\"roc_auc\"]#\"roc_auc_ovr\"]\n",
    "            if config['num_classes'] ==2:\n",
    "                o = 'binary:logistic'\n",
    "            else:\n",
    "                o = 'multi:softmax'\n",
    "            #model = XGBClassifier(n_estimators=10, max_depth=10, learning_rate=1, objective=o)#\n",
    "            #model = XGBoostOptim(n_optim=n_optim)\n",
    "            model = TabPFNClassifier(device='cpu', N_ensemble_configurations=3)\n",
    "            results = cross_validate_sample(model, x, y, metrics, strat_split, cv, sampling, max_samples)\n",
    "            #print(results[0])\n",
    "            rocs.append(results[0])\n",
    "            #print(\"ROC: \", results[0])\n",
    "    print(\"ROC: \", np.mean(np.array(rocs)), np.std(np.array(rocs)))\n",
    "    print(\"Pred avgs: \", np.mean(np.array(means)))\n",
    "    #result_matrix[ii,jj] = np.mean(np.array(rocs))\n",
    "    plt.hist(rocs, bins=20)\n",
    "    plt.show()\n",
    "    bins = np.arange(0,102,3)*1e-2\n",
    "    plt.hist(means, bins=bins)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3668474-a2cf-403d-8bb8-4d76ba6063d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "means = []\n",
    "for i in range(1000):\n",
    "    #print(f\"\\n\\n\\nRun #{i}\")\n",
    "    x, y = get_sample(dl)\n",
    "    #print(x[0])\n",
    "    #print(np.unique(y, return_counts=True))\n",
    "    x, y = torch.squeeze(x), torch.squeeze(y)\n",
    "    preds = torch.full(y.shape, torch.argmax(torch.unique(y, return_counts=True)[1]).item())\n",
    "    accuracy = torch.mean((preds==y)[y!=-100].float())\n",
    "    means.append(max(1-np.mean(y),np.mean(y)))\n",
    "    accs.append(accuracy)\n",
    "print(torch.mean(torch.tensor(accs)))\n",
    "print(torch.mean(torch.tensor(means)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb27a55-0322-4121-a7ba-f3380dfb2a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50,100,1):\n",
    "    print(f\"ROC sub {i*0.01} pred\",  np.mean(np.array(rocs)[np.array(means)<i*0.01]), len(np.array(rocs)[np.array(means)<i*0.01])/len(means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c363a4-132d-4c4b-9fd2-0f99eb63f4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8a0922-d04c-43e6-a2d5-fd3745858bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(means,rocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff1fee4-f743-4630-b62c-4a02b0880158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358a4488-dda9-43b0-890f-ab7ccd92a306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomial_dirichlet(size=(1000,100)):\n",
    "    M = 1000\n",
    "    alphas = np.random.beta(1,1,size[1])\n",
    "    thetas = [np.random.dirichlet(alphas) for i in range(size[0])]\n",
    "    #print(thetas, np.sum(thetas))\n",
    "    X = np.asarray([np.random.multinomial(M, theta)/M for theta in thetas])\n",
    "    return X\n",
    "X = multinomial_dirichlet()\n",
    "print(X[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc87ccf7-5976-483f-9eb4-41036e9d97c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f721f19-7023-4436-b374-886651722f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def get_sample(config_sample, prior_hyperparameters):\n",
    "    def eval_pos_seq_len_sampler():\n",
    "        single_eval_pos_gen = get_uniform_single_eval_pos_sampler(config.get('max_eval_pos', config['bptt']), min_len=config.get('min_eval_pos', 0))\n",
    "        single_eval_pos = single_eval_pos_gen()\n",
    "        return single_eval_pos, 1024\n",
    "    ''extra_prior_kwargs_dict={\n",
    "        'num_features': config['num_features'], \n",
    "        'hyperparameters': prior_hyperparameters\n",
    "    }\n",
    "    extra_kwargs = {}\n",
    "    if 'flexible' in config and config['flexible']:\n",
    "        get_batch_base = make_get_batch(model_proto)\n",
    "        extra_kwargs['get_batch'] = get_batch_base\n",
    "        model_proto = priors.flexible_categorical\n",
    "    if 'differentiable' in prior_hyperparameters and prior_hyperparameters['differentiable']:\n",
    "        get_batch_base = make_get_batch(model_proto, **extra_kwargs)\n",
    "        extra_kwargs = {'get_batch': get_batch_base, 'differentiable_hyperparameters': prior_hyperparameters['differentiable_hyperparameters']}\n",
    "        model_proto = priors.differentiable_prior\n",
    "        use_style = True\n",
    "    \n",
    "    dl = model_proto.DataLoader(num_steps=1, batch_size=1, \n",
    "                                eval_pos_seq_len_sampler=eval_pos_seq_len_sampler, \n",
    "                                seq_len_maximum=None, device=\"cpu\", **extra_prior_kwargs_dict)''\n",
    "    dl_class, extra_kwargs = get_dataloader(config_sample, device=\"cpu\", should_train=False, verbose=2)\n",
    "    dl = model_proto.DataLoader(num_steps=1, batch_size=1, \n",
    "                                eval_pos_seq_len_sampler=eval_pos_seq_len_sampler, \n",
    "                                seq_len_maximum=None, device=\"cpu\", **extra_kwargs)\n",
    "    dl.model = None\n",
    "    return next(iter(dl))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a955c12a-56e9-4895-a9af-07226073cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_sample(config_sample, prior_hyperparameters)\n",
    "print(a[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c161bd-ec80-43ac-b3a9-21011eec1d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['differentiable'] = False\n",
    "config['flexible'] = False\n",
    "\n",
    "# forest params\n",
    "config[\"min_features\"] = 10\n",
    "config[\"max_features\"] = 100\n",
    "config[\"n_samples\"] = 1000\n",
    "config[\"max_classes\"] = 10\n",
    "config[\"base_size\"] = 1000\n",
    "config[\"n_estimators\"] = 1\n",
    "config[\"min_depth\"] = 1\n",
    "config[\"max_depth\"] = 25\n",
    "config[\"categorical_x\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af348aba-7927-4722-829b-2b9067bec073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "from tabpfn import TabPFNClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from evaluate import scores\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, precision_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from data_prep_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import openml\n",
    "from tabularbench.data.synthetic_generator_forest import synthetic_dataset_generator_forest, synthetic_dataset_function_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5561d98a-122c-46a3-b816-198a5c986552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabularbench.data.synthetic_generator_tabpfn import synthetic_dataset_generator_tabpfn\n",
    "\n",
    "fig, axes = plt.subplots(8, 6, figsize=(20, 15))\n",
    "generator = synthetic_dataset_generator_tabpfn(n_samples=1000, max_classes=2, min_features=2, max_features=100)\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    x2, b = next(generator)\n",
    "    i1, i2 = np.random.choice(x2.shape[1], size=(2,), replace=False)\n",
    "    ax.scatter(x2[:, i1], x2[:, i2], c=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2a893a-7167-4aa4-a2df-513182935798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabularbench.data.synthetic_generator_forest import synthetic_dataset_generator_forest, synthetic_dataset_function_forest\n",
    "import matplotlib\n",
    "\n",
    "fig, axes = plt.subplots(8, 6, figsize=(20, 15))\n",
    "generator = synthetic_dataset_generator_forest(n_samples=1000, max_classes=2, min_features=2, max_features=2)\n",
    "\n",
    "cmap = matplotlib.colormaps['inferno']\n",
    "colors = cmap(np.linspace(0.0, 0.9, 10))\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    x2, b = synthetic_dataset_function_forest(base_size=1000, min_features=2, max_features=2, max_classes=3, min_depth=1, max_depth=25, categorical_x=True, n_samples=1024)\n",
    "    i1, i2 = np.random.choice(x2.shape[1], size=(2,), replace=False)\n",
    "    random_shuffle = np.random.permutation(np.arange(10))\n",
    "    colors_shuffle = colors[random_shuffle]\n",
    "    #b = np.array([colors_shuffle[i] for i in b])\n",
    "    ax.scatter(x2[:, i1], x2[:, i2], c=b)\n",
    "    ax.tick_params(axis='both', which='both', bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be43c36-ca72-4cb4-9cd4-72a59b27d90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zinb(size=(1000,100)):\n",
    "    pi = 0.25\n",
    "    p = np.random.uniform(0.1,0.9, size=size[1])\n",
    "    p = np.repeat(np.expand_dims(p,axis=0),size[0],axis=0)\n",
    "    #print\n",
    "    #p += 1e-5\n",
    "    X = np.random.negative_binomial(100,p)\n",
    "    X = np.random.binomial(1,1-pi,size)*X\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08769738-e744-49cc-98c2-206e3bb9d5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zinb_comp(size=(1000,100)):\n",
    "    X =  zinb(size)\n",
    "    print(X)\n",
    "    print(np.expand_dims(1/np.sum(X,axis=1),axis=1))\n",
    "    return np.expand_dims(1/np.sum(X,axis=1),axis=1)*X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada8a9e9-6a99-4d92-b869-ae69bdc53cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = zinb_comp((10,10))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ed9025-1fb0-4732-b3b7-ee311b528e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(out, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c62c558-446d-42bf-a2a3-39ed3b4a9c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prior.synthetic_generator_forest_altdata import synthetic_dataset_generator_forest_altdata, synthetic_dataset_function_forest_altdata\n",
    "\n",
    "fig, axes = plt.subplots(8, 6, figsize=(20, 15))\n",
    "\n",
    "cmap = matplotlib.colormaps['inferno']\n",
    "colors = cmap(np.linspace(0.0, 0.9, 10))\n",
    "\n",
    "\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    x2, b = synthetic_dataset_function_forest_altdata(zinb,base_size=1000, min_features=2, max_features=2, max_classes=3, min_depth=1, max_depth=25, categorical_x=True, n_samples=1024)\n",
    "    i1, i2 = np.random.choice(x2.shape[1], size=(2,), replace=False)\n",
    "    random_shuffle = np.random.permutation(np.arange(10))\n",
    "    colors_shuffle = colors[random_shuffle]\n",
    "    #b = np.array([colors_shuffle[i] for i in b])\n",
    "    ax.scatter(x2[:, i1], x2[:, i2], c=b)\n",
    "    ax.tick_params(axis='both', which='both', bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b35c21-16e3-44e8-80df-dd5de8fc536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_norm, b_norm = synthetic_dataset_function_forest(base_size=1000, min_features=2, max_features=2, max_classes=3, min_depth=1, max_depth=25, categorical_x=True, n_samples=1024)\n",
    "\n",
    "X_zinb, b_zinb = synthetic_dataset_function_forest_altdata(zinb,base_size=1000, min_features=2, max_features=2, max_classes=3, min_depth=1, max_depth=25, categorical_x=True, n_samples=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68200361-d086-41a0-8af3-ad83c009298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, roc_auc_score\n",
    "accuracy = np.zeros((3))\n",
    "runs = 10\n",
    "for ii in range(runs):\n",
    "    X_norm, b_norm = synthetic_dataset_function_forest(base_size=1000, min_features=2, max_features=2, max_classes=3, min_depth=1, max_depth=25, categorical_x=True, n_samples=1024)\n",
    "    models = [XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'), \n",
    "              LogisticRegression(max_iter=500), \n",
    "              TabPFNClassifier(device='cpu', N_ensemble_configurations=3)]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_norm, b_norm, test_size=0.2, random_state=42)\n",
    "    for mm, model in enumerate(models):\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy[mm] += accuracy_score(y_test,y_pred)\n",
    "print(accuracy/runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248ed978-b1b6-46ee-9233-e91434632ae2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, roc_auc_score\n",
    "accuracy = np.zeros((3))\n",
    "runs = 10\n",
    "for ii in range(runs):\n",
    "    print(ii)\n",
    "    X_zinb, b_zinb = synthetic_dataset_function_forest_altdata(zinb,base_size=1000, min_features=2, max_features=2, max_classes=3, min_depth=1, max_depth=25, categorical_x=True, n_samples=1024)\n",
    "    models = [XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'), \n",
    "              LogisticRegression(max_iter=500), \n",
    "              TabPFNClassifier(device='cpu', N_ensemble_configurations=3)]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_zinb, b_zinb, test_size=0.2, random_state=42)\n",
    "    for mm, model in enumerate(models):\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy[mm] += accuracy_score(y_test,y_pred)\n",
    "print(accuracy/runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa159333-33a1-4941-9ea3-31ab11dfcca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e818c922-3e6d-4443-884a-9fb01aa1db39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
