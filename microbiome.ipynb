{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "716a0985-a3e5-47be-acc6-d58fb95fa8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "from tabpfn import TabPFNClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from data_prep_utils import *\n",
    "from evaluate import *\n",
    "from load_models import *\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fd72aea-dcf4-4d2c-bc6b-63eb9ccfdd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/data_all.csv\"\n",
    "data, labels = get_microbiome(path)\n",
    "data = top_non_zero(data)\n",
    "data, labels = unison_shuffled_copies(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "879ba0a8-b675-49eb-a1dc-64e836de555b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-09-12 22:08:50.392\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1775 | Val score: 0.9384\u001b[0m\n",
      "\u001b[32m2024-09-12 22:10:08.251\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1742 | Train score: 0.9375 | Val loss: 0.1706 | Val score: 0.9422\u001b[0m\n",
      "\u001b[32m2024-09-12 22:11:17.136\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1857 | Train score: 0.9414 | Val loss: 0.1657 | Val score: 0.9438\u001b[0m\n",
      "\u001b[32m2024-09-12 22:12:15.666\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1614 | Train score: 0.9473 | Val loss: 0.1742 | Val score: 0.9400\u001b[0m\n",
      "\u001b[32m2024-09-12 22:13:19.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1317 | Train score: 0.9512 | Val loss: 0.1728 | Val score: 0.9427\u001b[0m\n",
      "\u001b[32m2024-09-12 22:14:23.018\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1675 | Train score: 0.9414 | Val loss: 0.1765 | Val score: 0.9433\u001b[0m\n",
      "\u001b[32m2024-09-12 22:15:28.004\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1437 | Train score: 0.9512 | Val loss: 0.1641 | Val score: 0.9449\u001b[0m\n",
      "\u001b[32m2024-09-12 22:16:28.149\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1936 | Train score: 0.9375 | Val loss: 0.1665 | Val score: 0.9427\u001b[0m\n",
      "\u001b[32m2024-09-12 22:17:31.087\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1297 | Train score: 0.9590 | Val loss: 0.1705 | Val score: 0.9411\u001b[0m\n",
      "\u001b[32m2024-09-12 22:18:28.019\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1758 | Train score: 0.9492 | Val loss: 0.1644 | Val score: 0.9460\u001b[0m\n",
      "\u001b[32m2024-09-12 22:19:29.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1772 | Train score: 0.9316 | Val loss: 0.1683 | Val score: 0.9427\u001b[0m\n",
      "\u001b[32m2024-09-12 22:20:49.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1711 | Val score: 0.9417\u001b[0m\n",
      "\u001b[32m2024-09-12 22:21:46.985\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1706 | Train score: 0.9434 | Val loss: 0.1655 | Val score: 0.9471\u001b[0m\n",
      "\u001b[32m2024-09-12 22:22:57.500\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1307 | Train score: 0.9570 | Val loss: 0.1612 | Val score: 0.9460\u001b[0m\n",
      "\u001b[32m2024-09-12 22:24:05.206\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1726 | Train score: 0.9551 | Val loss: 0.1622 | Val score: 0.9460\u001b[0m\n",
      "\u001b[32m2024-09-12 22:25:03.441\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1554 | Train score: 0.9473 | Val loss: 0.1642 | Val score: 0.9466\u001b[0m\n",
      "\u001b[32m2024-09-12 22:25:59.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1919 | Train score: 0.9414 | Val loss: 0.1650 | Val score: 0.9455\u001b[0m\n",
      "\u001b[32m2024-09-12 22:26:52.450\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1464 | Train score: 0.9512 | Val loss: 0.1567 | Val score: 0.9493\u001b[0m\n",
      "\u001b[32m2024-09-12 22:27:47.045\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2087 | Train score: 0.9258 | Val loss: 0.1638 | Val score: 0.9417\u001b[0m\n",
      "\u001b[32m2024-09-12 22:28:43.995\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1686 | Train score: 0.9395 | Val loss: 0.1643 | Val score: 0.9466\u001b[0m\n",
      "\u001b[32m2024-09-12 22:29:40.797\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1609 | Train score: 0.9453 | Val loss: 0.1635 | Val score: 0.9477\u001b[0m\n",
      "\u001b[32m2024-09-12 22:30:37.291\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1771 | Train score: 0.9395 | Val loss: 0.1581 | Val score: 0.9466\u001b[0m\n",
      "\u001b[32m2024-09-12 22:31:53.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1705 | Val score: 0.9438\u001b[0m\n",
      "\u001b[32m2024-09-12 22:32:46.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1783 | Train score: 0.9434 | Val loss: 0.1635 | Val score: 0.9460\u001b[0m\n",
      "\u001b[32m2024-09-12 22:33:43.554\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2056 | Train score: 0.9375 | Val loss: 0.1617 | Val score: 0.9482\u001b[0m\n",
      "\u001b[32m2024-09-12 22:34:43.124\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1707 | Train score: 0.9453 | Val loss: 0.1638 | Val score: 0.9455\u001b[0m\n",
      "\u001b[32m2024-09-12 22:35:56.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1740 | Train score: 0.9434 | Val loss: 0.1603 | Val score: 0.9438\u001b[0m\n",
      "\u001b[32m2024-09-12 22:36:51.980\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1784 | Train score: 0.9434 | Val loss: 0.1603 | Val score: 0.9471\u001b[0m\n",
      "\u001b[32m2024-09-12 22:37:44.312\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1631 | Train score: 0.9414 | Val loss: 0.1564 | Val score: 0.9487\u001b[0m\n",
      "\u001b[32m2024-09-12 22:38:35.860\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1448 | Train score: 0.9512 | Val loss: 0.1658 | Val score: 0.9449\u001b[0m\n",
      "\u001b[32m2024-09-12 22:39:26.146\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1132 | Train score: 0.9688 | Val loss: 0.1605 | Val score: 0.9487\u001b[0m\n",
      "\u001b[32m2024-09-12 22:40:14.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1722 | Train score: 0.9375 | Val loss: 0.1609 | Val score: 0.9477\u001b[0m\n",
      "\u001b[32m2024-09-12 22:41:06.228\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1510 | Train score: 0.9512 | Val loss: 0.1544 | Val score: 0.9477\u001b[0m\n",
      "\u001b[32m2024-09-12 22:42:14.351\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1835 | Val score: 0.9389\u001b[0m\n",
      "\u001b[32m2024-09-12 22:43:04.182\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2045 | Train score: 0.9277 | Val loss: 0.1762 | Val score: 0.9411\u001b[0m\n",
      "\u001b[32m2024-09-12 22:43:52.257\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1760 | Train score: 0.9453 | Val loss: 0.1763 | Val score: 0.9406\u001b[0m\n",
      "\u001b[32m2024-09-12 22:44:40.106\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1586 | Train score: 0.9434 | Val loss: 0.2048 | Val score: 0.9411\u001b[0m\n",
      "\u001b[32m2024-09-12 22:45:30.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1897 | Train score: 0.9395 | Val loss: 0.1770 | Val score: 0.9438\u001b[0m\n",
      "\u001b[32m2024-09-12 22:46:17.973\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2001 | Train score: 0.9434 | Val loss: 0.1729 | Val score: 0.9466\u001b[0m\n",
      "\u001b[32m2024-09-12 22:47:04.293\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1428 | Train score: 0.9551 | Val loss: 0.1787 | Val score: 0.9417\u001b[0m\n",
      "\u001b[32m2024-09-12 22:47:52.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2004 | Train score: 0.9297 | Val loss: 0.1778 | Val score: 0.9400\u001b[0m\n",
      "\u001b[32m2024-09-12 22:48:40.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1750 | Train score: 0.9395 | Val loss: 0.1776 | Val score: 0.9433\u001b[0m\n",
      "\u001b[32m2024-09-12 22:49:27.671\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1778 | Train score: 0.9355 | Val loss: 0.1747 | Val score: 0.9438\u001b[0m\n",
      "\u001b[32m2024-09-12 22:50:18.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1968 | Train score: 0.9297 | Val loss: 0.1733 | Val score: 0.9417\u001b[0m\n",
      "\u001b[32m2024-09-12 22:51:25.186\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1754 | Val score: 0.9395\u001b[0m\n",
      "\u001b[32m2024-09-12 22:52:11.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1957 | Train score: 0.9375 | Val loss: 0.1718 | Val score: 0.9433\u001b[0m\n",
      "\u001b[32m2024-09-12 22:52:58.983\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1805 | Train score: 0.9414 | Val loss: 0.1668 | Val score: 0.9422\u001b[0m\n",
      "\u001b[32m2024-09-12 22:53:47.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1789 | Train score: 0.9414 | Val loss: 0.1651 | Val score: 0.9411\u001b[0m\n",
      "\u001b[32m2024-09-12 22:54:34.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1867 | Train score: 0.9414 | Val loss: 0.1720 | Val score: 0.9400\u001b[0m\n",
      "\u001b[32m2024-09-12 22:55:21.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1949 | Train score: 0.9316 | Val loss: 0.1662 | Val score: 0.9411\u001b[0m\n",
      "\u001b[32m2024-09-12 22:56:09.945\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1559 | Train score: 0.9551 | Val loss: 0.1716 | Val score: 0.9411\u001b[0m\n",
      "\u001b[32m2024-09-12 22:56:55.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1795 | Train score: 0.9395 | Val loss: 0.1646 | Val score: 0.9422\u001b[0m\n",
      "\u001b[32m2024-09-12 22:57:42.489\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1836 | Train score: 0.9414 | Val loss: 0.1669 | Val score: 0.9411\u001b[0m\n",
      "\u001b[32m2024-09-12 22:58:28.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1910 | Train score: 0.9414 | Val loss: 0.1662 | Val score: 0.9427\u001b[0m\n",
      "\u001b[32m2024-09-12 22:59:14.302\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1494 | Train score: 0.9531 | Val loss: 0.1637 | Val score: 0.9417\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "cv = 5\n",
    "n_optim = 1000\n",
    "ft_epochs = 10\n",
    "sampling = None\n",
    "metrics = metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\"]\n",
    "models = [\n",
    "    XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "    XGBoostOptim(n_optim=n_optim),\n",
    "    LogisticRegression(max_iter=500), \n",
    "    TabPFNClassifier(device='cpu', N_ensemble_configurations=3),\n",
    "    TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\")\n",
    "]\n",
    "results = pd.DataFrame(np.zeros((len(models), len(metrics))), \n",
    "                       index=[m.__class__.__name__ for m in models],\n",
    "                      columns=metrics)\n",
    "\n",
    "for ii, model in enumerate(models):\n",
    "    results.iloc[ii,:] = cross_validate_sample(model, data, labels, metrics, cv, sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5983e6a1-a7ce-4694-b775-e83498cd3be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        accuracy  precision    recall   roc_auc\n",
      "XGBClassifier           0.930984   0.366046  0.211677  0.594237\n",
      "XGBoostOptim            0.949569   0.753080  0.239671  0.617281\n",
      "LogisticRegression      0.937877   0.378647  0.057392  0.525727\n",
      "TabPFNClassifier        0.940669   0.660000  0.017440  0.508488\n",
      "TabForestPFNClassifier  0.944334   0.632727  0.180309  0.586719\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
