{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "716a0985-a3e5-47be-acc6-d58fb95fa8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from tabpfn_new.scripts.transformer_prediction_interface import TabPFNClassifier, MedPFNClassifier\n",
    "from tabpfn_new.scripts.model_builder import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from data_prep_utils import *\n",
    "from evaluate import *\n",
    "from load_models import *\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import openml\n",
    "import time\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0e72742-f9d3-4748-939e-0aee908e04c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/data_all.csv\"\n",
    "all_data, labels = get_microbiome(path)\n",
    "all_data = remove_zero_features(all_data)\n",
    "all_data, labels = unison_shuffled_copies(all_data, labels, seed=42)\n",
    "#all_data = top_anova(all_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "adae1ec9-3ead-4f70-8f11-8936c9783725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.1.1`. \n",
      "\tWarning: Exception caused XGBoost to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.1.1`.\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.1.1`. \n",
      "\tWarning: Exception caused XGBoost to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.1.1`.\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.1.1`. \n",
      "\tWarning: Exception caused XGBoost to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.1.1`.\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.1.1`. \n",
      "\tWarning: Exception caused XGBoost to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.1.1`.\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.1.1`. \n",
      "\tWarning: Exception caused XGBoost to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.1.1`.\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.1.1`. \n",
      "\tWarning: Exception caused XGBoost to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.1.1`.\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.1.1`. \n",
      "\tWarning: Exception caused XGBoost to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.1.1`.\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.1.1`. \n",
      "\tWarning: Exception caused XGBoost to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.1.1`.\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.1.1`. \n",
      "\tWarning: Exception caused XGBoost to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.1.1`.\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.1.1`. \n",
      "\tWarning: Exception caused XGBoost to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.1.1`.\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001b[32m2024-10-29 23:32:54.003\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1366 | Val score: 0.9554\u001b[0m\n",
      "\u001b[32m2024-10-29 23:32:55.539\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.0863 | Train score: 0.9691 | Val loss: 0.1530 | Val score: 0.9554\u001b[0m\n",
      "\u001b[32m2024-10-29 23:32:57.163\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1955 | Train score: 0.9506 | Val loss: 0.1412 | Val score: 0.9554\u001b[0m\n",
      "\u001b[32m2024-10-29 23:32:58.713\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1147 | Train score: 0.9630 | Val loss: 0.1352 | Val score: 0.9554\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:00.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.0821 | Train score: 0.9691 | Val loss: 0.1330 | Val score: 0.9554\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:02.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1058 | Train score: 0.9630 | Val loss: 0.1323 | Val score: 0.9554\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:03.806\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1623 | Train score: 0.9444 | Val loss: 0.1315 | Val score: 0.9604\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:05.573\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.0636 | Train score: 0.9815 | Val loss: 0.1313 | Val score: 0.9554\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:07.186\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1103 | Train score: 0.9630 | Val loss: 0.1305 | Val score: 0.9554\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:08.706\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1085 | Train score: 0.9691 | Val loss: 0.1301 | Val score: 0.9554\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:10.406\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.0827 | Train score: 0.9691 | Val loss: 0.1296 | Val score: 0.9604\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:12.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1187 | Val score: 0.9505\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:13.974\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1287 | Train score: 0.9444 | Val loss: 0.1100 | Val score: 0.9653\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:15.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1421 | Train score: 0.9506 | Val loss: 0.1101 | Val score: 0.9604\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:17.368\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1286 | Train score: 0.9506 | Val loss: 0.1092 | Val score: 0.9653\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:18.947\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1440 | Train score: 0.9506 | Val loss: 0.1098 | Val score: 0.9604\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:20.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1324 | Train score: 0.9568 | Val loss: 0.1100 | Val score: 0.9604\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:22.392\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1201 | Train score: 0.9753 | Val loss: 0.1096 | Val score: 0.9604\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:23.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.0891 | Train score: 0.9753 | Val loss: 0.1060 | Val score: 0.9653\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:25.569\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1005 | Train score: 0.9691 | Val loss: 0.1028 | Val score: 0.9604\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:27.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.0792 | Train score: 0.9691 | Val loss: 0.1014 | Val score: 0.9604\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:28.923\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1147 | Train score: 0.9691 | Val loss: 0.1006 | Val score: 0.9604\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:30.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1163 | Val score: 0.9653\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:32.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1131 | Train score: 0.9506 | Val loss: 0.1230 | Val score: 0.9653\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:34.188\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.0753 | Train score: 0.9630 | Val loss: 0.1435 | Val score: 0.9653\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:35.801\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.0795 | Train score: 0.9630 | Val loss: 0.1479 | Val score: 0.9653\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:37.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.0971 | Train score: 0.9753 | Val loss: 0.1309 | Val score: 0.9653\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:38.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.0580 | Train score: 0.9753 | Val loss: 0.1302 | Val score: 0.9703\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:40.760\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2066 | Train score: 0.9506 | Val loss: 0.1172 | Val score: 0.9703\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:42.450\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1638 | Train score: 0.9630 | Val loss: 0.1095 | Val score: 0.9703\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:44.105\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.0940 | Train score: 0.9753 | Val loss: 0.1070 | Val score: 0.9703\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:45.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.0983 | Train score: 0.9568 | Val loss: 0.1064 | Val score: 0.9703\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:47.496\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.0952 | Train score: 0.9691 | Val loss: 0.1071 | Val score: 0.9703\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:49.187\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1421 | Val score: 0.9307\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:50.955\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1225 | Train score: 0.9630 | Val loss: 0.1495 | Val score: 0.9406\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:52.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.0973 | Train score: 0.9630 | Val loss: 0.1622 | Val score: 0.9455\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:54.475\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1281 | Train score: 0.9630 | Val loss: 0.1591 | Val score: 0.9455\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:56.440\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2351 | Train score: 0.9506 | Val loss: 0.1506 | Val score: 0.9455\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:58.234\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1029 | Train score: 0.9630 | Val loss: 0.1460 | Val score: 0.9455\u001b[0m\n",
      "\u001b[32m2024-10-29 23:33:59.894\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1244 | Train score: 0.9321 | Val loss: 0.1447 | Val score: 0.9455\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:01.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1164 | Train score: 0.9506 | Val loss: 0.1443 | Val score: 0.9455\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:03.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.0986 | Train score: 0.9630 | Val loss: 0.1458 | Val score: 0.9455\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:05.093\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1172 | Train score: 0.9568 | Val loss: 0.1489 | Val score: 0.9356\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:06.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.0990 | Train score: 0.9753 | Val loss: 0.1523 | Val score: 0.9356\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:08.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.0968 | Val score: 0.9604\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:10.197\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1142 | Train score: 0.9506 | Val loss: 0.0946 | Val score: 0.9653\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:11.720\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1111 | Train score: 0.9506 | Val loss: 0.0987 | Val score: 0.9703\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:13.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1849 | Train score: 0.9568 | Val loss: 0.0934 | Val score: 0.9653\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:15.226\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1022 | Train score: 0.9630 | Val loss: 0.0919 | Val score: 0.9752\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:16.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1169 | Train score: 0.9259 | Val loss: 0.0902 | Val score: 0.9752\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:18.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1557 | Train score: 0.9383 | Val loss: 0.0902 | Val score: 0.9653\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:19.806\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1589 | Train score: 0.9506 | Val loss: 0.0911 | Val score: 0.9703\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:21.307\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1473 | Train score: 0.9444 | Val loss: 0.0940 | Val score: 0.9653\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:22.829\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1091 | Train score: 0.9691 | Val loss: 0.0928 | Val score: 0.9653\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:24.345\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1246 | Train score: 0.9691 | Val loss: 0.0926 | Val score: 0.9653\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:25.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1408 | Val score: 0.9455\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:27.400\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1245 | Train score: 0.9444 | Val loss: 0.1330 | Val score: 0.9604\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:28.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1928 | Train score: 0.9568 | Val loss: 0.1308 | Val score: 0.9604\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:30.419\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1148 | Train score: 0.9691 | Val loss: 0.1320 | Val score: 0.9604\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:31.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1271 | Train score: 0.9568 | Val loss: 0.1315 | Val score: 0.9554\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:33.465\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1406 | Train score: 0.9506 | Val loss: 0.1299 | Val score: 0.9604\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:34.945\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1068 | Train score: 0.9753 | Val loss: 0.1284 | Val score: 0.9653\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:36.455\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.0829 | Train score: 0.9753 | Val loss: 0.1283 | Val score: 0.9653\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:37.901\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1097 | Train score: 0.9630 | Val loss: 0.1314 | Val score: 0.9653\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:39.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.0833 | Train score: 0.9691 | Val loss: 0.1422 | Val score: 0.9653\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:40.929\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1248 | Train score: 0.9691 | Val loss: 0.1329 | Val score: 0.9505\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:42.473\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1116 | Val score: 0.9505\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:44.105\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1142 | Train score: 0.9568 | Val loss: 0.1080 | Val score: 0.9554\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:45.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1418 | Train score: 0.9568 | Val loss: 0.1036 | Val score: 0.9554\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:47.595\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.0926 | Train score: 0.9568 | Val loss: 0.1030 | Val score: 0.9653\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:49.438\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1243 | Train score: 0.9630 | Val loss: 0.1016 | Val score: 0.9703\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:51.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1378 | Train score: 0.9568 | Val loss: 0.1016 | Val score: 0.9752\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:54.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1098 | Train score: 0.9630 | Val loss: 0.1024 | Val score: 0.9752\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:56.215\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1189 | Train score: 0.9691 | Val loss: 0.1029 | Val score: 0.9752\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:57.992\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.0721 | Train score: 0.9691 | Val loss: 0.1037 | Val score: 0.9752\u001b[0m\n",
      "\u001b[32m2024-10-29 23:34:59.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1364 | Train score: 0.9506 | Val loss: 0.1052 | Val score: 0.9703\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:01.610\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.0781 | Train score: 0.9753 | Val loss: 0.1081 | Val score: 0.9653\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:03.228\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1412 | Val score: 0.9307\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:04.743\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1524 | Train score: 0.9444 | Val loss: 0.1308 | Val score: 0.9406\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:06.299\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1051 | Train score: 0.9568 | Val loss: 0.1418 | Val score: 0.9406\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:08.304\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.0986 | Train score: 0.9691 | Val loss: 0.1532 | Val score: 0.9406\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:10.129\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.0845 | Train score: 0.9691 | Val loss: 0.1603 | Val score: 0.9406\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:12.007\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.0946 | Train score: 0.9753 | Val loss: 0.1538 | Val score: 0.9406\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:13.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1390 | Train score: 0.9630 | Val loss: 0.1408 | Val score: 0.9455\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:15.492\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1638 | Train score: 0.9630 | Val loss: 0.1326 | Val score: 0.9455\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:17.448\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.0928 | Train score: 0.9815 | Val loss: 0.1325 | Val score: 0.9455\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:19.228\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.0921 | Train score: 0.9753 | Val loss: 0.1337 | Val score: 0.9505\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:20.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1001 | Train score: 0.9691 | Val loss: 0.1338 | Val score: 0.9505\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:22.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1395 | Val score: 0.9554\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:24.253\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1351 | Train score: 0.9506 | Val loss: 0.1388 | Val score: 0.9455\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:26.003\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1408 | Train score: 0.9506 | Val loss: 0.1370 | Val score: 0.9554\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:27.852\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.0921 | Train score: 0.9691 | Val loss: 0.1414 | Val score: 0.9554\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:29.572\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1691 | Train score: 0.9506 | Val loss: 0.1348 | Val score: 0.9604\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:31.282\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1706 | Train score: 0.9444 | Val loss: 0.1313 | Val score: 0.9604\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:32.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1084 | Train score: 0.9691 | Val loss: 0.1298 | Val score: 0.9604\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:34.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.0956 | Train score: 0.9630 | Val loss: 0.1294 | Val score: 0.9604\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:36.266\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1137 | Train score: 0.9568 | Val loss: 0.1303 | Val score: 0.9604\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:38.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1674 | Train score: 0.9506 | Val loss: 0.1292 | Val score: 0.9554\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:39.927\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2041 | Train score: 0.9444 | Val loss: 0.1286 | Val score: 0.9554\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:41.523\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1076 | Val score: 0.9505\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:43.153\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1337 | Train score: 0.9383 | Val loss: 0.1051 | Val score: 0.9505\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:44.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.0933 | Train score: 0.9630 | Val loss: 0.1036 | Val score: 0.9604\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:46.397\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1162 | Train score: 0.9630 | Val loss: 0.1004 | Val score: 0.9604\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:47.882\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1228 | Train score: 0.9691 | Val loss: 0.1029 | Val score: 0.9604\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:49.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1084 | Train score: 0.9568 | Val loss: 0.1062 | Val score: 0.9604\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:50.889\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1196 | Train score: 0.9815 | Val loss: 0.1116 | Val score: 0.9554\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:52.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1334 | Train score: 0.9691 | Val loss: 0.1152 | Val score: 0.9554\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:53.931\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1048 | Train score: 0.9815 | Val loss: 0.1253 | Val score: 0.9554\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:55.468\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1183 | Train score: 0.9630 | Val loss: 0.1283 | Val score: 0.9554\u001b[0m\n",
      "\u001b[32m2024-10-29 23:35:56.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.0971 | Train score: 0.9630 | Val loss: 0.1295 | Val score: 0.9554\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " AnovaSelect \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "MajorityClass                   0.946         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.000        0.000\n",
      "XGBClassifier                   0.948         0.010           0.463          0.206        0.317       0.203         0.848        0.081    0.364   0.195         0.009        0.001\n",
      "MedPFNClassifier                0.950         0.010           0.659          0.239        0.383       0.107         0.891        0.048    0.446   0.067         2.481        0.044\n",
      "MedPFNClassifier                0.957         0.010           0.700          0.221        0.400       0.153         0.893        0.073    0.491   0.136         2.599        0.131\n",
      "MedPFNClassifier                0.957         0.014           0.645          0.169        0.483       0.203         0.899        0.076    0.535   0.153         7.124        0.120\n",
      "RandomForestClassifier          0.951         0.015           0.400          0.436        0.200       0.233         0.897        0.063    0.255   0.289         0.189        0.011\n",
      "AutoGluon                       0.948         0.007           0.433          0.389        0.133       0.125         0.874        0.049    0.196   0.173         7.879        0.440\n",
      "CatBoostGrid                    0.949         0.006           0.500          0.447        0.133       0.125         0.901        0.061    0.194   0.164        44.143        3.693\n",
      "XGBoostGrid                     0.946         0.000           0.000          0.000        0.000       0.000         0.871        0.054    0.000   0.000        29.163        1.403\n",
      "LogisticRegression              0.946         0.000           0.000          0.000        0.000       0.000         0.578        0.147    0.000   0.000         0.003        0.000\n",
      "TabPFNClassifier                0.946         0.018           0.373          0.397        0.200       0.194         0.906        0.057    0.259   0.261         2.714        0.083\n",
      "TabForestPFNClassifier          0.942         0.020           0.505          0.305        0.350       0.203         0.884        0.041    0.377   0.197        18.179        1.223\n"
     ]
    }
   ],
   "source": [
    "save = False\n",
    "sampling = None\n",
    "cv = 10\n",
    "best_delete = 0\n",
    "strat_split = True\n",
    "recomp = False\n",
    "n_optim = 1000\n",
    "cat_optim = 10\n",
    "ft_epochs = 10\n",
    "ft_lr = 1e-5\n",
    "max_s = 1024\n",
    "max_q = 128\n",
    "max_samples = 1024\n",
    "no_pre_process = False\n",
    "multi_decoder = \"permutation\"\n",
    "N_ens = 9\n",
    "seed = 42\n",
    "overwrite = True\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\", \"f1\"]\n",
    "dir_path = os.path.abspath(os.getcwd())\n",
    "run_name = \"medium_net_mlp_balance_bsplit_baseline\" ## NO PERM, VERY ROBUST\n",
    "#run_name2 = \"large_mlp_var_balance_05weight_anova\" ## das aller bestigste\n",
    "run_name = \"medium_mlp_var_balance_05weight_anova\" ## das aller bestigste\n",
    "run_name2 = \"medium_mlp_var_balance_05weight_anova_newprior\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_200\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_longer\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_balance_lownoisefixparam_anova\"\n",
    "#run_name = \"medium_mlp_0.5static_balance_anova\"\n",
    "#run_name = \"large_mlp_fullbalance\"\n",
    "#run_name = \"small_net_mlp_var_imbalance_05weight\" ## WORKS VERY WELL WITH PERM\n",
    "#run_name = \"small_net_mlp_varbalance_weight_batchunisplit\"\n",
    "#run_name = \"small_net_mlp_variable_balance_weight\"\n",
    "#run_name = \"small_net_mlp_balance_minevalup\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "path2 = dir_path + f\"/logs/trainrun_{run_name2}\"\n",
    "filename = \"model\"\n",
    "\n",
    "\n",
    "models = [\n",
    "    #CatBoostOptim(n_optim=cat_optim),\n",
    "    MajorityClass(),\n",
    "    XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "    MedPFNClassifier(base_path=path2, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "    MedPFNClassifier(base_path=path2, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process,\n",
    "                    ft_epochs=ft_epochs, ft_lr=ft_lr),\n",
    "    RandomForestClassifier(),\n",
    "    AutoGluon(),\n",
    "    CatBoostGrid(),\n",
    "    XGBoostGrid(),\n",
    "    #XGBoostOptim(n_optim=n_optim),\n",
    "    LogisticRegression(max_iter=1000), \n",
    "    TabPFNClassifier(device='cpu', N_ensemble_configurations=5, no_preprocess_mode=no_pre_process),\n",
    "    TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\", max_epochs=ft_epochs)\n",
    "]\n",
    "reducer  = AnovaSelect()\n",
    "#for reducer in [AnovaSelect(), RandomSelect(), NonZeroSelect(), MeanSelect(), StdSelect(), MaxSelect(), PCASelect()]:\n",
    "#data = clr(all_data)\n",
    "\n",
    "#all_data = normalize(all_data)\n",
    "\n",
    "results_mean = pd.DataFrame(np.zeros((len(models), len(metrics)+1)),\n",
    "                            index=[m.__class__.__name__ for m in models],\n",
    "                            columns=metrics+[\"runtime\"])\n",
    "results_std = pd.DataFrame(np.zeros((len(models), len(metrics)+1)), \n",
    "                           index=[m.__class__.__name__ for m in models],\n",
    "                           columns=metrics+[\"runtime\"])\n",
    "\n",
    "for ii, model in enumerate(models):\n",
    "    results_mean.iloc[ii,:], results_std.iloc[ii,:] = cross_validate_sample(\n",
    "        model, all_data, labels, metrics, strat_split, cv, sampling, \n",
    "        reducer, max_samples, seed=seed, overwrite=overwrite, n_best_delete=best_delete,\n",
    "        recomp=recomp)\n",
    "\n",
    "results_mean = results_mean.add_suffix(\" mean\")\n",
    "results_std = results_std.add_suffix(\" std\")\n",
    "results_full = pd.concat((results_mean, results_std), axis=1)\n",
    "cols = results_full.columns.tolist()\n",
    "new_cols = []\n",
    "for i in range(int(len(cols)/2)):\n",
    "    new_cols.append(cols[i])\n",
    "    new_cols.append(cols[i+int(len(cols)/2)])\n",
    "results_full = results_full[new_cols]\n",
    "red_name = reducer.__class__.__name__\n",
    "if save:\n",
    "    directory = f\"results/{red_name}\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    save_path = f'results/{red_name}/baseline_cv{cv}_mxsamp{max_samples}_sd{seed}_ovrw{int(overwrite)}'\n",
    "    results_full.to_csv(save_path)\n",
    "print(\"\\n\", \"\\n\", red_name, \"\\n\", results_full.round(3))\n",
    "#print(results_full.sort_values(\"roc_auc mean\").round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6ece149-a8cf-4c43-ab58-5e23d8612e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "MajorityClass                   0.946         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.000        0.000\n",
      "LogisticRegression              0.946         0.000           0.000          0.000        0.000       0.000         0.578        0.147    0.000   0.000         0.003        0.000\n",
      "XGBClassifier                   0.948         0.010           0.463          0.206        0.317       0.203         0.848        0.081    0.364   0.195         0.009        0.001\n",
      "XGBoostGrid                     0.946         0.000           0.000          0.000        0.000       0.000         0.871        0.054    0.000   0.000        29.163        1.403\n",
      "AutoGluon                       0.948         0.007           0.433          0.389        0.133       0.125         0.874        0.049    0.196   0.173         7.879        0.440\n",
      "TabForestPFNClassifier          0.942         0.020           0.505          0.305        0.350       0.203         0.884        0.041    0.377   0.197        18.179        1.223\n",
      "MedPFNClassifier                0.950         0.010           0.659          0.239        0.383       0.107         0.891        0.048    0.446   0.067         2.481        0.044\n",
      "MedPFNClassifier                0.957         0.010           0.700          0.221        0.400       0.153         0.893        0.073    0.491   0.136         2.599        0.131\n",
      "RandomForestClassifier          0.951         0.015           0.400          0.436        0.200       0.233         0.897        0.063    0.255   0.289         0.189        0.011\n",
      "MedPFNClassifier                0.957         0.014           0.645          0.169        0.483       0.203         0.899        0.076    0.535   0.153         7.124        0.120\n",
      "CatBoostGrid                    0.949         0.006           0.500          0.447        0.133       0.125         0.901        0.061    0.194   0.164        44.143        3.693\n",
      "TabPFNClassifier                0.946         0.018           0.373          0.397        0.200       0.194         0.906        0.057    0.259   0.261         2.714        0.083\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(results_full.sort_values(\"roc_auc mean\").round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2ab1872-e271-44ac-b463-9ee4fa7127d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/external_pdac.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df_data = df.iloc[:,2:-5]\n",
    "data = df_data.to_numpy()\n",
    "labels = df[\"Disease\"].to_numpy()\n",
    "labels[labels==\"Healthy\"] = 0\n",
    "labels[labels==\"PDAC\"] = 1\n",
    "all_data = data\n",
    "data_c0 = data[labels==0]\n",
    "data_c1 = data[labels==1]\n",
    "num_c1 = data_c1.shape[0]\n",
    "num_c0 = int(num_c1*0.1)\n",
    "data = np.concatenate((data_c0[:num_c0], data_c1), axis=0)\n",
    "labels = np.concatenate((np.zeros(num_c0), np.ones(num_c1)))\n",
    "all_data, labels = unison_shuffled_copies(data, labels)\n",
    "all_data = data_to_comp(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5860c111-1280-470e-a332-3945d589858d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-29 23:46:16.182\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2038 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:16.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.0927 | Train score: 0.9286 | Val loss: 0.2120 | Val score: 0.8857\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:17.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.0311 | Train score: 1.0000 | Val loss: 0.2328 | Val score: 0.8571\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:18.053\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.0338 | Train score: 1.0000 | Val loss: 0.2622 | Val score: 0.8571\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:18.688\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.0697 | Train score: 0.9643 | Val loss: 0.2753 | Val score: 0.8571\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:19.564\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.0227 | Train score: 1.0000 | Val loss: 0.2912 | Val score: 0.8571\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:20.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.0145 | Train score: 1.0000 | Val loss: 0.3100 | Val score: 0.8571\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:21.028\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.0284 | Train score: 1.0000 | Val loss: 0.3360 | Val score: 0.8857\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:21.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.0062 | Train score: 1.0000 | Val loss: 0.3615 | Val score: 0.8857\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:22.253\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.0119 | Train score: 1.0000 | Val loss: 0.3877 | Val score: 0.8857\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:22.988\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.0108 | Train score: 1.0000 | Val loss: 0.4113 | Val score: 0.8857\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:23.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1397 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:24.473\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.0901 | Train score: 0.9286 | Val loss: 0.1121 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:25.073\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1109 | Train score: 0.9286 | Val loss: 0.0960 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:25.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.0884 | Train score: 0.9286 | Val loss: 0.0834 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:26.199\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.0662 | Train score: 0.9643 | Val loss: 0.0761 | Val score: 0.9714\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:26.737\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.0344 | Train score: 1.0000 | Val loss: 0.0678 | Val score: 0.9714\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:27.211\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.0265 | Train score: 1.0000 | Val loss: 0.0564 | Val score: 0.9714\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:27.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.0122 | Train score: 1.0000 | Val loss: 0.0467 | Val score: 0.9714\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:28.123\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.0160 | Train score: 1.0000 | Val loss: 0.0397 | Val score: 0.9714\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:28.642\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.0165 | Train score: 1.0000 | Val loss: 0.0326 | Val score: 0.9714\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:29.129\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.0964 | Train score: 0.9643 | Val loss: 0.0463 | Val score: 0.9714\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:29.889\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1576 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:30.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1212 | Train score: 0.9286 | Val loss: 0.1100 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:30.901\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.0305 | Train score: 1.0000 | Val loss: 0.0992 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:31.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.0640 | Train score: 0.9286 | Val loss: 0.0884 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:31.878\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.0378 | Train score: 0.9643 | Val loss: 0.0845 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:32.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.0350 | Train score: 1.0000 | Val loss: 0.0873 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:33.003\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.0552 | Train score: 1.0000 | Val loss: 0.0957 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:33.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2582 | Train score: 0.9643 | Val loss: 0.0941 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:34.696\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.0458 | Train score: 1.0000 | Val loss: 0.0911 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:35.433\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1676 | Train score: 0.9286 | Val loss: 0.0878 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:36.149\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.0022 | Train score: 1.0000 | Val loss: 0.0826 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:36.907\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1595 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:37.410\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.0898 | Train score: 0.9286 | Val loss: 0.1648 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:37.869\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.0609 | Train score: 0.9286 | Val loss: 0.1868 | Val score: 0.8857\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:38.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.0356 | Train score: 1.0000 | Val loss: 0.2026 | Val score: 0.8857\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:38.942\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.0138 | Train score: 1.0000 | Val loss: 0.1937 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:39.571\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.0217 | Train score: 1.0000 | Val loss: 0.2031 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:40.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.0204 | Train score: 1.0000 | Val loss: 0.2353 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:40.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.0130 | Train score: 1.0000 | Val loss: 0.2798 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:41.208\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.0020 | Train score: 1.0000 | Val loss: 0.2806 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:41.824\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.0027 | Train score: 1.0000 | Val loss: 0.3300 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:42.396\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.0178 | Train score: 1.0000 | Val loss: 0.3020 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:43.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2010 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:43.706\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.0606 | Train score: 0.9643 | Val loss: 0.2121 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:44.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.0858 | Train score: 0.9286 | Val loss: 0.2022 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:44.723\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.0577 | Train score: 0.9643 | Val loss: 0.1917 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:45.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.0351 | Train score: 1.0000 | Val loss: 0.1816 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:45.951\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.0866 | Train score: 0.9643 | Val loss: 0.1905 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:46.425\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.0978 | Train score: 0.9643 | Val loss: 0.2030 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:46.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.0437 | Train score: 1.0000 | Val loss: 0.2113 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:47.403\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.0777 | Train score: 0.9643 | Val loss: 0.2284 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:47.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.0230 | Train score: 1.0000 | Val loss: 0.2506 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:48.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.0525 | Train score: 1.0000 | Val loss: 0.2801 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:49.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1916 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:49.685\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.0517 | Train score: 1.0000 | Val loss: 0.1993 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:50.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.0357 | Train score: 1.0000 | Val loss: 0.2212 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:50.676\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.0624 | Train score: 0.9643 | Val loss: 0.2367 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:51.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.0310 | Train score: 0.9643 | Val loss: 0.2461 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:51.629\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.0571 | Train score: 0.9286 | Val loss: 0.2078 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:52.090\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.0040 | Train score: 1.0000 | Val loss: 0.1774 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:52.525\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.0052 | Train score: 1.0000 | Val loss: 0.2688 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:53.012\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.0133 | Train score: 1.0000 | Val loss: 0.3321 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:53.562\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.0017 | Train score: 1.0000 | Val loss: 0.1903 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:54.046\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.0016 | Train score: 1.0000 | Val loss: 0.1900 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:54.993\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1829 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:55.508\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1146 | Train score: 0.9286 | Val loss: 0.1502 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:56.016\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1307 | Train score: 0.8929 | Val loss: 0.1448 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:56.557\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.0420 | Train score: 1.0000 | Val loss: 0.1605 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:57.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.0989 | Train score: 0.9643 | Val loss: 0.1712 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:57.592\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.0431 | Train score: 0.9643 | Val loss: 0.1837 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:57.987\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.0190 | Train score: 1.0000 | Val loss: 0.1772 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:58.417\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.0693 | Train score: 0.9643 | Val loss: 0.2181 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:59.165\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.0161 | Train score: 1.0000 | Val loss: 0.2580 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:46:59.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.0258 | Train score: 1.0000 | Val loss: 0.2959 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:47:00.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.0096 | Train score: 1.0000 | Val loss: 0.3305 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:47:01.431\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1901 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:47:01.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.0735 | Train score: 0.9643 | Val loss: 0.1706 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:47:02.373\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.0845 | Train score: 0.9643 | Val loss: 0.1537 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:47:02.846\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.0280 | Train score: 1.0000 | Val loss: 0.1468 | Val score: 0.9429\u001b[0m\n",
      "\u001b[32m2024-10-29 23:47:03.310\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.0437 | Train score: 1.0000 | Val loss: 0.1499 | Val score: 0.9714\u001b[0m\n",
      "\u001b[32m2024-10-29 23:47:03.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.0210 | Train score: 1.0000 | Val loss: 0.1610 | Val score: 0.9714\u001b[0m\n",
      "\u001b[32m2024-10-29 23:47:04.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.0118 | Train score: 1.0000 | Val loss: 0.1758 | Val score: 0.9714\u001b[0m\n",
      "\u001b[32m2024-10-29 23:47:05.047\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.0103 | Train score: 1.0000 | Val loss: 0.1859 | Val score: 0.9714\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "save = False\n",
    "sampling = None\n",
    "cv = 10\n",
    "best_delete = 0\n",
    "strat_split = True\n",
    "recomp = False\n",
    "n_optim = 1000\n",
    "cat_optim = 10\n",
    "ft_epochs = 10\n",
    "ft_lr = 1e-5\n",
    "max_s = 1024\n",
    "max_q = 128\n",
    "max_samples = 1024\n",
    "no_pre_process = False\n",
    "multi_decoder = \"permutation\"\n",
    "N_ens = 9\n",
    "seed = 42\n",
    "overwrite = True\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\", \"f1\"]\n",
    "dir_path = os.path.abspath(os.getcwd())\n",
    "run_name = \"medium_net_mlp_balance_bsplit_baseline\" ## NO PERM, VERY ROBUST\n",
    "#run_name2 = \"large_mlp_var_balance_05weight_anova\" ## das aller bestigste\n",
    "run_name = \"medium_mlp_var_balance_05weight_anova\" ## das aller bestigste\n",
    "run_name2 = \"medium_mlp_var_balance_05weight_anova_newprior\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_200\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_longer\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_balance_lownoisefixparam_anova\"\n",
    "#run_name = \"medium_mlp_0.5static_balance_anova\"\n",
    "#run_name = \"large_mlp_fullbalance\"\n",
    "#run_name = \"small_net_mlp_var_imbalance_05weight\" ## WORKS VERY WELL WITH PERM\n",
    "#run_name = \"small_net_mlp_varbalance_weight_batchunisplit\"\n",
    "#run_name = \"small_net_mlp_variable_balance_weight\"\n",
    "#run_name = \"small_net_mlp_balance_minevalup\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "path2 = dir_path + f\"/logs/trainrun_{run_name2}\"\n",
    "filename = \"model\"\n",
    "\n",
    "\n",
    "models = [\n",
    "    #CatBoostOptim(n_optim=cat_optim),\n",
    "    MajorityClass(),\n",
    "    XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "    MedPFNClassifier(base_path=path2, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "    MedPFNClassifier(base_path=path2, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process,\n",
    "                    ft_epochs=ft_epochs, ft_lr=ft_lr),\n",
    "    RandomForestClassifier(),\n",
    "    #AutoGluon(),\n",
    "    #CatBoostGrid(),\n",
    "    #XGBoostGrid(),\n",
    "    #XGBoostOptim(n_optim=n_optim),\n",
    "    LogisticRegression(max_iter=1000), \n",
    "    TabPFNClassifier(device='cpu', N_ensemble_configurations=5, no_preprocess_mode=no_pre_process),\n",
    "    TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\", max_epochs=ft_epochs)\n",
    "]\n",
    "\n",
    "for reducer in [AnovaSelect(), RandomSelect(), NonZeroSelect(), MeanSelect(), StdSelect(), MaxSelect(), PCASelect()]:\n",
    "    results_mean = pd.DataFrame(np.zeros((len(models), len(metrics)+1)),\n",
    "                                index=[m.__class__.__name__ for m in models],\n",
    "                                columns=metrics+[\"runtime\"])\n",
    "    results_std = pd.DataFrame(np.zeros((len(models), len(metrics)+1)), \n",
    "                               index=[m.__class__.__name__ for m in models],\n",
    "                               columns=metrics+[\"runtime\"])\n",
    "    \n",
    "    for ii, model in enumerate(models):\n",
    "        results_mean.iloc[ii,:], results_std.iloc[ii,:] = cross_validate_sample(\n",
    "            model, all_data, labels, metrics, strat_split, cv, sampling, \n",
    "            reducer, max_samples, seed=seed, overwrite=overwrite, n_best_delete=best_delete,\n",
    "            recomp=recomp)\n",
    "\n",
    "    results_mean = results_mean.add_suffix(\" mean\")\n",
    "    results_std = results_std.add_suffix(\" std\")\n",
    "    results_full = pd.concat((results_mean, results_std), axis=1)\n",
    "    cols = results_full.columns.tolist()\n",
    "    new_cols = []\n",
    "    for i in range(int(len(cols)/2)):\n",
    "        new_cols.append(cols[i])\n",
    "        new_cols.append(cols[i+int(len(cols)/2)])\n",
    "    results_full = results_full[new_cols]\n",
    "    red_name = reducer.__class__.__name__\n",
    "    if save:\n",
    "        directory = f\"results/{red_name}\"\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        save_path = f'results/{red_name}/baseline_cv{cv}_mxsamp{max_samples}_sd{seed}_ovrw{int(overwrite)}'\n",
    "        results_full.to_csv(save_path)\n",
    "    print(\"\\n\", \"\\n\", red_name, \"\\n\", results_full.round(3))\n",
    "    #print(results_full.sort_values(\"roc_auc mean\").round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51de1538-5dd7-40f8-9903-098b410b82cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = True\n",
    "sampling = None\n",
    "cv = 7\n",
    "best_delete = 0\n",
    "strat_split = True\n",
    "n_optim = 1000\n",
    "cat_optim = 10\n",
    "ft_epochs = 15\n",
    "ft_lr = 1e-7\n",
    "max_s = 1024\n",
    "max_q = 128\n",
    "max_samples = 1024\n",
    "no_pre_process = False\n",
    "multi_decoder = \"permutation\"\n",
    "N_ens = 7\n",
    "seed = 42\n",
    "overwrite = True\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\", \"f1\"]\n",
    "dir_path = os.path.abspath(os.getcwd())\n",
    "run_name = \"medium_net_mlp_balance_bsplit_baseline\" ## NO PERM, VERY ROBUST\n",
    "run_name = \"medium_mlp_var_balance_05weight_anova\" ## das aller bestigste\n",
    "#run_name = \"large_mlp_var_balance_05weight_anova\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_balance_lownoisefixparam_anova\"\n",
    "#run_name = \"medium_mlp_0.5static_balance_anova\"\n",
    "#run_name = \"large_mlp_fullbalance\"\n",
    "#run_name = \"small_net_mlp_var_imbalance_05weight\" ## WORKS VERY WELL WITH PERM\n",
    "#run_name = \"small_net_mlp_varbalance_weight_batchunisplit\"\n",
    "#run_name = \"small_net_mlp_variable_balance_weight\"\n",
    "#run_name = \"small_net_mlp_balance_minevalup\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "filename = \"model\"\n",
    "models = [\n",
    "    #CatBoostOptim(n_optim=cat_optim),\n",
    "    MajorityClass(),\n",
    "    XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "    #MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process,\n",
    "    #                ft_epochs=ft_epochs, ft_lr=ft_lr),\n",
    "    RandomForestClassifier(),\n",
    "    #XGBoostOptim(n_optim=n_optim),\n",
    "    LogisticRegression(max_iter=1000), \n",
    "    TabPFNClassifier(device='cpu', N_ensemble_configurations=5, no_preprocess_mode=no_pre_process),\n",
    "    TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\", max_epochs=ft_epochs)\n",
    "]\n",
    "\n",
    "for reducer in [AnovaSelect()]:#, RandomSelect(), NonZeroSelect(), MeanSelect(), StdSelect(), MaxSelect(), PCASelect()]:\n",
    "    for std in [1e-9,1e-8,1e-7,1e-6,1e-5,1e-4]:\n",
    "        print(\"\\n\\n\", std)\n",
    "        noise_data = all_data + np.abs(np.random.default_rng(seed=42).normal(0,std,size=all_data.shape))\n",
    "        results_mean = pd.DataFrame(np.zeros((len(models), len(metrics)+1)),\n",
    "                                    index=[m.__class__.__name__ for m in models],\n",
    "                                    columns=metrics+[\"runtime\"])\n",
    "        results_std = pd.DataFrame(np.zeros((len(models), len(metrics)+1)), \n",
    "                                   index=[m.__class__.__name__ for m in models],\n",
    "                                   columns=metrics+[\"runtime\"])\n",
    "        \n",
    "        for ii, model in enumerate(models):\n",
    "            results_mean.iloc[ii,:], results_std.iloc[ii,:] = cross_validate_sample(\n",
    "                model, noise_data, labels, metrics, strat_split, cv, sampling, \n",
    "                reducer, max_samples, seed=seed, overwrite=overwrite, n_best_delete=best_delete)\n",
    "    \n",
    "        results_mean = results_mean.add_suffix(\" mean\")\n",
    "        results_std = results_std.add_suffix(\" std\")\n",
    "        results_full = pd.concat((results_mean, results_std), axis=1)\n",
    "        cols = results_full.columns.tolist()\n",
    "        new_cols = []\n",
    "        for i in range(int(len(cols)/2)):\n",
    "            new_cols.append(cols[i])\n",
    "            new_cols.append(cols[i+int(len(cols)/2)])\n",
    "        results_full = results_full[new_cols]\n",
    "        red_name = \"noise\"\n",
    "        if save:\n",
    "            directory = f\"results/{red_name}\"\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            save_path = f'results/{red_name}/n{std}_cv{cv}_mxsamp{max_samples}_sd{seed}_ovrw{int(overwrite)}'\n",
    "            results_full.to_csv(save_path)\n",
    "        #print(\"\\n\", \"\\n\", red_name, \"\\n\", results_full.round(3))\n",
    "        print(results_full.sort_values(\"roc_auc mean\").round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eb424e-0b18-471f-9cf6-4b1ce31e88c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save = True\n",
    "sampling = None\n",
    "cv = 7\n",
    "best_delete = 0\n",
    "strat_split = True\n",
    "n_optim = 1000\n",
    "cat_optim = 10\n",
    "ft_epochs = 10\n",
    "ft_lr = 1e-7\n",
    "max_s = 1024\n",
    "max_q = 128\n",
    "max_samples = 1024\n",
    "no_pre_process = False\n",
    "multi_decoder = \"permutation\"\n",
    "N_ens = 7\n",
    "seed = 42\n",
    "overwrite = True\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\", \"f1\"]\n",
    "dir_path = os.path.abspath(os.getcwd())\n",
    "run_name = \"medium_net_mlp_balance_bsplit_baseline\" ## NO PERM, VERY ROBUST\n",
    "run_name = \"medium_mlp_var_balance_05weight_anova\" ### BEST!!!!!! ####\n",
    "#run_name = \"medium_mlp_balance_lownoisefixparam_anova\"\n",
    "#run_name = \"medium_mlp_0.5static_balance_anova\"\n",
    "#run_name = \"large_mlp_fullbalance\"\n",
    "#run_name = \"small_net_mlp_var_imbalance_05weight\" ## WORKS VERY WELL WITH PERM\n",
    "#run_name = \"small_net_mlp_varbalance_weight_batchunisplit\"\n",
    "#run_name = \"small_net_mlp_variable_balance_weight\"\n",
    "#run_name = \"small_net_mlp_balance_minevalup\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "filename = \"model\"\n",
    "models = [\n",
    "    #CatBoostOptim(n_optim=cat_optim),\n",
    "    XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "    RandomForestClassifier(),\n",
    "    #XGBoostOptim(n_optim=n_optim),\n",
    "    LogisticRegression(max_iter=1000), \n",
    "    TabPFNClassifier(device='cpu', N_ensemble_configurations=5, no_preprocess_mode=no_pre_process),\n",
    "    TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\", max_epochs=ft_epochs)\n",
    "]\n",
    "\n",
    "#for reducer in [AnovaSelect(), NonZeroSelect(), MeanSelect(), StdSelect(), MaxSelect(), PCASelect()]:\n",
    "reducer = AnovaSelect()\n",
    "#for reduce_data in [top_anova, top_non_zero, top_mean, top_std, top_max, pca_reduce]:\n",
    "    #data = reduce_data(all_data, labels, 100)\n",
    "    #print(all_data.shape)\n",
    "for best_delete in range(0,510,10):\n",
    "    #reducer.k = 100\n",
    "    #reducer = None\n",
    "    results_mean = pd.DataFrame(np.zeros((len(models), len(metrics)+1)),\n",
    "                                index=[m.__class__.__name__ for m in models],\n",
    "                                columns=metrics+[\"runtime\"])\n",
    "    results_std = pd.DataFrame(np.zeros((len(models), len(metrics)+1)), \n",
    "                               index=[m.__class__.__name__ for m in models],\n",
    "                               columns=metrics+[\"runtime\"])\n",
    "    \n",
    "    for ii, model in enumerate(models):\n",
    "        results_mean.iloc[ii,:], results_std.iloc[ii,:] = cross_validate_sample(\n",
    "            model, all_data, labels, metrics, strat_split, cv, sampling, \n",
    "            reducer, max_samples, seed=seed, overwrite=overwrite, n_best_delete=best_delete)\n",
    "\n",
    "    results_mean = results_mean.add_suffix(\" mean\")\n",
    "    results_std = results_std.add_suffix(\" std\")\n",
    "    results_full = pd.concat((results_mean, results_std), axis=1)\n",
    "    cols = results_full.columns.tolist()\n",
    "    new_cols = []\n",
    "    for i in range(int(len(cols)/2)):\n",
    "        new_cols.append(cols[i])\n",
    "        new_cols.append(cols[i+int(len(cols)/2)])\n",
    "    results_full = results_full[new_cols]\n",
    "    red_name = \"feature_select_shift_10step\"\n",
    "    if save:\n",
    "        directory = f\"results/{red_name}\"\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        save_path = f'results/{red_name}/{best_delete}_cv{cv}_mxsamp{max_samples}_sd{seed}_ovrw{int(overwrite)}'\n",
    "        results_full.to_csv(save_path)\n",
    "    print(\"\\n\", \"\\n\", red_name, \"\\n\", results_full.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16f86dc-a1ee-4706-bc17-cbedd58ff3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "all_labels = []\n",
    "for p in [\"datasets/CRC_AUS_LOSO.csv\", \n",
    "          \"datasets/CRC_FRA_LOSO.csv\",\n",
    "         \"datasets/CRC_CHI_LOSO.csv\",\n",
    "         \"datasets/CRC_GER_LOSO.csv\",\n",
    "         #\"datasets/CRC_IND_additional.csv\",\n",
    "         \"datasets/CRC_USA_LOSO.csv\"]:\n",
    "    df = pd.read_csv(p)\n",
    "    df_binary = df.loc[(df[\"disease\"] == \"healthy\") | (df[\"disease\"]==\"CRC\")]\n",
    "    df_data = df_binary.iloc[:,4:]\n",
    "    data = df_data.to_numpy()\n",
    "    labels = df_binary[\"disease\"].to_numpy()\n",
    "    labels[labels==\"healthy\"] = 0\n",
    "    labels[labels==\"CRC\"] = 1\n",
    "    data = (1/np.sum(data, axis=1, keepdims=True))*data\n",
    "    all_data.append(data)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "all_data = np.concatenate(all_data,axis=0)\n",
    "labels = np.concatenate(all_labels)\n",
    "all_data, labels = unison_shuffled_copies(all_data, labels, seed=412)\n",
    "c1_ind = (labels==1).nonzero()[0]\n",
    "c1_del = c1_ind[:int(len(c1_ind)*0.97)]\n",
    "all_data, labels = np.delete(all_data, c1_del, axis=0), np.delete(labels, c1_del, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbafc58-eff4-4b74-b585-52c0a8cf6234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da53d28-57c8-48ba-a1e1-3080b58d3553",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = 20\n",
    "rocs = []\n",
    "f1s = []\n",
    "for i in range(0,510,10):\n",
    "    results = pd.read_csv(f'results/feature_select_shift_10step/{i}_cv{7}_mxsamp{1024}_sd{42}_ovrw{1}')\n",
    "    rocs.append(results[\"roc_auc mean\"].values)\n",
    "    f1s.append(results[\"f1 mean\"].values)\n",
    "rocs = np.array(rocs)\n",
    "f1s = np.array(f1s)\n",
    "labels = results.iloc[:,0].values\n",
    "plt.figure(figsize=(12,3), dpi=200)\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "colors = []\n",
    "for cl in range(rocs.shape[1]):\n",
    "    ax1.plot(np.arange(rocs.shape[0]-ws+1)*10, moving_average(rocs[:,cl], ws), label=labels[cl])\n",
    "    ax2.plot(np.arange(rocs.shape[0]-ws+1)*10, moving_average(f1s[:,cl], ws), label=labels[cl])\n",
    "plt.suptitle(\"ROC AUC and F1-Score shifting feature selection\")\n",
    "#ax1.set_title(\"ROC AUC\")\n",
    "ax1.set_ylabel(\"ROC AUC\")\n",
    "#ax2.set_title(\"F1-Score\")\n",
    "ax2.set_xlabel(\"feature shift\")\n",
    "ax2.set_ylabel(\"F1\")\n",
    "ax1.legend(fontsize=7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d448cf5d-91f9-4f84-9477-6fe30540037e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229c531a-d6f8-464b-bb73-476293131bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.count_nonzero(all_data, axis=0)\n",
    "counts = 1-counts/all_data.shape[0]\n",
    "means = np.mean(all_data, axis=0)\n",
    "plt.scatter(counts, means, s=1)\n",
    "plt.show()\n",
    "plt.scatter(counts, np.max(all_data, axis=0), s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216c8ae-5da2-4b93-bbde-e5bd0009e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3d2554-f847-48c5-9566-e04693fa8eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.abspath(os.getcwd())\n",
    "run_name = \"medium_net_mlp_balance_bsplit_baseline\"\n",
    "run_name = \"medium_net_mlp_balance_bsplit_baseline_longer\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "filename = \"model\"\n",
    "for sampling in [None]:#, undersample]:\n",
    "    cv = 5\n",
    "    strat_split = True\n",
    "    n_optim = 1000\n",
    "    cat_optim = 10\n",
    "    ft_epochs = 10\n",
    "    ft_lr = 1e-8\n",
    "    max_s = 1024\n",
    "    max_q = 128\n",
    "    max_samples = None\n",
    "    no_pre_process = False\n",
    "    multi_decoder = None\n",
    "    N_ens = 5\n",
    "    metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\"]\n",
    "    models = [\n",
    "        #CatBoostOptim(n_optim=cat_optim),\n",
    "        #MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder, ft_epochs=ft_epochs, ft_lr=ft_lr,\n",
    "        #                 max_s=max_s, max_q=max_q, no_preprocess_mode=no_pre_process),\n",
    "        MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "        #MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=5, no_preprocess_mode=True),\n",
    "        XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "        XGBoostOptim(n_optim=n_optim),\n",
    "        LogisticRegression(max_iter=500), \n",
    "        TabPFNClassifier(device='cpu', N_ensemble_configurations=5, no_preprocess_mode=no_pre_process),\n",
    "        TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\", max_epochs=ft_epochs)\n",
    "    ]\n",
    "    results_mean = pd.DataFrame(np.zeros((len(models), len(metrics)+1)),\n",
    "                                index=[m.__class__.__name__ for m in models],\n",
    "                                columns=metrics+[\"runtime\"])\n",
    "    results_std = pd.DataFrame(np.zeros((len(models), len(metrics)+1)), \n",
    "                               index=[m.__class__.__name__ for m in models],\n",
    "                               columns=metrics+[\"runtime\"])\n",
    "    \n",
    "    for ii, model in enumerate(models):\n",
    "        results_mean.iloc[ii,:], results_std.iloc[ii,:] = cross_validate_sample(model, data, labels, metrics, strat_split, cv, sampling, max_samples)\n",
    "    #results_sorted = results.sort_values(\"roc_auc\")\n",
    "    #print(results_sorted)\n",
    "    print(results_mean)\n",
    "    print(results_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04052993-69b3-4c87-9bf8-163357aa9fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "for m in metrics + \"runtime\":\n",
    "    cols.append(m)\n",
    "    cols.append(m+\" std\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc23d5f-155a-4e3d-b696-1fd6439a0cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dir_path = os.path.abspath(os.getcwd())\n",
    "run_name = \"medium_net_mlp_balance_bsplit_baseline\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "filename = \"model\"\n",
    "#model, config = load_model(path, filename, device=\"cpu\", eval_positions=None, verbose=0)\n",
    "#pred_model = TabPFNClassifier(model[2], config, device=\"cpu\", N_ensemble_configurations=5, no_preprocess_mode=False)\n",
    "for sampling in [None]:\n",
    "    cv = 3\n",
    "    strat_split = True\n",
    "    n_optim = 10\n",
    "    ft_epochs = 10\n",
    "    max_samples = None\n",
    "    metrics = metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\"]\n",
    "    models = [\n",
    "        RandomForestClassifier()\n",
    "        #CatBoostOptim(n_optim=n_optim),\n",
    "        #pred_model,\n",
    "        #MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=3, no_preprocess_mode=True),\n",
    "        #XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "        #XGBoostOptim(n_optim=n_optim),\n",
    "        #LogisticRegression(max_iter=500), \n",
    "        #TabPFNClassifier(device='cpu', N_ensemble_configurations=3, no_preprocess_mode=True),\n",
    "        #TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\", max_epochs=ft_epochs)\n",
    "    ]\n",
    "    results = pd.DataFrame(np.zeros((len(models), len(metrics)+1)), \n",
    "                           index=[m.__class__.__name__ for m in models],\n",
    "                          columns=metrics+[\"runtime\"])\n",
    "    \n",
    "    for ii, model in enumerate(models):\n",
    "        results.iloc[ii,:] = cross_validate_sample(model, data, labels, metrics, strat_split, cv, sampling, max_samples)\n",
    "    results_sorted = results.sort_values(\"roc_auc\")\n",
    "    \n",
    "    print(results_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5983e6a1-a7ce-4694-b775-e83498cd3be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819f6306-686d-42f1-829c-303890202785",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac970c40-6003-4c24-b454-014ba31f6b26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
