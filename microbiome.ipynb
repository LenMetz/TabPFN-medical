{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "716a0985-a3e5-47be-acc6-d58fb95fa8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from tabpfn_new.scripts.transformer_prediction_interface import TabPFNClassifier, MedPFNClassifier\n",
    "from tabpfn_new.scripts.model_builder import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from data_prep_utils import *\n",
    "from evaluate import *\n",
    "from load_models import *\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import openml\n",
    "import time\n",
    "#pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0e72742-f9d3-4748-939e-0aee908e04c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/data_all.csv\"\n",
    "all_data, labels = get_microbiome(path)\n",
    "all_data = remove_zero_features(all_data)\n",
    "all_data, labels = unison_shuffled_copies(all_data, labels, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "879ba0a8-b675-49eb-a1dc-64e836de555b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " AnovaSelect \n",
      "                         accuracy mean  accuracy std  precision mean  \\\n",
      "MajorityClass                   0.941         0.000           0.000   \n",
      "XGBClassifier                   0.946         0.008           0.576   \n",
      "MedPFNClassifier                0.959         0.012           0.668   \n",
      "RandomForestClassifier          0.946         0.005           0.571   \n",
      "LogisticRegression              0.932         0.010           0.449   \n",
      "\n",
      "                        precision std  recall mean  recall std  roc_auc mean  \\\n",
      "MajorityClass                   0.000        0.000       0.000         0.500   \n",
      "XGBClassifier                   0.114        0.329       0.167         0.657   \n",
      "MedPFNClassifier                0.108        0.614       0.125         0.797   \n",
      "RandomForestClassifier          0.280        0.186       0.146         0.590   \n",
      "LogisticRegression              0.244        0.214       0.064         0.595   \n",
      "\n",
      "                        roc_auc std  f1 mean  f1 std  runtime mean  \\\n",
      "MajorityClass                 0.000    0.000   0.000         0.000   \n",
      "XGBClassifier                 0.080    0.394   0.160         0.012   \n",
      "MedPFNClassifier              0.064    0.635   0.106         1.776   \n",
      "RandomForestClassifier        0.071    0.260   0.173         0.197   \n",
      "LogisticRegression            0.031    0.268   0.070         0.011   \n",
      "\n",
      "                        runtime std  \n",
      "MajorityClass                 0.000  \n",
      "XGBClassifier                 0.001  \n",
      "MedPFNClassifier              0.087  \n",
      "RandomForestClassifier        0.008  \n",
      "LogisticRegression            0.002  \n"
     ]
    }
   ],
   "source": [
    "save = False\n",
    "sampling = None\n",
    "cv = 7\n",
    "best_delete = 0\n",
    "strat_split = True\n",
    "recomp = False\n",
    "n_optim = 1000\n",
    "cat_optim = 10\n",
    "ft_epochs = 10\n",
    "ft_lr = 1e-5\n",
    "max_s = 1024\n",
    "max_q = 128\n",
    "max_samples = 1024\n",
    "no_pre_process = False\n",
    "multi_decoder = \"permutation\"\n",
    "N_ens = 5\n",
    "seed = 42\n",
    "overwrite = True\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\", \"f1\"]\n",
    "dir_path = os.path.abspath(os.getcwd())\n",
    "run_name = \"medium_net_mlp_balance_bsplit_baseline\" ## NO PERM, VERY ROBUST\n",
    "run_name = \"medium_mlp_var_balance_05weight_anova\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_longer\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_balance_lownoisefixparam_anova\"\n",
    "#run_name = \"medium_mlp_0.5static_balance_anova\"\n",
    "#run_name = \"large_mlp_fullbalance\"\n",
    "#run_name = \"small_net_mlp_var_imbalance_05weight\" ## WORKS VERY WELL WITH PERM\n",
    "#run_name = \"small_net_mlp_varbalance_weight_batchunisplit\"\n",
    "#run_name = \"small_net_mlp_variable_balance_weight\"\n",
    "#run_name = \"small_net_mlp_balance_minevalup\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "#path2 = dir_path + f\"/logs/trainrun_{run_name2}\"\n",
    "filename = \"model\"\n",
    "\n",
    "#all_data = all_data+1e-10\n",
    "#all_data = np.log(all_data)-np.mean(np.log(all_data), axis=0)\n",
    "all_data = (all_data-np.mean(all_data,axis=0))/(np.std(all_data,axis=0)+1e-10)\n",
    "models = [\n",
    "    #CatBoostOptim(n_optim=cat_optim),\n",
    "    MajorityClass(),\n",
    "    XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "    #MedPFNClassifier(base_path=path2, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "    #MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process,\n",
    "    #                ft_epochs=ft_epochs, ft_lr=ft_lr),\n",
    "    RandomForestClassifier(),\n",
    "    #XGBoostOptim(n_optim=n_optim),\n",
    "    LogisticRegression(max_iter=1000), \n",
    "    #TabPFNClassifier(device='cpu', N_ensemble_configurations=5, no_preprocess_mode=no_pre_process),\n",
    "    #TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\", max_epochs=ft_epochs)\n",
    "]\n",
    "\n",
    "for reducer in [AnovaSelect()]:#, RandomSelect(), NonZeroSelect(), MeanSelect(), StdSelect(), MaxSelect(), PCASelect()]:\n",
    "    results_mean = pd.DataFrame(np.zeros((len(models), len(metrics)+1)),\n",
    "                                index=[m.__class__.__name__ for m in models],\n",
    "                                columns=metrics+[\"runtime\"])\n",
    "    results_std = pd.DataFrame(np.zeros((len(models), len(metrics)+1)), \n",
    "                               index=[m.__class__.__name__ for m in models],\n",
    "                               columns=metrics+[\"runtime\"])\n",
    "    \n",
    "    for ii, model in enumerate(models):\n",
    "        results_mean.iloc[ii,:], results_std.iloc[ii,:] = cross_validate_sample(\n",
    "            model, all_data, labels, metrics, strat_split, cv, sampling, \n",
    "            reducer, max_samples, seed=seed, overwrite=overwrite, n_best_delete=best_delete,\n",
    "            recomp=recomp)\n",
    "\n",
    "    results_mean = results_mean.add_suffix(\" mean\")\n",
    "    results_std = results_std.add_suffix(\" std\")\n",
    "    results_full = pd.concat((results_mean, results_std), axis=1)\n",
    "    cols = results_full.columns.tolist()\n",
    "    new_cols = []\n",
    "    for i in range(int(len(cols)/2)):\n",
    "        new_cols.append(cols[i])\n",
    "        new_cols.append(cols[i+int(len(cols)/2)])\n",
    "    results_full = results_full[new_cols]\n",
    "    red_name = reducer.__class__.__name__\n",
    "    if save:\n",
    "        directory = f\"results/{red_name}\"\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        save_path = f'results/{red_name}/baseline_cv{cv}_mxsamp{max_samples}_sd{seed}_ovrw{int(overwrite)}'\n",
    "        results_full.to_csv(save_path)\n",
    "    print(\"\\n\", \"\\n\", red_name, \"\\n\", results_full.round(3))\n",
    "    #print(results_full.sort_values(\"roc_auc mean\").round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51de1538-5dd7-40f8-9903-098b410b82cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 1e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-22 20:39:14.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1046 | Val score: 0.9507\u001b[0m\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "save = True\n",
    "sampling = None\n",
    "cv = 7\n",
    "best_delete = 0\n",
    "strat_split = True\n",
    "n_optim = 1000\n",
    "cat_optim = 10\n",
    "ft_epochs = 15\n",
    "ft_lr = 1e-7\n",
    "max_s = 1024\n",
    "max_q = 128\n",
    "max_samples = 1024\n",
    "no_pre_process = False\n",
    "multi_decoder = \"permutation\"\n",
    "N_ens = 7\n",
    "seed = 42\n",
    "overwrite = True\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\", \"f1\"]\n",
    "dir_path = os.path.abspath(os.getcwd())\n",
    "run_name = \"medium_net_mlp_balance_bsplit_baseline\" ## NO PERM, VERY ROBUST\n",
    "run_name = \"medium_mlp_var_balance_05weight_anova\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_balance_lownoisefixparam_anova\"\n",
    "#run_name = \"medium_mlp_0.5static_balance_anova\"\n",
    "#run_name = \"large_mlp_fullbalance\"\n",
    "#run_name = \"small_net_mlp_var_imbalance_05weight\" ## WORKS VERY WELL WITH PERM\n",
    "#run_name = \"small_net_mlp_varbalance_weight_batchunisplit\"\n",
    "#run_name = \"small_net_mlp_variable_balance_weight\"\n",
    "#run_name = \"small_net_mlp_balance_minevalup\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "filename = \"model\"\n",
    "models = [\n",
    "    #CatBoostOptim(n_optim=cat_optim),\n",
    "    MajorityClass(),\n",
    "    XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "    #MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process,\n",
    "    #                ft_epochs=ft_epochs, ft_lr=ft_lr),\n",
    "    RandomForestClassifier(),\n",
    "    #XGBoostOptim(n_optim=n_optim),\n",
    "    LogisticRegression(max_iter=1000), \n",
    "    TabPFNClassifier(device='cpu', N_ensemble_configurations=5, no_preprocess_mode=no_pre_process),\n",
    "    TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\", max_epochs=ft_epochs)\n",
    "]\n",
    "\n",
    "for reducer in [AnovaSelect()]:#, RandomSelect(), NonZeroSelect(), MeanSelect(), StdSelect(), MaxSelect(), PCASelect()]:\n",
    "    for std in [1e-9,1e-8,1e-7,1e-6,1e-5,1e-4]:\n",
    "        print(\"\\n\\n\", std)\n",
    "        noise_data = all_data + np.abs(np.random.default_rng(seed=42).normal(0,std,size=all_data.shape))\n",
    "        results_mean = pd.DataFrame(np.zeros((len(models), len(metrics)+1)),\n",
    "                                    index=[m.__class__.__name__ for m in models],\n",
    "                                    columns=metrics+[\"runtime\"])\n",
    "        results_std = pd.DataFrame(np.zeros((len(models), len(metrics)+1)), \n",
    "                                   index=[m.__class__.__name__ for m in models],\n",
    "                                   columns=metrics+[\"runtime\"])\n",
    "        \n",
    "        for ii, model in enumerate(models):\n",
    "            results_mean.iloc[ii,:], results_std.iloc[ii,:] = cross_validate_sample(\n",
    "                model, noise_data, labels, metrics, strat_split, cv, sampling, \n",
    "                reducer, max_samples, seed=seed, overwrite=overwrite, n_best_delete=best_delete)\n",
    "    \n",
    "        results_mean = results_mean.add_suffix(\" mean\")\n",
    "        results_std = results_std.add_suffix(\" std\")\n",
    "        results_full = pd.concat((results_mean, results_std), axis=1)\n",
    "        cols = results_full.columns.tolist()\n",
    "        new_cols = []\n",
    "        for i in range(int(len(cols)/2)):\n",
    "            new_cols.append(cols[i])\n",
    "            new_cols.append(cols[i+int(len(cols)/2)])\n",
    "        results_full = results_full[new_cols]\n",
    "        red_name = \"noise\"\n",
    "        if save:\n",
    "            directory = f\"results/{red_name}\"\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            save_path = f'results/{red_name}/n{std}_cv{cv}_mxsamp{max_samples}_sd{seed}_ovrw{int(overwrite)}'\n",
    "            results_full.to_csv(save_path)\n",
    "        #print(\"\\n\", \"\\n\", red_name, \"\\n\", results_full.round(3))\n",
    "        print(results_full.sort_values(\"roc_auc mean\").round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eb424e-0b18-471f-9cf6-4b1ce31e88c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save = True\n",
    "sampling = None\n",
    "cv = 7\n",
    "best_delete = 0\n",
    "strat_split = True\n",
    "n_optim = 1000\n",
    "cat_optim = 10\n",
    "ft_epochs = 10\n",
    "ft_lr = 1e-7\n",
    "max_s = 1024\n",
    "max_q = 128\n",
    "max_samples = 1024\n",
    "no_pre_process = False\n",
    "multi_decoder = \"permutation\"\n",
    "N_ens = 7\n",
    "seed = 42\n",
    "overwrite = True\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\", \"f1\"]\n",
    "dir_path = os.path.abspath(os.getcwd())\n",
    "run_name = \"medium_net_mlp_balance_bsplit_baseline\" ## NO PERM, VERY ROBUST\n",
    "run_name = \"medium_mlp_var_balance_05weight_anova\" ### BEST!!!!!! ####\n",
    "#run_name = \"medium_mlp_balance_lownoisefixparam_anova\"\n",
    "#run_name = \"medium_mlp_0.5static_balance_anova\"\n",
    "#run_name = \"large_mlp_fullbalance\"\n",
    "#run_name = \"small_net_mlp_var_imbalance_05weight\" ## WORKS VERY WELL WITH PERM\n",
    "#run_name = \"small_net_mlp_varbalance_weight_batchunisplit\"\n",
    "#run_name = \"small_net_mlp_variable_balance_weight\"\n",
    "#run_name = \"small_net_mlp_balance_minevalup\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "filename = \"model\"\n",
    "models = [\n",
    "    #CatBoostOptim(n_optim=cat_optim),\n",
    "    XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "    RandomForestClassifier(),\n",
    "    #XGBoostOptim(n_optim=n_optim),\n",
    "    LogisticRegression(max_iter=1000), \n",
    "    TabPFNClassifier(device='cpu', N_ensemble_configurations=5, no_preprocess_mode=no_pre_process),\n",
    "    TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\", max_epochs=ft_epochs)\n",
    "]\n",
    "\n",
    "#for reducer in [AnovaSelect(), NonZeroSelect(), MeanSelect(), StdSelect(), MaxSelect(), PCASelect()]:\n",
    "reducer = AnovaSelect()\n",
    "#for reduce_data in [top_anova, top_non_zero, top_mean, top_std, top_max, pca_reduce]:\n",
    "    #data = reduce_data(all_data, labels, 100)\n",
    "    #print(all_data.shape)\n",
    "for best_delete in range(0,510,10):\n",
    "    #reducer.k = 100\n",
    "    #reducer = None\n",
    "    results_mean = pd.DataFrame(np.zeros((len(models), len(metrics)+1)),\n",
    "                                index=[m.__class__.__name__ for m in models],\n",
    "                                columns=metrics+[\"runtime\"])\n",
    "    results_std = pd.DataFrame(np.zeros((len(models), len(metrics)+1)), \n",
    "                               index=[m.__class__.__name__ for m in models],\n",
    "                               columns=metrics+[\"runtime\"])\n",
    "    \n",
    "    for ii, model in enumerate(models):\n",
    "        results_mean.iloc[ii,:], results_std.iloc[ii,:] = cross_validate_sample(\n",
    "            model, all_data, labels, metrics, strat_split, cv, sampling, \n",
    "            reducer, max_samples, seed=seed, overwrite=overwrite, n_best_delete=best_delete)\n",
    "\n",
    "    results_mean = results_mean.add_suffix(\" mean\")\n",
    "    results_std = results_std.add_suffix(\" std\")\n",
    "    results_full = pd.concat((results_mean, results_std), axis=1)\n",
    "    cols = results_full.columns.tolist()\n",
    "    new_cols = []\n",
    "    for i in range(int(len(cols)/2)):\n",
    "        new_cols.append(cols[i])\n",
    "        new_cols.append(cols[i+int(len(cols)/2)])\n",
    "    results_full = results_full[new_cols]\n",
    "    red_name = \"feature_select_shift_10step\"\n",
    "    if save:\n",
    "        directory = f\"results/{red_name}\"\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        save_path = f'results/{red_name}/{best_delete}_cv{cv}_mxsamp{max_samples}_sd{seed}_ovrw{int(overwrite)}'\n",
    "        results_full.to_csv(save_path)\n",
    "    print(\"\\n\", \"\\n\", red_name, \"\\n\", results_full.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16f86dc-a1ee-4706-bc17-cbedd58ff3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "all_labels = []\n",
    "for p in [\"datasets/CRC_AUS_LOSO.csv\", \n",
    "          \"datasets/CRC_FRA_LOSO.csv\",\n",
    "         \"datasets/CRC_CHI_LOSO.csv\",\n",
    "         \"datasets/CRC_GER_LOSO.csv\",\n",
    "         #\"datasets/CRC_IND_additional.csv\",\n",
    "         \"datasets/CRC_USA_LOSO.csv\"]:\n",
    "    df = pd.read_csv(p)\n",
    "    df_binary = df.loc[(df[\"disease\"] == \"healthy\") | (df[\"disease\"]==\"CRC\")]\n",
    "    df_data = df_binary.iloc[:,4:]\n",
    "    data = df_data.to_numpy()\n",
    "    labels = df_binary[\"disease\"].to_numpy()\n",
    "    labels[labels==\"healthy\"] = 0\n",
    "    labels[labels==\"CRC\"] = 1\n",
    "    data = (1/np.sum(data, axis=1, keepdims=True))*data\n",
    "    all_data.append(data)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "all_data = np.concatenate(all_data,axis=0)\n",
    "labels = np.concatenate(all_labels)\n",
    "all_data, labels = unison_shuffled_copies(all_data, labels, seed=412)\n",
    "c1_ind = (labels==1).nonzero()[0]\n",
    "c1_del = c1_ind[:int(len(c1_ind)*0.97)]\n",
    "all_data, labels = np.delete(all_data, c1_del, axis=0), np.delete(labels, c1_del, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbafc58-eff4-4b74-b585-52c0a8cf6234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da53d28-57c8-48ba-a1e1-3080b58d3553",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = 20\n",
    "rocs = []\n",
    "f1s = []\n",
    "for i in range(0,510,10):\n",
    "    results = pd.read_csv(f'results/feature_select_shift_10step/{i}_cv{7}_mxsamp{1024}_sd{42}_ovrw{1}')\n",
    "    rocs.append(results[\"roc_auc mean\"].values)\n",
    "    f1s.append(results[\"f1 mean\"].values)\n",
    "rocs = np.array(rocs)\n",
    "f1s = np.array(f1s)\n",
    "labels = results.iloc[:,0].values\n",
    "plt.figure(figsize=(12,3), dpi=200)\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "colors = []\n",
    "for cl in range(rocs.shape[1]):\n",
    "    ax1.plot(np.arange(rocs.shape[0]-ws+1)*10, moving_average(rocs[:,cl], ws), label=labels[cl])\n",
    "    ax2.plot(np.arange(rocs.shape[0]-ws+1)*10, moving_average(f1s[:,cl], ws), label=labels[cl])\n",
    "plt.suptitle(\"ROC AUC and F1-Score shifting feature selection\")\n",
    "#ax1.set_title(\"ROC AUC\")\n",
    "ax1.set_ylabel(\"ROC AUC\")\n",
    "#ax2.set_title(\"F1-Score\")\n",
    "ax2.set_xlabel(\"feature shift\")\n",
    "ax2.set_ylabel(\"F1\")\n",
    "ax1.legend(fontsize=7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d448cf5d-91f9-4f84-9477-6fe30540037e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229c531a-d6f8-464b-bb73-476293131bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.count_nonzero(all_data, axis=0)\n",
    "counts = 1-counts/all_data.shape[0]\n",
    "means = np.mean(all_data, axis=0)\n",
    "plt.scatter(counts, means, s=1)\n",
    "plt.show()\n",
    "plt.scatter(counts, np.max(all_data, axis=0), s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216c8ae-5da2-4b93-bbde-e5bd0009e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3d2554-f847-48c5-9566-e04693fa8eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.abspath(os.getcwd())\n",
    "run_name = \"medium_net_mlp_balance_bsplit_baseline\"\n",
    "run_name = \"medium_net_mlp_balance_bsplit_baseline_longer\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "filename = \"model\"\n",
    "for sampling in [None]:#, undersample]:\n",
    "    cv = 5\n",
    "    strat_split = True\n",
    "    n_optim = 1000\n",
    "    cat_optim = 10\n",
    "    ft_epochs = 10\n",
    "    ft_lr = 1e-8\n",
    "    max_s = 1024\n",
    "    max_q = 128\n",
    "    max_samples = None\n",
    "    no_pre_process = False\n",
    "    multi_decoder = None\n",
    "    N_ens = 5\n",
    "    metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\"]\n",
    "    models = [\n",
    "        #CatBoostOptim(n_optim=cat_optim),\n",
    "        #MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder, ft_epochs=ft_epochs, ft_lr=ft_lr,\n",
    "        #                 max_s=max_s, max_q=max_q, no_preprocess_mode=no_pre_process),\n",
    "        MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "        #MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=5, no_preprocess_mode=True),\n",
    "        XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "        XGBoostOptim(n_optim=n_optim),\n",
    "        LogisticRegression(max_iter=500), \n",
    "        TabPFNClassifier(device='cpu', N_ensemble_configurations=5, no_preprocess_mode=no_pre_process),\n",
    "        TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\", max_epochs=ft_epochs)\n",
    "    ]\n",
    "    results_mean = pd.DataFrame(np.zeros((len(models), len(metrics)+1)),\n",
    "                                index=[m.__class__.__name__ for m in models],\n",
    "                                columns=metrics+[\"runtime\"])\n",
    "    results_std = pd.DataFrame(np.zeros((len(models), len(metrics)+1)), \n",
    "                               index=[m.__class__.__name__ for m in models],\n",
    "                               columns=metrics+[\"runtime\"])\n",
    "    \n",
    "    for ii, model in enumerate(models):\n",
    "        results_mean.iloc[ii,:], results_std.iloc[ii,:] = cross_validate_sample(model, data, labels, metrics, strat_split, cv, sampling, max_samples)\n",
    "    #results_sorted = results.sort_values(\"roc_auc\")\n",
    "    #print(results_sorted)\n",
    "    print(results_mean)\n",
    "    print(results_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04052993-69b3-4c87-9bf8-163357aa9fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "for m in metrics + \"runtime\":\n",
    "    cols.append(m)\n",
    "    cols.append(m+\" std\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc23d5f-155a-4e3d-b696-1fd6439a0cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dir_path = os.path.abspath(os.getcwd())\n",
    "run_name = \"medium_net_mlp_balance_bsplit_baseline\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "filename = \"model\"\n",
    "#model, config = load_model(path, filename, device=\"cpu\", eval_positions=None, verbose=0)\n",
    "#pred_model = TabPFNClassifier(model[2], config, device=\"cpu\", N_ensemble_configurations=5, no_preprocess_mode=False)\n",
    "for sampling in [None]:\n",
    "    cv = 3\n",
    "    strat_split = True\n",
    "    n_optim = 10\n",
    "    ft_epochs = 10\n",
    "    max_samples = None\n",
    "    metrics = metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\"]\n",
    "    models = [\n",
    "        RandomForestClassifier()\n",
    "        #CatBoostOptim(n_optim=n_optim),\n",
    "        #pred_model,\n",
    "        #MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=3, no_preprocess_mode=True),\n",
    "        #XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "        #XGBoostOptim(n_optim=n_optim),\n",
    "        #LogisticRegression(max_iter=500), \n",
    "        #TabPFNClassifier(device='cpu', N_ensemble_configurations=3, no_preprocess_mode=True),\n",
    "        #TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\", max_epochs=ft_epochs)\n",
    "    ]\n",
    "    results = pd.DataFrame(np.zeros((len(models), len(metrics)+1)), \n",
    "                           index=[m.__class__.__name__ for m in models],\n",
    "                          columns=metrics+[\"runtime\"])\n",
    "    \n",
    "    for ii, model in enumerate(models):\n",
    "        results.iloc[ii,:] = cross_validate_sample(model, data, labels, metrics, strat_split, cv, sampling, max_samples)\n",
    "    results_sorted = results.sort_values(\"roc_auc\")\n",
    "    \n",
    "    print(results_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5983e6a1-a7ce-4694-b775-e83498cd3be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819f6306-686d-42f1-829c-303890202785",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac970c40-6003-4c24-b454-014ba31f6b26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
