{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "716a0985-a3e5-47be-acc6-d58fb95fa8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "from tabpfn import TabPFNClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from data_prep_utils import *\n",
    "from evaluate import *\n",
    "from load_models import *\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df4242a6-93bf-44ba-ab8b-28680bb27aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 0., 0.],\n",
      "        [0., 1., 1., 1.],\n",
      "        [0., 1., 1., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 1., 1.],\n",
      "        [0., 1., 1., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randint(0,101, (10,4))\n",
    "print((a>torch.median(a)).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fd72aea-dcf4-4d2c-bc6b-63eb9ccfdd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/data_all.csv\"\n",
    "data, labels = get_microbiome(path)\n",
    "data = top_non_zero(data)\n",
    "data, labels = unison_shuffled_copies(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "879ba0a8-b675-49eb-a1dc-64e836de555b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001b[32m2024-09-13 10:48:31.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1753 | Val score: 0.9399\u001b[0m\n",
      "\u001b[32m2024-09-13 10:49:13.608\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1416 | Train score: 0.9551 | Val loss: 0.1735 | Val score: 0.9420\u001b[0m\n",
      "\u001b[32m2024-09-13 10:49:53.294\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1965 | Train score: 0.9375 | Val loss: 0.1715 | Val score: 0.9430\u001b[0m\n",
      "\u001b[32m2024-09-13 10:50:35.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1715 | Train score: 0.9395 | Val loss: 0.1707 | Val score: 0.9445\u001b[0m\n",
      "\u001b[32m2024-09-13 10:51:13.329\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1449 | Train score: 0.9473 | Val loss: 0.1721 | Val score: 0.9445\u001b[0m\n",
      "\u001b[32m2024-09-13 10:51:52.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1674 | Train score: 0.9453 | Val loss: 0.1674 | Val score: 0.9445\u001b[0m\n",
      "\u001b[32m2024-09-13 10:52:31.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1166 | Train score: 0.9648 | Val loss: 0.1674 | Val score: 0.9455\u001b[0m\n",
      "\u001b[32m2024-09-13 10:53:11.377\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1423 | Train score: 0.9551 | Val loss: 0.1774 | Val score: 0.9440\u001b[0m\n",
      "\u001b[32m2024-09-13 10:53:49.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1971 | Train score: 0.9375 | Val loss: 0.1713 | Val score: 0.9450\u001b[0m\n",
      "\u001b[32m2024-09-13 10:54:29.649\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1438 | Train score: 0.9531 | Val loss: 0.1718 | Val score: 0.9440\u001b[0m\n",
      "\u001b[32m2024-09-13 10:55:08.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1367 | Train score: 0.9492 | Val loss: 0.1748 | Val score: 0.9415\u001b[0m\n",
      "\u001b[32m2024-09-13 10:55:59.748\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1749 | Val score: 0.9440\u001b[0m\n",
      "\u001b[32m2024-09-13 10:56:36.351\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1843 | Train score: 0.9375 | Val loss: 0.1701 | Val score: 0.9445\u001b[0m\n",
      "\u001b[32m2024-09-13 10:57:15.846\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1834 | Train score: 0.9434 | Val loss: 0.1668 | Val score: 0.9455\u001b[0m\n",
      "\u001b[32m2024-09-13 10:57:54.579\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1871 | Train score: 0.9297 | Val loss: 0.1710 | Val score: 0.9420\u001b[0m\n",
      "\u001b[32m2024-09-13 10:58:33.117\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1533 | Train score: 0.9453 | Val loss: 0.1702 | Val score: 0.9430\u001b[0m\n",
      "\u001b[32m2024-09-13 10:59:09.931\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2198 | Train score: 0.9355 | Val loss: 0.1730 | Val score: 0.9415\u001b[0m\n",
      "\u001b[32m2024-09-13 10:59:47.986\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1619 | Train score: 0.9473 | Val loss: 0.1728 | Val score: 0.9450\u001b[0m\n",
      "\u001b[32m2024-09-13 11:00:28.147\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1767 | Train score: 0.9473 | Val loss: 0.1670 | Val score: 0.9440\u001b[0m\n",
      "\u001b[32m2024-09-13 11:01:05.374\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1538 | Train score: 0.9434 | Val loss: 0.1666 | Val score: 0.9450\u001b[0m\n",
      "\u001b[32m2024-09-13 11:01:43.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1739 | Train score: 0.9453 | Val loss: 0.1633 | Val score: 0.9455\u001b[0m\n",
      "\u001b[32m2024-09-13 11:02:21.743\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1662 | Train score: 0.9434 | Val loss: 0.1623 | Val score: 0.9476\u001b[0m\n",
      "\u001b[32m2024-09-13 11:03:09.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1735 | Val score: 0.9415\u001b[0m\n",
      "\u001b[32m2024-09-13 11:03:48.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1518 | Train score: 0.9473 | Val loss: 0.1671 | Val score: 0.9461\u001b[0m\n",
      "\u001b[32m2024-09-13 11:04:29.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1755 | Train score: 0.9375 | Val loss: 0.1637 | Val score: 0.9471\u001b[0m\n",
      "\u001b[32m2024-09-13 11:05:08.809\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1772 | Train score: 0.9434 | Val loss: 0.1661 | Val score: 0.9455\u001b[0m\n",
      "\u001b[32m2024-09-13 11:05:47.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1265 | Train score: 0.9629 | Val loss: 0.1635 | Val score: 0.9435\u001b[0m\n",
      "\u001b[32m2024-09-13 11:06:25.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1840 | Train score: 0.9395 | Val loss: 0.1624 | Val score: 0.9476\u001b[0m\n",
      "\u001b[32m2024-09-13 11:07:04.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1668 | Train score: 0.9395 | Val loss: 0.1647 | Val score: 0.9425\u001b[0m\n",
      "\u001b[32m2024-09-13 11:07:43.018\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1735 | Train score: 0.9434 | Val loss: 0.1601 | Val score: 0.9440\u001b[0m\n",
      "\u001b[32m2024-09-13 11:08:20.214\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2137 | Train score: 0.9316 | Val loss: 0.1653 | Val score: 0.9461\u001b[0m\n",
      "\u001b[32m2024-09-13 11:08:59.798\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1766 | Train score: 0.9395 | Val loss: 0.1679 | Val score: 0.9430\u001b[0m\n",
      "\u001b[32m2024-09-13 11:09:38.198\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1690 | Train score: 0.9453 | Val loss: 0.1673 | Val score: 0.9445\u001b[0m\n",
      "\u001b[32m2024-09-13 11:10:27.950\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1779 | Val score: 0.9405\u001b[0m\n",
      "\u001b[32m2024-09-13 11:11:08.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1736 | Train score: 0.9336 | Val loss: 0.1782 | Val score: 0.9425\u001b[0m\n",
      "\u001b[32m2024-09-13 11:11:48.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2189 | Train score: 0.9277 | Val loss: 0.1713 | Val score: 0.9450\u001b[0m\n",
      "\u001b[32m2024-09-13 11:12:28.351\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1722 | Train score: 0.9375 | Val loss: 0.1690 | Val score: 0.9445\u001b[0m\n",
      "\u001b[32m2024-09-13 11:13:07.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1614 | Train score: 0.9473 | Val loss: 0.1701 | Val score: 0.9445\u001b[0m\n",
      "\u001b[32m2024-09-13 11:13:46.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1703 | Train score: 0.9473 | Val loss: 0.1775 | Val score: 0.9445\u001b[0m\n",
      "\u001b[32m2024-09-13 11:14:25.915\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1966 | Train score: 0.9395 | Val loss: 0.1717 | Val score: 0.9491\u001b[0m\n",
      "\u001b[32m2024-09-13 11:15:05.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1459 | Train score: 0.9531 | Val loss: 0.1786 | Val score: 0.9430\u001b[0m\n",
      "\u001b[32m2024-09-13 11:15:44.000\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1713 | Train score: 0.9453 | Val loss: 0.1742 | Val score: 0.9425\u001b[0m\n",
      "\u001b[32m2024-09-13 11:16:24.030\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1496 | Train score: 0.9512 | Val loss: 0.1781 | Val score: 0.9435\u001b[0m\n",
      "\u001b[32m2024-09-13 11:17:01.852\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1567 | Train score: 0.9355 | Val loss: 0.1747 | Val score: 0.9410\u001b[0m\n",
      "\u001b[32m2024-09-13 11:17:49.857\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1722 | Val score: 0.9415\u001b[0m\n",
      "\u001b[32m2024-09-13 11:18:28.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1859 | Train score: 0.9355 | Val loss: 0.1644 | Val score: 0.9415\u001b[0m\n",
      "\u001b[32m2024-09-13 11:19:05.254\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1614 | Train score: 0.9434 | Val loss: 0.1606 | Val score: 0.9440\u001b[0m\n",
      "\u001b[32m2024-09-13 11:19:41.835\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1466 | Train score: 0.9473 | Val loss: 0.1693 | Val score: 0.9430\u001b[0m\n",
      "\u001b[32m2024-09-13 11:20:22.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2110 | Train score: 0.9316 | Val loss: 0.1634 | Val score: 0.9455\u001b[0m\n",
      "\u001b[32m2024-09-13 11:20:59.102\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1726 | Train score: 0.9453 | Val loss: 0.1621 | Val score: 0.9445\u001b[0m\n",
      "\u001b[32m2024-09-13 11:21:35.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1988 | Train score: 0.9375 | Val loss: 0.1638 | Val score: 0.9445\u001b[0m\n",
      "\u001b[32m2024-09-13 11:22:11.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1540 | Train score: 0.9453 | Val loss: 0.1698 | Val score: 0.9430\u001b[0m\n",
      "\u001b[32m2024-09-13 11:22:50.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1882 | Train score: 0.9395 | Val loss: 0.1648 | Val score: 0.9435\u001b[0m\n",
      "\u001b[32m2024-09-13 11:23:28.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1691 | Train score: 0.9355 | Val loss: 0.1677 | Val score: 0.9430\u001b[0m\n",
      "\u001b[32m2024-09-13 11:24:07.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1855 | Train score: 0.9316 | Val loss: 0.1681 | Val score: 0.9440\u001b[0m\n",
      "\u001b[32m2024-09-13 11:24:55.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1726 | Val score: 0.9430\u001b[0m\n",
      "\u001b[32m2024-09-13 11:25:31.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1971 | Train score: 0.9355 | Val loss: 0.1682 | Val score: 0.9445\u001b[0m\n",
      "\u001b[32m2024-09-13 11:26:10.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1970 | Train score: 0.9297 | Val loss: 0.1640 | Val score: 0.9461\u001b[0m\n",
      "\u001b[32m2024-09-13 11:26:49.806\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1859 | Train score: 0.9355 | Val loss: 0.1643 | Val score: 0.9486\u001b[0m\n",
      "\u001b[32m2024-09-13 11:27:27.284\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1983 | Train score: 0.9258 | Val loss: 0.1640 | Val score: 0.9445\u001b[0m\n",
      "\u001b[32m2024-09-13 11:28:04.759\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1928 | Train score: 0.9297 | Val loss: 0.1677 | Val score: 0.9455\u001b[0m\n",
      "\u001b[32m2024-09-13 11:28:44.299\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1668 | Train score: 0.9453 | Val loss: 0.1667 | Val score: 0.9506\u001b[0m\n",
      "\u001b[32m2024-09-13 11:29:21.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1540 | Train score: 0.9531 | Val loss: 0.1598 | Val score: 0.9476\u001b[0m\n",
      "\u001b[32m2024-09-13 11:29:59.294\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1893 | Train score: 0.9277 | Val loss: 0.1587 | Val score: 0.9491\u001b[0m\n",
      "\u001b[32m2024-09-13 11:30:37.705\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1713 | Train score: 0.9336 | Val loss: 0.1584 | Val score: 0.9506\u001b[0m\n",
      "\u001b[32m2024-09-13 11:31:17.095\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1735 | Train score: 0.9375 | Val loss: 0.1600 | Val score: 0.9461\u001b[0m\n",
      "\u001b[32m2024-09-13 11:32:07.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1655 | Val score: 0.9415\u001b[0m\n",
      "\u001b[32m2024-09-13 11:32:48.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1568 | Train score: 0.9531 | Val loss: 0.1571 | Val score: 0.9445\u001b[0m\n",
      "\u001b[32m2024-09-13 11:33:27.419\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1714 | Train score: 0.9434 | Val loss: 0.1593 | Val score: 0.9440\u001b[0m\n",
      "\u001b[32m2024-09-13 11:34:07.027\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1745 | Train score: 0.9414 | Val loss: 0.1552 | Val score: 0.9501\u001b[0m\n",
      "\u001b[32m2024-09-13 11:34:44.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1837 | Train score: 0.9375 | Val loss: 0.1579 | Val score: 0.9466\u001b[0m\n",
      "\u001b[32m2024-09-13 11:35:20.968\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1843 | Train score: 0.9316 | Val loss: 0.1629 | Val score: 0.9491\u001b[0m\n",
      "\u001b[32m2024-09-13 11:35:58.189\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1419 | Train score: 0.9609 | Val loss: 0.1559 | Val score: 0.9517\u001b[0m\n",
      "\u001b[32m2024-09-13 11:36:35.689\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1684 | Train score: 0.9434 | Val loss: 0.1598 | Val score: 0.9481\u001b[0m\n",
      "\u001b[32m2024-09-13 11:37:13.753\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1599 | Train score: 0.9492 | Val loss: 0.1531 | Val score: 0.9486\u001b[0m\n",
      "\u001b[32m2024-09-13 11:37:51.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1476 | Train score: 0.9551 | Val loss: 0.1563 | Val score: 0.9466\u001b[0m\n",
      "\u001b[32m2024-09-13 11:38:30.175\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1718 | Train score: 0.9434 | Val loss: 0.1531 | Val score: 0.9481\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "cv = 7\n",
    "n_optim = 1000\n",
    "ft_epochs = 10\n",
    "sampling = None\n",
    "metrics = metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\"]\n",
    "models = [\n",
    "    XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "    XGBoostOptim(n_optim=n_optim),\n",
    "    LogisticRegression(max_iter=500), \n",
    "    TabPFNClassifier(device='cpu', N_ensemble_configurations=3),\n",
    "    TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\")\n",
    "]\n",
    "results = pd.DataFrame(np.zeros((len(models), len(metrics))), \n",
    "                       index=[m.__class__.__name__ for m in models],\n",
    "                      columns=metrics)\n",
    "\n",
    "for ii, model in enumerate(models):\n",
    "    results.iloc[ii,:] = cross_validate_sample(model, data, labels, metrics, cv, sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5983e6a1-a7ce-4694-b775-e83498cd3be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        accuracy  precision    recall   roc_auc\n",
      "XGBClassifier           0.928273   0.372057  0.235522  0.604521\n",
      "XGBoostOptim            0.947731   0.780016  0.208970  0.602537\n",
      "LogisticRegression      0.936039   0.396803  0.059431  0.526420\n",
      "TabPFNClassifier        0.938918   0.238095  0.008360  0.504087\n",
      "TabForestPFNClassifier  0.945200   0.679909  0.211052  0.602137\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
