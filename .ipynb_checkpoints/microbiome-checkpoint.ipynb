{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "716a0985-a3e5-47be-acc6-d58fb95fa8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from tabpfn_new.scripts.transformer_prediction_interface import TabPFNClassifier, MedPFNClassifier\n",
    "from tabpfn_new.scripts.model_builder import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from data_prep_utils import *\n",
    "from evaluate import *\n",
    "from load_models import *\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import openml\n",
    "import time\n",
    "import matplotlib as mpl\n",
    "#from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0e72742-f9d3-4748-939e-0aee908e04c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/data_all.csv\"\n",
    "all_data, labels = get_microbiome(path)\n",
    "all_data = remove_zero_features(all_data)\n",
    "all_data, labels = unison_shuffled_copies(all_data, labels, seed=45)\n",
    "#all_data = top_anova(all_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1e999e-e823-415e-992c-45f39c827e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_name =\"fi-loco\"\n",
    "m = \"rocs\"\n",
    "file1 = np.load(f\"results/{red_name}/extranew_{m}1.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6fcab7-a90e-43bf-9076-791ec84ae3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.argsort(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d07adc-6cab-4451-a081-23442a94c23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.delete(all_data, best[:500], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10a5130-cda2-47ab-a5c2-70a2840cfa1d",
   "metadata": {},
   "source": [
    "## Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adae1ec9-3ead-4f70-8f11-8936c9783725",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save = False\n",
    "sampling = None\n",
    "cv = 10\n",
    "best_delete = 0\n",
    "strat_split = True\n",
    "recomp = False\n",
    "n_optim = 1000\n",
    "cat_optim = 10\n",
    "ft_epochs = 40\n",
    "ft_epochs_forest = 10\n",
    "ft_lr = 1e-5\n",
    "max_s = 1024\n",
    "max_q = 128\n",
    "max_samples = 1024\n",
    "preprocess_mode = 'mix'\n",
    "multi_decoder = \"permutation\"\n",
    "N_ens = 7\n",
    "seed = 42\n",
    "overwrite = True\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\", \"f1\"]\n",
    "dir_path = os.path.abspath(os.getcwd())\n",
    "#run_name = \"medium_net_mlp_balance_bsplit_baseline\" ## NO PERM, VERY ROBUST\n",
    "#run_name2 = \"large_mlp_var_balance_05weight_anova\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_newprior\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_newprior\" ## das aller bestigste\n",
    "run_name = \"small_mlp_var_balance_05weight_anova_bestwithnoisebnn_100\" ## good start, best with ft\n",
    "#run_name = \"large_mlp_var_balance_05weight_anova_bestwithnoisebnn_100_moreweight\" ## best start, ft no so good\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_200\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_longer\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_balance_lownoisefixparam_anova\"\n",
    "#run_name = \"medium_mlp_0.5static_balance_anova\"\n",
    "#run_name = \"large_mlp_fullbalance\"\n",
    "#run_name = \"small_net_mlp_var_imbalance_05weight\" ## WORKS VERY WELL WITH PERM\n",
    "#run_name = \"small_net_mlp_varbalance_weight_batchunisplit\"\n",
    "#run_name = \"small_net_mlp_variable_balance_weight\"\n",
    "#run_name = \"small_net_mlp_balance_minevalup\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "#path2 = dir_path + f\"/logs/trainrun_{run_name2}\"\n",
    "#path3 = dir_path + f\"/logs/trainrun_{run_name3}\"\n",
    "filename = \"model\"\n",
    "\n",
    "\n",
    "\n",
    "models = [\n",
    "    MajorityClass(),\n",
    "    XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "    #MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=1, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  preprocess_mode=preprocess_mode),\n",
    "    #MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  preprocess_mode=preprocess_mode,\n",
    "    #                ft_epochs=ft_epochs, ft_lr=ft_lr),\n",
    "    RandomForestClassifier(),\n",
    "    #AutoGluon(),\n",
    "    #CatBoostGrid(),\n",
    "    #XGBoostGrid(),\n",
    "    LogisticRegressionClassifier(), \n",
    "    #TabPFNClassifier(device='cpu', N_ensemble_configurations=5, no_preprocess_mode=no_pre_process),\n",
    "    #TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\", max_epochs=ft_epochs_forest)\n",
    "]\n",
    "reducer  = AnovaSelect(k=5)\n",
    "\n",
    "\n",
    "results_mean = pd.DataFrame(np.zeros((len(models), len(metrics)+1)),\n",
    "                            index=[m.__class__.__name__ for m in models],\n",
    "                            columns=metrics+[\"runtime\"])\n",
    "results_std = pd.DataFrame(np.zeros((len(models), len(metrics)+1)), \n",
    "                           index=[m.__class__.__name__ for m in models],\n",
    "                           columns=metrics+[\"runtime\"])\n",
    "\n",
    "for ii, model in enumerate(models):\n",
    "    results_mean.iloc[ii,:], results_std.iloc[ii,:] = cross_validate_sample(\n",
    "        model, all_data, labels, metrics, strat_split, cv, sampling, \n",
    "        reducer, max_samples, seed=seed, overwrite=overwrite, n_best_delete=best_delete,\n",
    "        recomp=recomp)\n",
    "\n",
    "results_mean = results_mean.add_suffix(\" mean\")\n",
    "results_std = results_std.add_suffix(\" std\")\n",
    "results_full = pd.concat((results_mean, results_std), axis=1)\n",
    "cols = results_full.columns.tolist()\n",
    "new_cols = []\n",
    "for i in range(int(len(cols)/2)):\n",
    "    new_cols.append(cols[i])\n",
    "    new_cols.append(cols[i+int(len(cols)/2)])\n",
    "results_full = results_full[new_cols]\n",
    "red_name = reducer.__class__.__name__\n",
    "if save:\n",
    "    directory = f\"results/{red_name}\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    save_path = f'results/{red_name}/baseline_cv{cv}_mxsamp{max_samples}_sd{seed}_ovrw{int(overwrite)}.csv'\n",
    "    results_full.to_csv(save_path)\n",
    "print(\"\\n\", results_full.round(3))\n",
    "#print(results_full.sort_values(\"roc_auc mean\").round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6c7211-581b-426c-b21f-eaa6988627f1",
   "metadata": {},
   "source": [
    "## Baseline Repeated cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cbf3b4-c737-4652-a371-ec8d3608a1e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save = False\n",
    "sampling = None\n",
    "cv = 10\n",
    "best_delete = 0\n",
    "strat_split = True\n",
    "recomp = False\n",
    "n_optim = 1000\n",
    "cat_optim = 10\n",
    "ft_epochs = 10\n",
    "ft_epochs_forest = 10\n",
    "ft_lr = 1e-5\n",
    "max_s = 1024\n",
    "max_q = 128\n",
    "max_samples = 1024\n",
    "preprocess_mode = 'mix'\n",
    "multi_decoder = \"permutation\"\n",
    "N_ens = 5\n",
    "seed = 42\n",
    "overwrite = True\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\", \"f1\"]\n",
    "dir_path = os.path.abspath(os.getcwd())\n",
    "#run_name = \"medium_net_mlp_balance_bsplit_baseline\" ## NO PERM, VERY ROBUST\n",
    "#run_name2 = \"large_mlp_var_balance_05weight_anova\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_newprior\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_newprior\" ## das aller bestigste\n",
    "run_name = \"small_mlp_var_balance_05weight_anova_bestwithnoisebnn_100\" ## good start, best with ft\n",
    "run_name = \"medium_mlp_var_balance_05weight_anova_bestwithnoisebnn_100\" ## good start, best with ft\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_bestwithnoisebnn_100_moreweight\" ## best start, ft no so good\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_200\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_longer\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_balance_lownoisefixparam_anova\"\n",
    "#run_name = \"medium_mlp_0.5static_balance_anova\"\n",
    "#run_name = \"large_mlp_fullbalance\"\n",
    "#run_name = \"small_net_mlp_var_imbalance_05weight\" ## WORKS VERY WELL WITH PERM\n",
    "#run_name = \"small_net_mlp_varbalance_weight_batchunisplit\"\n",
    "#run_name = \"small_net_mlp_variable_balance_weight\"\n",
    "#run_name = \"small_net_mlp_balance_minevalup\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "#path2 = dir_path + f\"/logs/trainrun_{run_name2}\"\n",
    "#path3 = dir_path + f\"/logs/trainrun_{run_name3}\"\n",
    "filename = \"model\"\n",
    "\n",
    "\n",
    "\n",
    "models = [\n",
    "    MajorityClass(),\n",
    "    XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=1, multiclass_decoder=multi_decoder,  preprocess_mode=preprocess_mode),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  preprocess_mode=preprocess_mode),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  preprocess_mode=preprocess_mode,\n",
    "                    ft_epochs=ft_epochs, ft_lr=ft_lr),\n",
    "    RandomForestClassifier(),\n",
    "    #AutoGluon(),\n",
    "    #CatBoostGrid(),\n",
    "    #XGBoostGrid(),\n",
    "    LogisticRegressionClassifier(), \n",
    "    #TabPFNClassifier(device='cpu', N_ensemble_configurations=5, no_preprocess_mode=no_pre_process),\n",
    "    #TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\", max_epochs=ft_epochs_forest)\n",
    "]\n",
    "reducer  = AnovaSelect()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "results_mean = pd.DataFrame(np.zeros((len(models), len(metrics)+1)),\n",
    "                            index=[m.__class__.__name__ for m in models],\n",
    "                            columns=metrics+[\"runtime\"])\n",
    "results_std = pd.DataFrame(np.zeros((len(models), len(metrics)+1)), \n",
    "                           index=[m.__class__.__name__ for m in models],\n",
    "                           columns=metrics+[\"runtime\"])\n",
    "\n",
    "size = min(labels.shape[0],np.floor(max_samples*(cv/(cv-1))))\n",
    "runs = int(all_data.shape[0]//size)\n",
    "data_sections, label_sections = stratified_split(all_data, labels,cv=runs)\n",
    "\n",
    "\n",
    "\n",
    "for section in range(runs):\n",
    "    print(\"Step: \", section)\n",
    "    for ii, model in enumerate(models):\n",
    "        out_mean, out_std = cross_validate_sample(\n",
    "            model, data_sections[section], label_sections[section], metrics, strat_split, cv, sampling, \n",
    "            reducer, max_samples, seed=seed, overwrite=overwrite, n_best_delete=best_delete,\n",
    "            recomp=recomp)\n",
    "        results_mean.iloc[ii,:] += out_mean\n",
    "        results_std.iloc[ii,:] += out_std \n",
    "    \n",
    "results_mean = results_mean.add_suffix(\" mean\")\n",
    "results_std = results_std.add_suffix(\" std\")\n",
    "results_full = pd.concat((results_mean, results_std), axis=1)\n",
    "cols = results_full.columns.tolist()\n",
    "new_cols = []\n",
    "for i in range(int(len(cols)/2)):\n",
    "    new_cols.append(cols[i])\n",
    "    new_cols.append(cols[i+int(len(cols)/2)])\n",
    "results_full = results_full[new_cols]\n",
    "results_full = results_full/runs\n",
    "red_name = \"repeated_cv\"\n",
    "if save:\n",
    "    directory = f\"results/{red_name}\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    save_path = f'results/{red_name}/2ndedit_baseline_cv{cv}_mxsamp{max_samples}_sd{seed}_ovrw{int(overwrite)}.csv'\n",
    "    results_full.to_csv(save_path)\n",
    "print(\"\\n\", \"\\n\", red_name, \"\\n\", results_full.round(3))\n",
    "#print(results_full.sort_values(\"roc_auc mean\").round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6a78a6-8bac-461d-b9e3-8f2ad2a2fb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f'results/repeated_cv/2ndedit_baseline_cv10_mxsamp1024_sd42_ovrw1.csv'\n",
    "data = pd.read_csv(save_path)\n",
    "#data.round(3).to_csv(f'results/repeated_cv/rounded_2ndedit_baseline_cv10_mxsamp1024_sd42_ovrw1.csv')\n",
    "rows = [2,3,4,5,6,7,8,9,10]\n",
    "names = [r\"MedPFN-$N_1$\", r\"MedPFN-$N_{10}$\", r\"MedPFN-$N_{10}FT$\", \"Random Forest\", \"CatBoost\", \"XGBoost\", \"Logistic Regression\", \"TabPFN\", \"TabForestPFN\"]\n",
    "times = data.iloc[:,-2]\n",
    "print(np.sum(times)*100/3600)\n",
    "data = data.iloc[rows,:]\n",
    "print(data)\n",
    "fig, axs = plt.subplots(3, 1, figsize=(8, 5), dpi=100, sharex=True)\n",
    "print()\n",
    "# Plot error bar plots in each subplot\n",
    "models = data.iloc[:,0].values\n",
    "colors = [\"gold\", \"darkorange\", \"crimson\", \"yellowgreen\", \"teal\", \"royalblue\", \"purple\", \"forestgreen\", \"olive\"]\n",
    "#print(models)\n",
    "x = np.arange(len(data.iloc[:,1].values))\n",
    "for i in range(len(models)):\n",
    "    axs[0].set_xticks([])\n",
    "    axs[0].set_xlim(-1,len(rows))\n",
    "    axs[0].errorbar(x[i], data.iloc[i,1], yerr=data.iloc[i,2], fmt='+', mew=2, lw=2, markersize=12, capsize=0, color=colors[i], label=names[i])\n",
    "    axs[0].set_ylabel(\"Accuracy\")\n",
    "    \n",
    "    axs[1].errorbar(x[i], data.iloc[i,7], yerr=data.iloc[i,8], fmt='+', mew=2, lw=2, markersize=12, capsize=0, color=colors[i])\n",
    "    axs[1].set_ylabel(\"ROC AUC\")\n",
    "    #axs[1].legend()\n",
    "    \n",
    "    axs[2].errorbar(x[i], data.iloc[i,9], yerr=data.iloc[i,10], fmt='+', mew=2, lw=2, markersize=12, capsize=0, color=colors[i])\n",
    "    axs[2].set_ylabel(r\"$F_1$-score\")\n",
    "    #axs[2].legend()\n",
    "    \n",
    "    #axs[3].errorbar(x[i], data.iloc[i,7], yerr=data.iloc[i,8], fmt='+', mew=2, lw=2, markersize=12, capsize=0, color=colors[i])\n",
    "    #axs[3].set_ylabel(\"\")\n",
    "    #axs[3].legend()\n",
    "    \n",
    "    # Set common x-axis label\n",
    "    #axs[3].set_xlabel(\"X-axis\")\n",
    "\n",
    "inds = [1,7,9]\n",
    "for j in range(3):\n",
    "    #print(data.columns[1+2*j])\n",
    "    #print(data.iloc[:,1+2*j].values,np.argmax(data.iloc[:,1+2*j].values))\n",
    "    axs[j].hlines(np.max(data.iloc[:,inds[j]].values),-1,len(rows), lw=1, colors=colors[np.argmax(data.iloc[:,inds[j]].values)], linestyles=\"dashed\")\n",
    "    #axs[j].grid()\n",
    "axs[j].hlines(0,0,0,linestyles=\"dashed\", colors=[\"gray\"], label=\"Highest (color of winner)\")\n",
    "axs[0].set_ylim(0.9,0.97)\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1),\n",
    "          ncol=3, fancybox=True, shadow=True)\n",
    "# Adjust layout to prevent overlap\n",
    "#plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\"results/plots/baselinecompare.eps\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4906d353-1d35-4139-babd-0a320b4bc376",
   "metadata": {},
   "source": [
    "## Mine vs static vs normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31180d61-b1bb-4e55-a0ad-c1b64197bb49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save = True\n",
    "sampling = None\n",
    "cv = 10\n",
    "best_delete = 0\n",
    "strat_split = True\n",
    "recomp = False\n",
    "n_optim = 1000\n",
    "cat_optim = 10\n",
    "ft_epochs = 20\n",
    "ft_epochs_forest = 10\n",
    "ft_lr = 1e-5\n",
    "max_s = 1024\n",
    "max_q = 128\n",
    "max_samples = 1024\n",
    "no_pre_process = False\n",
    "multi_decoder = \"permutation\"\n",
    "N_ens = 5\n",
    "seed = 42\n",
    "overwrite = True\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\", \"f1\"]\n",
    "dir_path = os.path.abspath(os.getcwd())\n",
    "run_name = \"small_mlp_var_balance_05weight_anova_bestwithnoisebnn_100\" \n",
    "run_name2 = \"small_mlp_static_balance_05weight_anova_bestwithnoisebnn_100\" \n",
    "run_name3 = \"small_mlp_var_balance_05weight_anova_bestwithnoisebnn_100_normalprior\" \n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "path2 = dir_path + f\"/logs/trainrun_{run_name2}\"\n",
    "path3 = dir_path + f\"/logs/trainrun_{run_name3}\"\n",
    "filename = \"model\"\n",
    "\n",
    "\n",
    "models = [\n",
    "    \n",
    "    #MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "    #MedPFNClassifier(base_path=path2, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "    #MedPFNClassifier(base_path=path3, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  preprocess_mode=preprocess_mode, ft_epochs=ft_epochs, ft_lr=ft_lr),\n",
    "    MedPFNClassifier(base_path=path2, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  preprocess_mode=preprocess_mode,ft_epochs=ft_epochs, ft_lr=ft_lr),\n",
    "    MedPFNClassifier(base_path=path3, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  preprocess_mode=preprocess_mode,ft_epochs=ft_epochs, ft_lr=ft_lr),\n",
    "]\n",
    "reducer  = AnovaSelect()\n",
    "#for reducer in [AnovaSelect(), RandomSelect(), NonZeroSelect(), MeanSelect(), StdSelect(), MaxSelect(), PCASelect()]:\n",
    "#data = clr(all_data)\n",
    "\n",
    "#all_data = normalize(all_data)\n",
    "\n",
    "results_mean = pd.DataFrame(np.zeros((len(models), len(metrics)+1)),\n",
    "                            index=[m.__class__.__name__ for m in models],\n",
    "                            columns=metrics+[\"runtime\"])\n",
    "results_std = pd.DataFrame(np.zeros((len(models), len(metrics)+1)), \n",
    "                           index=[m.__class__.__name__ for m in models],\n",
    "                           columns=metrics+[\"runtime\"])\n",
    "\n",
    "for ii, model in enumerate(models):\n",
    "    results_mean.iloc[ii,:], results_std.iloc[ii,:] = cross_validate_sample(\n",
    "        model, all_data, labels, metrics, strat_split, cv, sampling, \n",
    "        reducer, max_samples, seed=seed, overwrite=overwrite, n_best_delete=best_delete,\n",
    "        recomp=recomp)\n",
    "\n",
    "results_mean = results_mean.add_suffix(\" mean\")\n",
    "results_std = results_std.add_suffix(\" std\")\n",
    "results_full = pd.concat((results_mean, results_std), axis=1)\n",
    "cols = results_full.columns.tolist()\n",
    "new_cols = []\n",
    "for i in range(int(len(cols)/2)):\n",
    "    new_cols.append(cols[i])\n",
    "    new_cols.append(cols[i+int(len(cols)/2)])\n",
    "results_full = results_full[new_cols]\n",
    "red_name = \"nomethodscompare\"\n",
    "if save:\n",
    "    directory = f\"results/{red_name}\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    save_path = f'results/{red_name}/results_1mine_2staticbal_3normalprior.csv'\n",
    "    results_full.to_csv(save_path)\n",
    "print(\"\\n\", \"\\n\", red_name, \"\\n\", results_full.round(3))\n",
    "#print(results_full.sort_values(\"roc_auc mean\").round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c65c8e1-8f50-4735-bb45-352f0fb9f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f'results/nomethodscompare/baseline_cv10_mxsamp1024_sd42_ovrw1.csv'\n",
    "df = pd.read_csv(save_path)\n",
    "save_path = f'results/nomethodscompare/rounded_baseline_cv10_mxsamp1024_sd42_ovrw1.csv'\n",
    "df.round(3).to_csv(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b718491-a7e4-46c1-aeb7-c443398e4379",
   "metadata": {},
   "source": [
    "## Context length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604773e7-27c7-4417-93a3-204f30d38e5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save = True\n",
    "sampling = None\n",
    "cv = 10\n",
    "best_delete = 0\n",
    "strat_split = True\n",
    "recomp = False\n",
    "n_optim = 1000\n",
    "cat_optim = 10\n",
    "ft_epochs = 20\n",
    "ft_epochs_forest = 10\n",
    "ft_lr = 1e-5\n",
    "max_s = 1024\n",
    "max_q = 128\n",
    "max_samples = 1024\n",
    "no_pre_process = False\n",
    "multi_decoder = \"permutation\"\n",
    "N_ens = 5\n",
    "seed = 42\n",
    "overwrite = True\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\", \"f1\"]\n",
    "dir_path = os.path.abspath(os.getcwd())\n",
    "#run_name = \"medium_net_mlp_balance_bsplit_baseline\" ## NO PERM, VERY ROBUST\n",
    "#run_name2 = \"large_mlp_var_balance_05weight_anova\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_newprior\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_newprior\" ## das aller bestigste\n",
    "run_name = \"medium_mlp_var_balance_05weight_anova_bestwithnoisebnn_100\" ## good start, best with ft\n",
    "#run_name3 = \"medium_mlp_var_balance_05weight_anova_bestwithnoisebnn_100_moreweight\" ## best start, ft no so good\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_200\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_longer\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_balance_lownoisefixparam_anova\"\n",
    "#run_name = \"medium_mlp_0.5static_balance_anova\"\n",
    "#run_name = \"large_mlp_fullbalance\"\n",
    "#run_name = \"small_net_mlp_var_imbalance_05weight\" ## WORKS VERY WELL WITH PERM\n",
    "#run_name = \"small_net_mlp_varbalance_weight_batchunisplit\"\n",
    "#run_name = \"small_net_mlp_variable_balance_weight\"\n",
    "#run_name = \"small_net_mlp_balance_minevalup\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "#path2 = dir_path + f\"/logs/trainrun_{run_name2}\"\n",
    "#path3 = dir_path + f\"/logs/trainrun_{run_name3}\"\n",
    "filename = \"model\"\n",
    "\n",
    "lengths = [512,1024,2048,4096,8192]\n",
    "\n",
    "models = [\n",
    "    MajorityClass(),\n",
    "    XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=1, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process,\n",
    "                    ft_epochs=ft_epochs, ft_lr=ft_lr),\n",
    "    RandomForestClassifier(),\n",
    "    #AutoGluon(),\n",
    "    CatBoostGrid(),\n",
    "    XGBoostGrid(),\n",
    "    LogisticRegressionClassifier(), \n",
    "    TabPFNClassifier(device='cpu', N_ensemble_configurations=5, no_preprocess_mode=no_pre_process),\n",
    "    TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\", max_epochs=ft_epochs_forest)\n",
    "]\n",
    "reducer  = AnovaSelect()\n",
    "#for reducer in [AnovaSelect(), RandomSelect(), NonZeroSelect(), MeanSelect(), StdSelect(), MaxSelect(), PCASelect()]:\n",
    "#data = clr(all_data)\n",
    "\n",
    "#all_data = normalize(all_data)\n",
    "\n",
    "for cl in lengths:\n",
    "    results_mean = pd.DataFrame(np.zeros((len(models), len(metrics)+1)),\n",
    "                                index=[m.__class__.__name__ for m in models],\n",
    "                                columns=metrics+[\"runtime\"])\n",
    "    results_std = pd.DataFrame(np.zeros((len(models), len(metrics)+1)), \n",
    "                               index=[m.__class__.__name__ for m in models],\n",
    "                               columns=metrics+[\"runtime\"])\n",
    "    \n",
    "    for ii, model in enumerate(models):\n",
    "        results_mean.iloc[ii,:], results_std.iloc[ii,:] = cross_validate_sample(\n",
    "            model, all_data, labels, metrics, strat_split, cv, sampling, \n",
    "            reducer, cl, seed=seed, overwrite=overwrite, n_best_delete=best_delete,\n",
    "            recomp=recomp)\n",
    "    \n",
    "    results_mean = results_mean.add_suffix(\" mean\")\n",
    "    results_std = results_std.add_suffix(\" std\")\n",
    "    results_full = pd.concat((results_mean, results_std), axis=1)\n",
    "    cols = results_full.columns.tolist()\n",
    "    new_cols = []\n",
    "    for i in range(int(len(cols)/2)):\n",
    "        new_cols.append(cols[i])\n",
    "        new_cols.append(cols[i+int(len(cols)/2)])\n",
    "    results_full = results_full[new_cols]\n",
    "    red_name = \"context_length\"\n",
    "    if save:\n",
    "        directory = f\"results/{red_name}\"\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        save_path = f'results/{red_name}/context_length_{cl}.csv'\n",
    "        results_full.to_csv(save_path)\n",
    "    print(\"\\n\", \"\\n\", red_name, \"\\n\", results_full.round(3))\n",
    "    #print(results_full.sort_values(\"roc_auc mean\").round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b4b8c5-0c53-43b5-96a7-3eec013f6415",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = [\"Majclass\", \"MedPFN-1\", \"MedPFN-7\", \"MedPFN-7FT\", \"RandomForest\", \"CatBoost\", \"XGBoost\", \"Logistic Regression\", \"TabPFN\", \"TabForestPFN\"]\n",
    "fig, axs = plt.subplots(1,1,figsize=(10,5))#plt.subplots(1, 3, figsize=(25, 5))\n",
    "all_accuracies = []\n",
    "all_rocs = []\n",
    "all_f1s = []\n",
    "lengths = [512,1024,2048,4096,8192]\n",
    "for m in range(1,len(models)):\n",
    "    accuracies = []\n",
    "    rocs = []\n",
    "    f1s = []\n",
    "    for cl in lengths:\n",
    "    \n",
    "        path = f'results/context_length/context_length_{cl}.csv'\n",
    "        df = pd.read_csv(path)\n",
    "        accuracies.append(df.iloc[m,1])\n",
    "        rocs.append(df.iloc[m,7])\n",
    "        f1s.append(df.iloc[m,9])\n",
    "    all_accuracies.append(accuracies)\n",
    "    all_rocs.append(rocs)\n",
    "    all_f1s.append(f1s)\n",
    "    \n",
    "#axs[0].plot(all_accuracies[3])\n",
    "#axs[1].plot(all_rocs[3])\n",
    "#axs[2].plot(all_f1s[2])\n",
    "mean_accuracies = all_accuracies[3]-np.max(np.array(all_accuracies)[4:],axis=0)\n",
    "#axs.plot(fracs, mean_accuracies)\n",
    "mean_rocs = all_rocs[3]-np.max(np.array(all_rocs)[4:],axis=0)\n",
    "#axs.plot(fracs, mean_rocs)\n",
    "mean_f1s = all_f1s[3]-np.max(np.array(all_f1s)[4:],axis=0)\n",
    "#axs.plot(fracs, mean_f1s)\n",
    "axs.plot(lengths, np.zeros(len(lengths)), linestyle='--', c=\"black\", label=\"Win threshhold\")\n",
    "axs.plot(lengths, mean_rocs, marker=\"v\", linewidth=2.5, markersize=7.0, label='ROC AUC', c=\"darkviolet\")\n",
    "axs.plot(lengths, mean_accuracies, marker=\"v\", linewidth=2.5, markersize=7.0, label='Accuracy', c=\"crimson\")\n",
    "axs.plot(lengths, mean_f1s, marker=\"v\", linewidth=2.5, markersize=7.0, label='F1-score', c=\"limegreen\")\n",
    "#axs.set_yticks([-0.05,0,0.05,0.1])\n",
    "#axs.set_xlim(0.5,0.94)\n",
    "axs.set_xlabel(\"Fraction of class 0 in data\")\n",
    "axs.set_ylabel(\"MedPFN - mean[baselines]\")\n",
    "axs.grid()\n",
    "axs.legend(fontsize=12)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "'''df = pd.read_csv(path)\n",
    "        accuracies.append(df.iloc[m,1])\n",
    "        rocs.append(df.iloc[m,7])\n",
    "        f1s.append(df.iloc[m,9])\n",
    "    axs[0].plot(lengths, accuracies)\n",
    "    axs[1].plot(lengths, rocs, label = models[m])\n",
    "    axs[2].plot(lengths, f1s)\n",
    "    \n",
    "axs[1].legend(fontsize=12)\n",
    "fig.show()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc866d4c-75ed-4715-a25d-b0cb8caf5d63",
   "metadata": {},
   "source": [
    "## Imbalance anaysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2fcae7-9f0c-4d93-a6a2-b2b6bf4bcf8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save = True\n",
    "sampling = None\n",
    "cv = 10\n",
    "best_delete = 0\n",
    "strat_split = True\n",
    "recomp = False\n",
    "n_optim = 1000\n",
    "cat_optim = 10\n",
    "ft_epochs = 20\n",
    "ft_epochs_forest = 10\n",
    "ft_lr = 1e-5\n",
    "max_s = 1024\n",
    "max_q = 128\n",
    "max_samples = 1024\n",
    "no_pre_process = False\n",
    "multi_decoder = \"permutation\"\n",
    "N_ens = 5\n",
    "seed = 42\n",
    "overwrite = True\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\", \"f1\"]\n",
    "dir_path = os.path.abspath(os.getcwd())\n",
    "#run_name = \"medium_net_mlp_balance_bsplit_baseline\" ## NO PERM, VERY ROBUST\n",
    "#run_name2 = \"large_mlp_var_balance_05weight_anova\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_newprior\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_newprior\" ## das aller bestigste\n",
    "run_name = \"medium_mlp_var_balance_05weight_anova_bestwithnoisebnn_100\" ## good start, best with ft\n",
    "#run_name3 = \"medium_mlp_var_balance_05weight_anova_bestwithnoisebnn_100_moreweight\" ## best start, ft no so good\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_200\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_longer\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_balance_lownoisefixparam_anova\"\n",
    "#run_name = \"medium_mlp_0.5static_balance_anova\"\n",
    "#run_name = \"large_mlp_fullbalance\"\n",
    "#run_name = \"small_net_mlp_var_imbalance_05weight\" ## WORKS VERY WELL WITH PERM\n",
    "#run_name = \"small_net_mlp_varbalance_weight_batchunisplit\"\n",
    "#run_name = \"small_net_mlp_variable_balance_weight\"\n",
    "#run_name = \"small_net_mlp_balance_minevalup\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "#path2 = dir_path + f\"/logs/trainrun_{run_name2}\"\n",
    "#path3 = dir_path + f\"/logs/trainrun_{run_name3}\"\n",
    "filename = \"model\"\n",
    "\n",
    "\n",
    "\n",
    "models = [\n",
    "    MajorityClass(),\n",
    "    XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=1, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process,\n",
    "                    ft_epochs=ft_epochs, ft_lr=ft_lr),\n",
    "    RandomForestClassifier(),\n",
    "    #AutoGluon(),\n",
    "    CatBoostGrid(),\n",
    "    XGBoostGrid(),\n",
    "    LogisticRegressionClassifier(), \n",
    "    TabPFNClassifier(device='cpu', N_ensemble_configurations=5, no_preprocess_mode=no_pre_process),\n",
    "    TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\", max_epochs=ft_epochs_forest)\n",
    "]\n",
    "reducer  = AnovaSelect()\n",
    "#for reducer in [AnovaSelect(), RandomSelect(), NonZeroSelect(), MeanSelect(), StdSelect(), MaxSelect(), PCASelect()]:\n",
    "#data = clr(all_data)\n",
    "\n",
    "#all_data = normalize(all_data)\n",
    "\n",
    "frac = [0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99]\n",
    "for f in frac:\n",
    "    data_c0 = all_data[labels==0]\n",
    "    data_c1 = all_data[labels==1]\n",
    "    num_c1 = data_c1.shape[0]\n",
    "    num_c0 = min(int(num_c1*f/(1-f)),data_c0.shape[0])\n",
    "    num_c1 = min(num_c1, int(num_c0*(1-f)/f))\n",
    "    print(num_c1,num_c0)\n",
    "    data = np.concatenate((data_c0[:num_c0], data_c1[:num_c1]), axis=0)\n",
    "    print(data.shape)\n",
    "    labels_new = np.concatenate((np.zeros(num_c0), np.ones(num_c1)))\n",
    "    data_new, labels_new = unison_shuffled_copies(data, labels_new)\n",
    "    counts = np.unique(labels_new, return_counts=True)[1]\n",
    "    print(counts[0]/np.sum(counts))\n",
    "    results_mean = pd.DataFrame(np.zeros((len(models), len(metrics)+1)),\n",
    "                                index=[m.__class__.__name__ for m in models],\n",
    "                                columns=metrics+[\"runtime\"])\n",
    "    results_std = pd.DataFrame(np.zeros((len(models), len(metrics)+1)), \n",
    "                               index=[m.__class__.__name__ for m in models],\n",
    "                               columns=metrics+[\"runtime\"])\n",
    "    \n",
    "    for ii, model in enumerate(models):\n",
    "        results_mean.iloc[ii,:], results_std.iloc[ii,:] = cross_validate_sample(\n",
    "            model, data_new, labels_new, metrics, strat_split, cv, sampling, \n",
    "            reducer, max_samples, seed=seed, overwrite=overwrite, n_best_delete=best_delete,\n",
    "            recomp=recomp)\n",
    "    \n",
    "    results_mean = results_mean.add_suffix(\" mean\")\n",
    "    results_std = results_std.add_suffix(\" std\")\n",
    "    results_full = pd.concat((results_mean, results_std), axis=1)\n",
    "    cols = results_full.columns.tolist()\n",
    "    new_cols = []\n",
    "    for i in range(int(len(cols)/2)):\n",
    "        new_cols.append(cols[i])\n",
    "        new_cols.append(cols[i+int(len(cols)/2)])\n",
    "    results_full = results_full[new_cols]\n",
    "    red_name = \"imbalance\"\n",
    "    if save:\n",
    "        directory = f\"results/{red_name}\"\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        save_path = f'results/{red_name}/majclass_{f}.csv'\n",
    "        results_full.to_csv(save_path)\n",
    "    print(\"\\n\", \"\\n\", red_name, \"\\n\", results_full.round(3))\n",
    "    #print(results_full.sort_values(\"roc_auc mean\").round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb97879e-b2c6-4571-ab1d-c0ec7cba47ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imb = all_data[labels==0].shape[0]/(all_data[labels==0].shape[0]+all_data[labels==1].shape[0])\n",
    "print(data_imb)\n",
    "#models = [\"Majclass\", \"MedPFN-1\", \"MedPFN-7\", \"MedPFN-7FT\", \"RandomForest\", \"CatBoost\", \"XGBoost\", \"Logistic Regression\", \"TabPFN\", \"TabForestPFN\"]\n",
    "models = [\"XGBoost\", \"MedPFN-Full\", \"MedPFN-Balance\", \"Random Forest\",\"Logistic Regression\", \"TabPFN\"]\n",
    "fig, axs = plt.subplots(1, 1, figsize=(10, 5))\n",
    "all_accuracies = []\n",
    "all_rocs = []\n",
    "all_f1s = []\n",
    "fracs = [0.5,0.6,0.7,0.8,0.85,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99]\n",
    "for m in range(0,len(models)):\n",
    "    accuracies = []\n",
    "    rocs = []\n",
    "    f1s = []\n",
    "    for f in fracs:\n",
    "    \n",
    "        path = f'results/imbalance/new_majclass_{f}.csv'\n",
    "        df = pd.read_csv(path)\n",
    "        accuracies.append(df.iloc[m,1])\n",
    "        rocs.append(df.iloc[m,7])\n",
    "        f1s.append(df.iloc[m,9])\n",
    "    all_accuracies.append(accuracies)\n",
    "    all_rocs.append(rocs)\n",
    "    all_f1s.append(f1s)\n",
    "    \n",
    "#axs[0].plot(all_accuracies[3])\n",
    "#axs[1].plot(all_rocs[3])\n",
    "#axs[2].plot(all_f1s[2])\n",
    "base = [0,3,5]\n",
    "mean_accuracies = all_accuracies[1]-np.mean(np.array(all_accuracies)[base],axis=0)\n",
    "#axs.plot(fracs, mean_accuracies)\n",
    "mean_rocs = all_rocs[1]-np.mean(np.array(all_rocs)[base],axis=0)\n",
    "#axs.plot(fracs, mean_rocs)\n",
    "mean_f1s = all_f1s[1]-np.mean(np.array(all_f1s)[base],axis=0)\n",
    "#axs.plot(fracs, mean_f1s)\n",
    "colors = [\"teal\", \"darkorange\", \"purple\"]\n",
    "axs.plot(fracs, np.zeros(len(fracs)), linestyle='--', c=\"black\", label=\"Win threshhold\")\n",
    "axs.vlines(data_imb,-0.075,0.25, linestyles=\"dashed\", colors=\"crimson\", label=\"Original data\")\n",
    "axs.plot(fracs, mean_rocs, marker=\"v\", linewidth=2.5, markersize=7.0, label='ROC AUC', c=\"teal\")\n",
    "axs.plot(fracs, mean_accuracies, marker=\"v\", linewidth=2.5, markersize=7.0, label='Accuracy', c=\"darkorange\")\n",
    "axs.plot(fracs, mean_f1s, marker=\"v\", linewidth=2.5, markersize=7.0, label='F1-score', c=\"purple\")\n",
    "#for ii, m in enumerate(models):\n",
    "\n",
    "\n",
    "mpl.rcParams['axes.linewidth'] = 1.5\n",
    "heights = [1,1,1]\n",
    "fig1,ax = plt.subplots(3,1,figsize=(12,10), gridspec_kw={'height_ratios': heights}, sharex=True, dpi=100)\n",
    "plt.subplots_adjust(hspace=0)\n",
    "styles = [\"solid\",'dashed', \"dotted\"]\n",
    "for s, ii in enumerate([1,2,5]):\n",
    "    ax[1].plot(fracs, all_accuracies[ii], marker=\".\", linewidth=2.5, markersize=7.0, linestyle=styles[s], label=models[ii], c=\"darkorange\")\n",
    "    ax[0].plot(fracs, all_rocs[ii], marker=\".\", linewidth=2.5, markersize=7.0, linestyle=styles[s], label=models[ii], c=\"teal\")\n",
    "    ax[2].plot(fracs, all_f1s[ii], marker=\".\", linewidth=2.5, markersize=7.0, linestyle=styles[s], label=models[ii], c=\"purple\")\n",
    "#axs.plot(fracs, all_rocs[5], marker=\"v\", linewidth=2.5, markersize=7.0, label='Accuracy', c=\"darkorange\")\n",
    "metrics = [\"ROC AUC\", \"Accuracy\", r\"$F_1$-score\"]\n",
    "for x in range(3):\n",
    "    ax[x].grid()\n",
    "    ax[x].set_ylabel(metrics[x], fontsize=15)\n",
    "#axs.set_yticks([-0.05,0,0.05,0.1])\n",
    "axs.set_ylim(-0.05,0.2)\n",
    "axs.set_xlim(0.5,0.99)\n",
    "axs.set_xticks([0.5,0.6,0.7,0.8,0.9,0.95])\n",
    "ax[0].set_xlim(0.5,0.99)\n",
    "ax[0].set_xticks([0.5,0.6,0.7,0.8,0.9,0.95])\n",
    "ax[0].legend(loc=(0.35,0.025), fontsize=15)\n",
    "ax[2].set_xlabel(\"Fraction of class 0 in data\", fontsize=15)\n",
    "axs.set_xlabel(\"Fraction of class 0 in data\", fontsize=15)\n",
    "axs.set_ylabel(\"MedPFN - mean[baselines]\", fontsize=15)\n",
    "axs.grid()\n",
    "axs.legend(fontsize=15)\n",
    "fig.savefig(\"results/plots/imbalance.eps\")\n",
    "fig1.show()\n",
    "fig1.savefig(\"results/plots/imbalance2.eps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92b57f2-8a3b-41ad-98b6-235cd6a2454f",
   "metadata": {},
   "source": [
    "## Ensemble testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730e1ff3-52fd-4746-b51e-7cce8cc46c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = True\n",
    "sampling = None\n",
    "cv = 10\n",
    "best_delete = 0\n",
    "strat_split = True\n",
    "recomp = False\n",
    "n_optim = 1000\n",
    "cat_optim = 10\n",
    "ft_epochs = 0\n",
    "ft_lr = 1e-5\n",
    "max_s = 1024\n",
    "max_q = 128\n",
    "max_samples = 1024\n",
    "no_pre_process = False\n",
    "multi_decoder = \"permutation\"\n",
    "N_ens = 5\n",
    "seed = 42\n",
    "overwrite = True\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\", \"f1\"]\n",
    "dir_path = os.path.abspath(os.getcwd())\n",
    "run_name = \"small_mlp_var_balance_05weight_anova_bestwithnoisebnn_100\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "filename = \"model\"\n",
    "\n",
    "\n",
    "enses = [25,40,50]\n",
    "ens_num = len(enses)\n",
    "\n",
    "\n",
    "reducer  = AnovaSelect()\n",
    "\n",
    "results_mean = pd.DataFrame(np.zeros((ens_num, len(metrics)+1)),\n",
    "                            index=np.arange(1,1+ens_num),\n",
    "                            columns=metrics+[\"runtime\"])\n",
    "results_std = pd.DataFrame(np.zeros((ens_num, len(metrics)+1)),\n",
    "                           index=np.arange(1,1+ens_num),\n",
    "                           columns=metrics+[\"runtime\"])\n",
    "\n",
    "for nens in range(1,ens_num+1):\n",
    "    #print(nens)\n",
    "    model = MedPFNClassifier(base_path=path, filename=filename, device=device, N_ensemble_configurations=enses[nens-1], multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process)\n",
    "\n",
    "    size = min(labels.shape[0],np.floor(max_samples*(cv/(cv-1))))\n",
    "    runs = int(all_data.shape[0]//size)\n",
    "    data_sections, label_sections = stratified_split(all_data, labels,cv=runs)\n",
    "    #print(np.array(data_sections).shape, size)\n",
    "\n",
    "    for section in range(runs):\n",
    "        out_mean, out_std = cross_validate_sample(\n",
    "            model, data_sections[section], label_sections[section], metrics, strat_split, cv, sampling,\n",
    "            reducer, max_samples, seed=seed, overwrite=overwrite, n_best_delete=best_delete,\n",
    "            recomp=recomp)\n",
    "        results_mean.iloc[nens-1,:] += out_mean\n",
    "        results_std.iloc[nens-1,:] += out_std\n",
    "\n",
    "###\n",
    "results_mean=results_mean/runs\n",
    "results_std=results_std/runs\n",
    "results_mean = results_mean.add_suffix(\" mean\")\n",
    "results_std = results_std.add_suffix(\" std\")\n",
    "results_full = pd.concat((results_mean, results_std), axis=1)\n",
    "cols = results_full.columns.tolist()\n",
    "new_cols = []\n",
    "for i in range(int(len(cols)/2)):\n",
    "    new_cols.append(cols[i])\n",
    "    new_cols.append(cols[i+int(len(cols)/2)])\n",
    "results_full = results_full[new_cols]\n",
    "red_name = \"ensemble\"\n",
    "if save:\n",
    "    directory = f\"results/{red_name}\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    save_path = f'results/{red_name}/medpfn_maxens{ens_num}'\n",
    "    results_full.to_csv(save_path)\n",
    "print(\"\\n\", \"\\n\", red_name, \"\\n\", results_full.round(3))\n",
    "#print(results_full.sort_values(\"roc_auc mean\").round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90749abf-81cc-4bbe-8f85-6f3326524d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_num = 25\n",
    "results_full = pd.read_csv(f'results/ensemble/medpfn_maxens{25}')\n",
    "#for i in range(ens_num):\n",
    "x = np.arange(1,1+ens_num)\n",
    "acc = results_full.iloc[:,1]\n",
    "acc_error = results_full.iloc[:,2]\n",
    "roc = results_full.iloc[:,7]\n",
    "roc_error = results_full.iloc[:,8]\n",
    "f1 = results_full.iloc[:,9]\n",
    "f1_error = results_full.iloc[:,10]\n",
    "\n",
    "heights = [1,1,1]\n",
    "mpl.rcParams['axes.linewidth'] = 1.5\n",
    "fig, ax = plt.subplots(3, 1, figsize=(8, 8), gridspec_kw={'height_ratios': heights},sharex=True)\n",
    "\n",
    "colors = [\"teal\", \"darkorange\", \"purple\"]\n",
    "#ax[0].fill_between(x, roc - roc_error, roc + roc_error, alpha=0.25, color=colors[0])\n",
    "#ax[1].fill_between(x, acc - acc_error, acc + acc_error, alpha=0.25,color=colors[1])\n",
    "#ax[2].fill_between(x, f1 - f1_error, f1 + f1_error, alpha=0.25, color=colors[2])\n",
    "ax[1].plot(x, acc, marker=\"v\", linewidth=2.5, markersize=7.0, label='Accuracy', c=colors[1])\n",
    "ax[0].plot(x, roc, marker=\"v\", linewidth=2.5, markersize=7.0, label='ROC AUC', c=colors[0])\n",
    "ax[2].plot(x, f1, marker=\"v\", linewidth=2.5, markersize=7.0, label='F1-score', color=colors[2])\n",
    "\n",
    "#ax[0].set_ylim(0.85,0.95)\n",
    "#ax[1].set_ylim(0.945,0.955)\n",
    "#ax[2].set_ylim(0.15,0.35)\n",
    "ax[0].set_yticks([0.82,0.84,0.86])\n",
    "ax[1].set_yticks([0.945,0.95,0.955])\n",
    "ax[2].set_yticks([0.38,0.4,0.42])\n",
    "# Adding labels and title\n",
    "ax[0].set_ylabel('ROC AUC', fontsize=15)\n",
    "ax[1].set_ylabel('Accuracy', fontsize=15)\n",
    "ax[2].set_ylabel('$F_1$-score', fontsize=15)\n",
    "ax[2].set_xlabel('Ensemble size', fontsize=15)\n",
    "ax[2].set_xticks([1,5,10,15,20])\n",
    "#plt.yticks([0.3,0.6,0.9])\n",
    "ax[2].set_xlim(1,20)\n",
    "#plt.ylim(0,1)\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "ax[2].grid()\n",
    "#plt.ylabel('Metric score')\n",
    "#plt.title('Plot with Uncertainty Band')\n",
    "fig.legend(loc=(0.7,0.7), fontsize=15)\n",
    "plt.subplots_adjust(hspace=0)\n",
    "#plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\"results/plots/ensemble.eps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c0856f-0ec2-4a8d-8cc9-b8c0cccf7f2c",
   "metadata": {},
   "source": [
    "## Finetuning analysis ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a73c8e1-1139-4049-a448-03f5060243ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save = True\n",
    "sampling = None\n",
    "cv = 10\n",
    "best_delete = 0\n",
    "strat_split = True\n",
    "recomp = False\n",
    "n_optim = 1000\n",
    "cat_optim = 10\n",
    "ft_epochs = 10\n",
    "ft_lr = 1e-5\n",
    "max_s = 1024\n",
    "max_q = 128\n",
    "max_samples = 1024\n",
    "preprocess = \"mix\"\n",
    "multi_decoder = \"permutation\"\n",
    "N_ens = 5\n",
    "seed = 42\n",
    "overwrite = True\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\", \"f1\"]\n",
    "dir_path = os.path.abspath(os.getcwd())\n",
    "run_name = \"small_mlp_var_balance_05weight_anova_bestwithnoisebnn_100\" ## good start, best with ftpath = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "filename = \"model\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "ens_num = 10\n",
    "\n",
    "    \n",
    "reducer  = AnovaSelect()\n",
    "\n",
    "results_mean = pd.DataFrame(np.zeros((ens_num, len(metrics)+1)),\n",
    "                            index=np.arange(1,1+ens_num),\n",
    "                            columns=metrics+[\"runtime\"])\n",
    "results_std = pd.DataFrame(np.zeros((ens_num, len(metrics)+1)), \n",
    "                           index=np.arange(1,1+ens_num),\n",
    "                           columns=metrics+[\"runtime\"])\n",
    "\n",
    "lrs = np.arange(0,105,5)\n",
    "size = min(labels.shape[0],np.floor(max_samples*(cv/(cv-1))))\n",
    "runs = int(all_data.shape[0]//size)\n",
    "data_sections, label_sections = stratified_split(all_data, labels,cv=runs)\n",
    "\n",
    "\n",
    "for ens in range(len(lrs)):\n",
    "    print(ens)\n",
    "    for section in range(runs):\n",
    "    #print(section)\n",
    "        model = MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  preprocess_mode=preprocess,\n",
    "                ft_epochs=lrs[ens], ft_lr=ft_lr)\n",
    "        out_mean, out_std = cross_validate_sample(\n",
    "            model, data_sections[section], label_sections[section], metrics, strat_split, cv, sampling, \n",
    "            reducer, max_samples, seed=seed, overwrite=overwrite, n_best_delete=best_delete,\n",
    "            recomp=recomp)\n",
    "        results_mean.iloc[ens,:] += out_mean\n",
    "        results_std.iloc[ens,:] += out_std\n",
    "results_mean=results_mean/runs\n",
    "results_std=results_std/runs\n",
    "    \n",
    "results_mean = results_mean.add_suffix(\" mean\")\n",
    "results_std = results_std.add_suffix(\" std\")\n",
    "results_full = pd.concat((results_mean, results_std), axis=1)\n",
    "cols = results_full.columns.tolist()\n",
    "new_cols = []\n",
    "for i in range(int(len(cols)/2)):\n",
    "    new_cols.append(cols[i])\n",
    "    new_cols.append(cols[i+int(len(cols)/2)])\n",
    "results_full = results_full[new_cols]\n",
    "red_name = \"finetuning\"\n",
    "if save:\n",
    "    directory = f\"results/{red_name}\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    save_path = f'results/{red_name}/2new_medpfn_maxsteps{ens_num}_lr{ft_lr}'\n",
    "    results_full.to_csv(save_path)\n",
    "print(\"\\n\", \"\\n\", red_name, \"\\n\", results_full.round(3))\n",
    "    #print(results_full.sort_values(\"roc_auc mean\").round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea4aae-125f-4d43-af6b-bfaffa9674c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_num = 21\n",
    "results_full = pd.read_csv(f'results/finetuning/2new_medpfn_maxsteps{ens_num}_lr{1e-05}')\n",
    "#for i in range(ens_num):\n",
    "x = np.arange(0,ens_num)*5\n",
    "acc = results_full.iloc[:,1]\n",
    "acc_error = results_full.iloc[:,2]\n",
    "roc = results_full.iloc[:,7]\n",
    "roc_error = results_full.iloc[:,8]\n",
    "f1 = results_full.iloc[:,9]\n",
    "f1_error = results_full.iloc[:,10]\n",
    "\n",
    "heights = [1,1,1]\n",
    "mpl.rcParams['axes.linewidth'] = 1.5\n",
    "fig, ax = plt.subplots(3, 1, figsize=(8, 8), gridspec_kw={'height_ratios': heights},sharex=True)\n",
    "\n",
    "colors = [\"teal\", \"darkorange\", \"purple\"]\n",
    "#ax[0].fill_between(x, roc - roc_error, roc + roc_error, alpha=0.25, color=colors[0])\n",
    "#ax[1].fill_between(x, acc - acc_error, acc + acc_error, alpha=0.25,color=colors[1])\n",
    "#ax[2].fill_between(x, f1 - f1_error, f1 + f1_error, alpha=0.25, color=colors[2])\n",
    "ax[1].plot(x, acc, marker=\"v\", linewidth=2.5, markersize=7.0, label='Accuracy', c=colors[1])\n",
    "ax[0].plot(x, roc, marker=\"v\", linewidth=2.5, markersize=7.0, label='ROC AUC', c=colors[0])\n",
    "ax[2].plot(x, f1, marker=\"v\", linewidth=2.5, markersize=7.0, label='F1-score', color=colors[2])\n",
    "\n",
    "#ax[0].set_ylim(0.9,0.94)\n",
    "ax[1].set_ylim(0.94,0.96)\n",
    "#ax[2].set_ylim(0.2,0.6)\n",
    "#ax[0].set_yticks([0.9,0.92,0.94])\n",
    "ax[1].set_yticks([0.945,0.95, 0.955])\n",
    "#ax[2].set_yticks([0.2,0.4])\n",
    "# Adding labels and title\n",
    "ax[2].set_xlabel('Finetuning iterations')\n",
    "ax[0].set_ylabel('ROC AUC', fontsize=15)\n",
    "ax[1].set_ylabel('Accuracy', fontsize=15)\n",
    "ax[2].set_ylabel('$F_1$-score', fontsize=15)\n",
    "#ax[2].set_xticks(np.arange(0,50,10))\n",
    "#plt.yticks([0.3,0.6,0.9])\n",
    "ax[2].set_xlim(0,100)\n",
    "#plt.ylim(0,1)\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "ax[2].grid()\n",
    "#plt.ylabel('Metric score')\n",
    "#plt.title('Plot with Uncertainty Band')\n",
    "fig.legend(loc=(0.7,0.65), fontsize=15)\n",
    "plt.subplots_adjust(hspace=0)\n",
    "#plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\"results/plots/finetuning.eps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29f154f-c835-46c7-bc4f-729e8a6bb353",
   "metadata": {},
   "source": [
    "### Feature removal ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3f2fce-a23e-4370-9c28-15a0e7f6170d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save = True\n",
    "sampling = None\n",
    "cv = 10\n",
    "best_delete = 0\n",
    "strat_split = True\n",
    "recomp = False\n",
    "n_optim = 1000\n",
    "cat_optim = 10\n",
    "ft_epochs = 20\n",
    "ft_epochs_forest = 10\n",
    "ft_lr = 1e-5\n",
    "max_s = 1024\n",
    "max_q = 128\n",
    "max_samples = 1024\n",
    "no_pre_process = False\n",
    "multi_decoder = \"permutation\"\n",
    "N_ens = 5\n",
    "seed = 42\n",
    "overwrite = True\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\", \"f1\"]\n",
    "dir_path = os.path.abspath(os.getcwd())\n",
    "run_name = \"medium_mlp_var_balance_05weight_anova_bestwithnoisebnn_100\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "filename = \"model\"\n",
    "\n",
    "\n",
    "models = [\n",
    "    MajorityClass(),\n",
    "    #XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=1, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process,\n",
    "                    ft_epochs=ft_epochs, ft_lr=ft_lr),\n",
    "    RandomForestClassifier(),\n",
    "    #AutoGluon(),\n",
    "    #CatBoostGrid(),\n",
    "    #XGBoostGrid(),\n",
    "    LogisticRegressionClassifier(), \n",
    "    TabPFNClassifier(device='cpu', N_ensemble_configurations=5, no_preprocess_mode=no_pre_process),\n",
    "    TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\", max_epochs=ft_epochs_forest)\n",
    "]\n",
    "reducer  = AnovaSelect()\n",
    "#for reducer in [AnovaSelect(), RandomSelect(), NonZeroSelect(), MeanSelect(), StdSelect(), MaxSelect(), PCASelect()]:\n",
    "#data = clr(all_data)\n",
    "\n",
    "#all_data = normalize(all_data)\n",
    "to_delete = [0,1,5,10,25,50,100,250,500]\n",
    "for best_delete in to_delete:\n",
    "    results_mean = pd.DataFrame(np.zeros((len(models), len(metrics)+1)),\n",
    "                                index=[m.__class__.__name__ for m in models],\n",
    "                                columns=metrics+[\"runtime\"])\n",
    "    results_std = pd.DataFrame(np.zeros((len(models), len(metrics)+1)), \n",
    "                               index=[m.__class__.__name__ for m in models],\n",
    "                               columns=metrics+[\"runtime\"])\n",
    "    \n",
    "    for ii, model in enumerate(models):\n",
    "        results_mean.iloc[ii,:], results_std.iloc[ii,:] = cross_validate_sample(\n",
    "            model, all_data, labels, metrics, strat_split, cv, sampling, \n",
    "            reducer, max_samples, seed=seed, overwrite=overwrite, n_best_delete=best_delete,\n",
    "            recomp=recomp)\n",
    "    \n",
    "    results_mean = results_mean.add_suffix(\" mean\")\n",
    "    results_std = results_std.add_suffix(\" std\")\n",
    "    results_full = pd.concat((results_mean, results_std), axis=1)\n",
    "    cols = results_full.columns.tolist()\n",
    "    new_cols = []\n",
    "    for i in range(int(len(cols)/2)):\n",
    "        new_cols.append(cols[i])\n",
    "        new_cols.append(cols[i+int(len(cols)/2)])\n",
    "    results_full = results_full[new_cols]\n",
    "    red_name = \"feature_removal\"\n",
    "    if save:\n",
    "        directory = f\"results/{red_name}\"\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        save_path = f'results/{red_name}/best_delete{best_delete}.csv'\n",
    "        results_full.to_csv(save_path)\n",
    "    print(\"\\n\", \"\\n\", best_delete, \"\\n\", results_full.round(3))\n",
    "    #print(results_full.sort_values(\"roc_auc mean\").round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef7a442-b698-44d5-869a-756ec1e70b55",
   "metadata": {},
   "source": [
    "## New data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2ab1872-e271-44ac-b463-9ee4fa7127d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/external_pdac.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df_data = df.iloc[:,2:-5]\n",
    "data = df_data.to_numpy()\n",
    "labels = df[\"Disease\"].to_numpy()\n",
    "labels[labels==\"Healthy\"] = 0\n",
    "labels[labels==\"PDAC\"] = 1\n",
    "all_data = data\n",
    "data_c0 = data[labels==0]\n",
    "data_c1 = data[labels==1]\n",
    "num_c1 = data_c1.shape[0]\n",
    "num_c0 = int(num_c1*0.05)\n",
    "#data = np.concatenate((data_c0[:num_c0], data_c1), axis=0)\n",
    "#labels = np.concatenate((np.zeros(num_c0), np.ones(num_c1)))\n",
    "#all_data, labels = unison_shuffled_copies(data, labels)\n",
    "all_data = data_to_comp(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5860c111-1280-470e-a332-3945d589858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "sampling = None\n",
    "cv = 10\n",
    "best_delete = 0\n",
    "strat_split = True\n",
    "recomp = False\n",
    "n_optim = 1000\n",
    "cat_optim = 10\n",
    "ft_epochs = 20\n",
    "ft_lr = 1e-5\n",
    "max_s = 1024\n",
    "max_q = 128\n",
    "max_samples = 1024\n",
    "preprocess = \"false\"\n",
    "multi_decoder = \"permutation\"\n",
    "N_ens = 5\n",
    "seed = 42\n",
    "overwrite = True\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\", \"f1\"]\n",
    "dir_path = os.path.abspath(os.getcwd())\n",
    "run_name = \"medium_net_mlp_balance_bsplit_baseline\" ## NO PERM, VERY ROBUST\n",
    "#run_name2 = \"large_mlp_var_balance_05weight_anova\" ## das aller bestigste\n",
    "run_name = \"medium_mlp_var_balance_05weight_anova_bestwithnoisebnn_100\" ## das aller bestigste\n",
    "run_name2 = \"small_mlp_var_balance_05weight_anova_bestwithnoisebnn_100\" ## das aller bestigste ## das aller bestigste\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_200\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_longer\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_balance_lownoisefixparam_anova\"\n",
    "#run_name = \"medium_mlp_0.5static_balance_anova\"\n",
    "#run_name = \"large_mlp_fullbalance\"\n",
    "#run_name = \"small_net_mlp_var_imbalance_05weight\" ## WORKS VERY WELL WITH PERM\n",
    "#run_name = \"small_net_mlp_varbalance_weight_batchunisplit\"\n",
    "#run_name = \"small_net_mlp_variable_balance_weight\"\n",
    "#run_name = \"small_net_mlp_balance_minevalup\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "path2 = dir_path + f\"/logs/trainrun_{run_name2}\"\n",
    "filename = \"model\"\n",
    "\n",
    "\n",
    "models = [\n",
    "    #CatBoostOptim(n_optim=cat_optim),\n",
    "    MajorityClass(),\n",
    "    XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  preprocess_mode=preprocess_mode),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  preprocess_mode=preprocess_mode, ft_epochs=ft_epochs, ft_lr=ft_lr),\n",
    "    MedPFNClassifier(base_path=path2, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  preprocess_mode=preprocess_mode),\n",
    "    MedPFNClassifier(base_path=path2, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  preprocess_mode=preprocess_mode, ft_epochs=ft_epochs, ft_lr=ft_lr),\n",
    "    RandomForestClassifier(),\n",
    "    #AutoGluon(),\n",
    "    #CatBoostGrid(),\n",
    "    #XGBoostGrid(),\n",
    "    #XGBoostOptim(n_optim=n_optim),\n",
    "    LogisticRegression(max_iter=1000), \n",
    "    TabPFNClassifier(device='cpu', N_ensemble_configurations=5, no_preprocess_mode=False),\n",
    "    #TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\", max_epochs=ft_epochs)\n",
    "]\n",
    "\n",
    "for reducer in [AnovaSelect(), RandomSelect(), NonZeroSelect(), MeanSelect(), StdSelect(), MaxSelect(), PCASelect()]:\n",
    "    results_mean = pd.DataFrame(np.zeros((len(models), len(metrics)+1)),\n",
    "                                index=[m.__class__.__name__ for m in models],\n",
    "                                columns=metrics+[\"runtime\"])\n",
    "    results_std = pd.DataFrame(np.zeros((len(models), len(metrics)+1)), \n",
    "                               index=[m.__class__.__name__ for m in models],\n",
    "                               columns=metrics+[\"runtime\"])\n",
    "    \n",
    "    for ii, model in enumerate(models):\n",
    "        results_mean.iloc[ii,:], results_std.iloc[ii,:] = cross_validate_sample(\n",
    "            model, all_data, labels, metrics, strat_split, cv, sampling, \n",
    "            reducer, max_samples, seed=seed, overwrite=overwrite, n_best_delete=best_delete,\n",
    "            recomp=recomp)\n",
    "\n",
    "    results_mean = results_mean.add_suffix(\" mean\")\n",
    "    results_std = results_std.add_suffix(\" std\")\n",
    "    results_full = pd.concat((results_mean, results_std), axis=1)\n",
    "    cols = results_full.columns.tolist()\n",
    "    new_cols = []\n",
    "    for i in range(int(len(cols)/2)):\n",
    "        new_cols.append(cols[i])\n",
    "        new_cols.append(cols[i+int(len(cols)/2)])\n",
    "    results_full = results_full[new_cols]\n",
    "    red_name = reducer.__class__.__name__\n",
    "    if save:\n",
    "        directory = f\"results/{red_name}\"\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        save_path = f'results/{red_name}/baseline_cv{cv}_mxsamp{max_samples}_sd{seed}_ovrw{int(overwrite)}'\n",
    "        results_full.to_csv(save_path)\n",
    "    print(\"\\n\", \"\\n\", red_name, \"\\n\", results_full.round(3))\n",
    "    #print(results_full.sort_values(\"roc_auc mean\").round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef9e5ef-c542-4525-8e60-6ae0ad51b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "sampling = None\n",
    "cv = 10\n",
    "best_delete = 0\n",
    "strat_split = True\n",
    "recomp = False\n",
    "n_optim = 1000\n",
    "cat_optim = 10\n",
    "ft_epochs = 20\n",
    "ft_epochs_forest = 10\n",
    "ft_lr = 1e-5\n",
    "max_s = 1024\n",
    "max_q = 128\n",
    "max_samples = 1024\n",
    "no_pre_process = False\n",
    "multi_decoder = \"permutation\"\n",
    "N_ens = 5\n",
    "seed = 42\n",
    "overwrite = True\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\", \"f1\"]\n",
    "dir_path = os.path.abspath(os.getcwd())\n",
    "#run_name = \"medium_net_mlp_balance_bsplit_baseline\" ## NO PERM, VERY ROBUST\n",
    "#run_name2 = \"large_mlp_var_balance_05weight_anova\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_newprior\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_newprior\" ## das aller bestigste\n",
    "run_name = \"medium_mlp_var_balance_05weight_anova_bestwithnoisebnn_100\" ## good start, best with ft\n",
    "#run_name3 = \"medium_mlp_var_balance_05weight_anova_bestwithnoisebnn_100_moreweight\" ## best start, ft no so good\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_200\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_longer\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_balance_lownoisefixparam_anova\"\n",
    "#run_name = \"medium_mlp_0.5static_balance_anova\"\n",
    "#run_name = \"large_mlp_fullbalance\"\n",
    "#run_name = \"small_net_mlp_var_imbalance_05weight\" ## WORKS VERY WELL WITH PERM\n",
    "#run_name = \"small_net_mlp_varbalance_weight_batchunisplit\"\n",
    "#run_name = \"small_net_mlp_variable_balance_weight\"\n",
    "#run_name = \"small_net_mlp_balance_minevalup\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "#path2 = dir_path + f\"/logs/trainrun_{run_name2}\"\n",
    "#path3 = dir_path + f\"/logs/trainrun_{run_name3}\"\n",
    "filename = \"model\"\n",
    "\n",
    "\n",
    "models = [\n",
    "    MajorityClass(),\n",
    "    XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=1, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process,\n",
    "                    ft_epochs=ft_epochs, ft_lr=ft_lr),\n",
    "    RandomForestClassifier(),\n",
    "    #AutoGluon(),\n",
    "    #CatBoostGrid(),\n",
    "    #XGBoostGrid(),\n",
    "    LogisticRegressionClassifier(), \n",
    "    #TabPFNClassifier(device='cpu', N_ensemble_configurations=5, no_preprocess_mode=no_pre_process),\n",
    "    #TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\", max_epochs=ft_epochs_forest)\n",
    "]\n",
    "reducer  = AnovaSelect()\n",
    "#for reducer in [AnovaSelect(), RandomSelect(), NonZeroSelect(), MeanSelect(), StdSelect(), MaxSelect(), PCASelect()]:\n",
    "#data = clr(all_data)\n",
    "\n",
    "#all_data = normalize(all_data)\n",
    "\n",
    "results_mean = pd.DataFrame(np.zeros((len(models), len(metrics)+1)),\n",
    "                            index=[m.__class__.__name__ for m in models],\n",
    "                            columns=metrics+[\"runtime\"])\n",
    "results_std = pd.DataFrame(np.zeros((len(models), len(metrics)+1)), \n",
    "                           index=[m.__class__.__name__ for m in models],\n",
    "                           columns=metrics+[\"runtime\"])\n",
    "\n",
    "for ii, model in enumerate(models):\n",
    "    results_mean.iloc[ii,:], results_std.iloc[ii,:] = cross_validate_sample(\n",
    "        model, all_data, labels, metrics, strat_split, cv, sampling, \n",
    "        reducer, max_samples, seed=seed, overwrite=overwrite, n_best_delete=best_delete,\n",
    "        recomp=recomp)\n",
    "\n",
    "results_mean = results_mean.add_suffix(\" mean\")\n",
    "results_std = results_std.add_suffix(\" std\")\n",
    "results_full = pd.concat((results_mean, results_std), axis=1)\n",
    "cols = results_full.columns.tolist()\n",
    "new_cols = []\n",
    "for i in range(int(len(cols)/2)):\n",
    "    new_cols.append(cols[i])\n",
    "    new_cols.append(cols[i+int(len(cols)/2)])\n",
    "results_full = results_full[new_cols]\n",
    "red_name = reducer.__class__.__name__\n",
    "if save:\n",
    "    directory = f\"results/{red_name}\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    save_path = f'results/{red_name}/baseline_cv{cv}_mxsamp{max_samples}_sd{seed}_ovrw{int(overwrite)}.csv'\n",
    "    results_full.to_csv(save_path)\n",
    "print(\"\\n\", \"\\n\", red_name, \"\\n\", results_full.round(3))\n",
    "#print(results_full.sort_values(\"roc_auc mean\").round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eb424e-0b18-471f-9cf6-4b1ce31e88c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save = True\n",
    "sampling = None\n",
    "cv = 7\n",
    "best_delete = 0\n",
    "strat_split = True\n",
    "n_optim = 1000\n",
    "cat_optim = 10\n",
    "ft_epochs = 10\n",
    "ft_lr = 1e-7\n",
    "max_s = 1024\n",
    "max_q = 128\n",
    "max_samples = 1024\n",
    "no_pre_process = False\n",
    "multi_decoder = \"permutation\"\n",
    "N_ens = 7\n",
    "seed = 42\n",
    "overwrite = True\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\", \"f1\"]\n",
    "dir_path = os.path.abspath(os.getcwd())\n",
    "run_name = \"medium_net_mlp_balance_bsplit_baseline\" ## NO PERM, VERY ROBUST\n",
    "run_name = \"medium_mlp_var_balance_05weight_anova\" ### BEST!!!!!! ####\n",
    "#run_name = \"medium_mlp_balance_lownoisefixparam_anova\"\n",
    "#run_name = \"medium_mlp_0.5static_balance_anova\"\n",
    "#run_name = \"large_mlp_fullbalance\"\n",
    "#run_name = \"small_net_mlp_var_imbalance_05weight\" ## WORKS VERY WELL WITH PERM\n",
    "#run_name = \"small_net_mlp_varbalance_weight_batchunisplit\"\n",
    "#run_name = \"small_net_mlp_variable_balance_weight\"\n",
    "#run_name = \"small_net_mlp_balance_minevalup\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "filename = \"model\"\n",
    "models = [\n",
    "    #CatBoostOptim(n_optim=cat_optim),\n",
    "    XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "    RandomForestClassifier(),\n",
    "    #XGBoostOptim(n_optim=n_optim),\n",
    "    LogisticRegression(max_iter=1000), \n",
    "    TabPFNClassifier(device='cpu', N_ensemble_configurations=5, no_preprocess_mode=no_pre_process),\n",
    "    TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\", max_epochs=ft_epochs)\n",
    "]\n",
    "\n",
    "#for reducer in [AnovaSelect(), NonZeroSelect(), MeanSelect(), StdSelect(), MaxSelect(), PCASelect()]:\n",
    "reducer = AnovaSelect()\n",
    "#for reduce_data in [top_anova, top_non_zero, top_mean, top_std, top_max, pca_reduce]:\n",
    "    #data = reduce_data(all_data, labels, 100)\n",
    "    #print(all_data.shape)\n",
    "for best_delete in range(0,510,10):\n",
    "    #reducer.k = 100\n",
    "    #reducer = None\n",
    "    results_mean = pd.DataFrame(np.zeros((len(models), len(metrics)+1)),\n",
    "                                index=[m.__class__.__name__ for m in models],\n",
    "                                columns=metrics+[\"runtime\"])\n",
    "    results_std = pd.DataFrame(np.zeros((len(models), len(metrics)+1)), \n",
    "                               index=[m.__class__.__name__ for m in models],\n",
    "                               columns=metrics+[\"runtime\"])\n",
    "    \n",
    "    for ii, model in enumerate(models):\n",
    "        results_mean.iloc[ii,:], results_std.iloc[ii,:] = cross_validate_sample(\n",
    "            model, all_data, labels, metrics, strat_split, cv, sampling, \n",
    "            reducer, max_samples, seed=seed, overwrite=overwrite, n_best_delete=best_delete)\n",
    "\n",
    "    results_mean = results_mean.add_suffix(\" mean\")\n",
    "    results_std = results_std.add_suffix(\" std\")\n",
    "    results_full = pd.concat((results_mean, results_std), axis=1)\n",
    "    cols = results_full.columns.tolist()\n",
    "    new_cols = []\n",
    "    for i in range(int(len(cols)/2)):\n",
    "        new_cols.append(cols[i])\n",
    "        new_cols.append(cols[i+int(len(cols)/2)])\n",
    "    results_full = results_full[new_cols]\n",
    "    red_name = \"feature_select_shift_10step\"\n",
    "    if save:\n",
    "        directory = f\"results/{red_name}\"\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        save_path = f'results/{red_name}/{best_delete}_cv{cv}_mxsamp{max_samples}_sd{seed}_ovrw{int(overwrite)}'\n",
    "        results_full.to_csv(save_path)\n",
    "    print(\"\\n\", \"\\n\", red_name, \"\\n\", results_full.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16f86dc-a1ee-4706-bc17-cbedd58ff3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "all_labels = []\n",
    "for p in [\"datasets/CRC_AUS_LOSO.csv\", \n",
    "          \"datasets/CRC_FRA_LOSO.csv\",\n",
    "         \"datasets/CRC_CHI_LOSO.csv\",\n",
    "         \"datasets/CRC_GER_LOSO.csv\",\n",
    "         #\"datasets/CRC_IND_additional.csv\",\n",
    "         \"datasets/CRC_USA_LOSO.csv\"]:\n",
    "    df = pd.read_csv(p)\n",
    "    df_binary = df.loc[(df[\"disease\"] == \"healthy\") | (df[\"disease\"]==\"CRC\")]\n",
    "    df_data = df_binary.iloc[:,4:]\n",
    "    data = df_data.to_numpy()\n",
    "    labels = df_binary[\"disease\"].to_numpy()\n",
    "    labels[labels==\"healthy\"] = 0\n",
    "    labels[labels==\"CRC\"] = 1\n",
    "    data = (1/np.sum(data, axis=1, keepdims=True))*data\n",
    "    all_data.append(data)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "all_data = np.concatenate(all_data,axis=0)\n",
    "labels = np.concatenate(all_labels)\n",
    "all_data, labels = unison_shuffled_copies(all_data, labels, seed=412)\n",
    "c1_ind = (labels==1).nonzero()[0]\n",
    "c1_del = c1_ind[:int(len(c1_ind)*0.97)]\n",
    "all_data, labels = np.delete(all_data, c1_del, axis=0), np.delete(labels, c1_del, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbafc58-eff4-4b74-b585-52c0a8cf6234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da53d28-57c8-48ba-a1e1-3080b58d3553",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = 20\n",
    "rocs = []\n",
    "f1s = []\n",
    "for i in range(0,510,10):\n",
    "    results = pd.read_csv(f'results/feature_select_shift_10step/{i}_cv{7}_mxsamp{1024}_sd{42}_ovrw{1}')\n",
    "    rocs.append(results[\"roc_auc mean\"].values)\n",
    "    f1s.append(results[\"f1 mean\"].values)\n",
    "rocs = np.array(rocs)\n",
    "f1s = np.array(f1s)\n",
    "labels = results.iloc[:,0].values\n",
    "plt.figure(figsize=(12,3), dpi=200)\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "colors = []\n",
    "for cl in range(rocs.shape[1]):\n",
    "    ax1.plot(np.arange(rocs.shape[0]-ws+1)*10, moving_average(rocs[:,cl], ws), label=labels[cl])\n",
    "    ax2.plot(np.arange(rocs.shape[0]-ws+1)*10, moving_average(f1s[:,cl], ws), label=labels[cl])\n",
    "plt.suptitle(\"ROC AUC and F1-Score shifting feature selection\")\n",
    "#ax1.set_title(\"ROC AUC\")\n",
    "ax1.set_ylabel(\"ROC AUC\")\n",
    "#ax2.set_title(\"F1-Score\")\n",
    "ax2.set_xlabel(\"feature shift\")\n",
    "ax2.set_ylabel(\"F1\")\n",
    "ax1.legend(fontsize=7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d448cf5d-91f9-4f84-9477-6fe30540037e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229c531a-d6f8-464b-bb73-476293131bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.count_nonzero(all_data, axis=0)\n",
    "counts = 1-counts/all_data.shape[0]\n",
    "means = np.mean(all_data, axis=0)\n",
    "plt.scatter(counts, means, s=1)\n",
    "plt.show()\n",
    "plt.scatter(counts, np.max(all_data, axis=0), s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216c8ae-5da2-4b93-bbde-e5bd0009e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3d2554-f847-48c5-9566-e04693fa8eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.abspath(os.getcwd())\n",
    "run_name = \"medium_net_mlp_balance_bsplit_baseline\"\n",
    "run_name = \"medium_net_mlp_balance_bsplit_baseline_longer\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "filename = \"model\"\n",
    "for sampling in [None]:#, undersample]:\n",
    "    cv = 5\n",
    "    strat_split = True\n",
    "    n_optim = 1000\n",
    "    cat_optim = 10\n",
    "    ft_epochs = 10\n",
    "    ft_lr = 1e-8\n",
    "    max_s = 1024\n",
    "    max_q = 128\n",
    "    max_samples = None\n",
    "    no_pre_process = False\n",
    "    multi_decoder = None\n",
    "    N_ens = 5\n",
    "    metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\"]\n",
    "    models = [\n",
    "        #CatBoostOptim(n_optim=cat_optim),\n",
    "        #MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder, ft_epochs=ft_epochs, ft_lr=ft_lr,\n",
    "        #                 max_s=max_s, max_q=max_q, no_preprocess_mode=no_pre_process),\n",
    "        MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "        #MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=5, no_preprocess_mode=True),\n",
    "        XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "        XGBoostOptim(n_optim=n_optim),\n",
    "        LogisticRegression(max_iter=500), \n",
    "        TabPFNClassifier(device='cpu', N_ensemble_configurations=5, no_preprocess_mode=no_pre_process),\n",
    "        TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\", max_epochs=ft_epochs)\n",
    "    ]\n",
    "    results_mean = pd.DataFrame(np.zeros((len(models), len(metrics)+1)),\n",
    "                                index=[m.__class__.__name__ for m in models],\n",
    "                                columns=metrics+[\"runtime\"])\n",
    "    results_std = pd.DataFrame(np.zeros((len(models), len(metrics)+1)), \n",
    "                               index=[m.__class__.__name__ for m in models],\n",
    "                               columns=metrics+[\"runtime\"])\n",
    "    \n",
    "    for ii, model in enumerate(models):\n",
    "        results_mean.iloc[ii,:], results_std.iloc[ii,:] = cross_validate_sample(model, data, labels, metrics, strat_split, cv, sampling, max_samples)\n",
    "    #results_sorted = results.sort_values(\"roc_auc\")\n",
    "    #print(results_sorted)\n",
    "    print(results_mean)\n",
    "    print(results_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04052993-69b3-4c87-9bf8-163357aa9fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "for m in metrics + \"runtime\":\n",
    "    cols.append(m)\n",
    "    cols.append(m+\" std\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc23d5f-155a-4e3d-b696-1fd6439a0cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dir_path = os.path.abspath(os.getcwd())\n",
    "run_name = \"medium_net_mlp_balance_bsplit_baseline\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "filename = \"model\"\n",
    "#model, config = load_model(path, filename, device=\"cpu\", eval_positions=None, verbose=0)\n",
    "#pred_model = TabPFNClassifier(model[2], config, device=\"cpu\", N_ensemble_configurations=5, no_preprocess_mode=False)\n",
    "for sampling in [None]:\n",
    "    cv = 3\n",
    "    strat_split = True\n",
    "    n_optim = 10\n",
    "    ft_epochs = 10\n",
    "    max_samples = None\n",
    "    metrics = metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\"]\n",
    "    models = [\n",
    "        RandomForestClassifier()\n",
    "        #CatBoostOptim(n_optim=n_optim),\n",
    "        #pred_model,\n",
    "        #MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=3, no_preprocess_mode=True),\n",
    "        #XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "        #XGBoostOptim(n_optim=n_optim),\n",
    "        #LogisticRegression(max_iter=500), \n",
    "        #TabPFNClassifier(device='cpu', N_ensemble_configurations=3, no_preprocess_mode=True),\n",
    "        #TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\", max_epochs=ft_epochs)\n",
    "    ]\n",
    "    results = pd.DataFrame(np.zeros((len(models), len(metrics)+1)), \n",
    "                           index=[m.__class__.__name__ for m in models],\n",
    "                          columns=metrics+[\"runtime\"])\n",
    "    \n",
    "    for ii, model in enumerate(models):\n",
    "        results.iloc[ii,:] = cross_validate_sample(model, data, labels, metrics, strat_split, cv, sampling, max_samples)\n",
    "    results_sorted = results.sort_values(\"roc_auc\")\n",
    "    \n",
    "    print(results_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5983e6a1-a7ce-4694-b775-e83498cd3be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819f6306-686d-42f1-829c-303890202785",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac970c40-6003-4c24-b454-014ba31f6b26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
