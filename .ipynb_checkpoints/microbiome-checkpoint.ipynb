{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "716a0985-a3e5-47be-acc6-d58fb95fa8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from tabpfn_new.scripts.transformer_prediction_interface import TabPFNClassifier, MedPFNClassifier\n",
    "from tabpfn_new.scripts.model_builder import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from data_prep_utils import *\n",
    "from evaluate import *\n",
    "from load_models import *\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import openml\n",
    "import time\n",
    "#pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0e72742-f9d3-4748-939e-0aee908e04c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_all.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_all.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m all_data, labels \u001b[38;5;241m=\u001b[39m \u001b[43mget_microbiome\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m all_data \u001b[38;5;241m=\u001b[39m remove_zero_features(all_data)\n\u001b[0;32m      4\u001b[0m all_data, labels \u001b[38;5;241m=\u001b[39m unison_shuffled_copies(all_data, labels, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\MT\\TabPFN-medical\\data_prep_utils.py:50\u001b[0m, in \u001b[0;36mget_microbiome\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_microbiome\u001b[39m(path):\n\u001b[1;32m---> 50\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     df_binary \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisease\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhealthy\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m|\u001b[39m (df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisease\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCRC\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m     52\u001b[0m     df_data \u001b[38;5;241m=\u001b[39m df_binary\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\master4\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\master4\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\master4\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\master4\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\master4\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_all.csv'"
     ]
    }
   ],
   "source": [
    "path = \"datasets/data_all.csv\"\n",
    "all_data, labels = get_microbiome(path)\n",
    "all_data = remove_zero_features(all_data)\n",
    "all_data, labels = unison_shuffled_copies(all_data, labels, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2ef4bc-ddca-4ee1-935d-9b8ecce76719",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/CRC_AUS_LOSO.csv\"\n",
    "df = pd.read_csv(path)\n",
    "print(df.head)\n",
    "path = \"datasets/data_all.csv\"\n",
    "df = pd.read_csv(path)\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "879ba0a8-b675-49eb-a1dc-64e836de555b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   10935    21864    32794 ... 15062973 15114671 15095590]\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " AnovaSelect \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "MajorityClass                   0.876         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.000        0.000\n",
      "XGBClassifier                   0.883         0.013           0.530          0.133        0.259       0.105         0.615        0.052    0.344   0.127         0.035        0.004\n",
      "MedPFNClassifier                0.664         0.069           0.118          0.017        0.265       0.111         0.493        0.026    0.158   0.033         1.127        0.094\n",
      "RandomForestClassifier          0.878         0.003           0.429          0.495        0.020       0.024         0.510        0.012    0.039   0.045         0.629        0.044\n",
      "LogisticRegression              0.876         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.004        0.001\n",
      "TabPFNClassifier                0.889         0.009           0.584          0.058        0.361       0.095         0.662        0.045    0.441   0.084         2.881        0.268\n",
      "[   10933    21865    32794 ... 15112012 15126908 15087328]\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " AnovaSelect \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "MajorityClass                   0.834         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.001        0.000\n",
      "XGBClassifier                   0.905         0.025           0.794          0.120        0.607       0.113         0.786        0.054    0.677   0.091         0.033        0.004\n",
      "MedPFNClassifier                0.560         0.024           0.089          0.045        0.194       0.109         0.414        0.038    0.121   0.064         1.134        0.072\n",
      "RandomForestClassifier          0.946         0.032           0.953          0.035        0.704       0.186         0.849        0.094    0.797   0.143         0.452        0.020\n",
      "LogisticRegression              0.834         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.003        0.000\n",
      "TabPFNClassifier                0.924         0.016           0.819          0.032        0.704       0.152         0.836        0.070    0.744   0.088         2.692        0.299\n",
      "[   10935    21865    32799 ... 15068796 15072215 15109330]\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " AnovaSelect \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "MajorityClass                   0.923         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.000        0.000\n",
      "XGBClassifier                   0.936         0.023           0.673          0.264        0.407       0.114         0.693        0.064    0.500   0.155         0.031        0.005\n",
      "MedPFNClassifier                0.719         0.101           0.071          0.045        0.264       0.243         0.511        0.068    0.106   0.071         1.247        0.087\n",
      "RandomForestClassifier          0.926         0.003           0.500          0.463        0.044       0.038         0.522        0.019    0.080   0.070         0.465        0.015\n",
      "LogisticRegression              0.923         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.003        0.000\n",
      "TabPFNClassifier                0.942         0.007           0.696          0.105        0.451       0.027         0.717        0.014    0.544   0.041         2.672        0.206\n",
      "[   10934    21868    32796 ... 15066134 15035812 15026001]\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " AnovaSelect \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "MajorityClass                   0.953         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.000        0.000\n",
      "XGBClassifier                   0.948         0.009           0.286          0.452        0.036       0.056         0.515        0.030    0.063   0.100         0.037        0.008\n",
      "MedPFNClassifier                0.791         0.054           0.075          0.025        0.304       0.147         0.560        0.055    0.117   0.040         1.188        0.074\n",
      "RandomForestClassifier          0.951         0.004           0.000          0.000        0.000       0.000         0.499        0.002    0.000   0.000         0.630        0.019\n",
      "LogisticRegression              0.953         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.004        0.001\n",
      "TabPFNClassifier                0.948         0.004           0.143          0.350        0.018       0.044         0.506        0.023    0.032   0.078         3.074        0.359\n",
      "[   10935    21868    32793 ... 15039854 15037043 15056928]\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " AnovaSelect \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "MajorityClass                   0.781         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.000        0.000\n",
      "XGBClassifier                   0.868         0.015           0.783          0.075        0.568       0.063         0.760        0.025    0.652   0.035         0.038        0.008\n",
      "MedPFNClassifier                0.653         0.103           0.386          0.119        0.664       0.079         0.657        0.041    0.468   0.058         1.164        0.102\n",
      "RandomForestClassifier          0.866         0.018           0.928          0.064        0.417       0.061         0.704        0.033    0.574   0.068         0.506        0.014\n",
      "LogisticRegression              0.781         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.003        0.001\n",
      "TabPFNClassifier                0.899         0.020           0.807          0.049        0.707       0.078         0.829        0.039    0.751   0.056         2.808        0.300\n"
     ]
    }
   ],
   "source": [
    "save = False\n",
    "sampling = None\n",
    "cv = 7\n",
    "best_delete = 0\n",
    "strat_split = True\n",
    "n_optim = 1000\n",
    "cat_optim = 10\n",
    "ft_epochs = 10\n",
    "ft_lr = 1e-5\n",
    "max_s = 1024\n",
    "max_q = 128\n",
    "max_samples = 1024\n",
    "no_pre_process = False\n",
    "multi_decoder = \"permutation\"\n",
    "N_ens = 3\n",
    "seed = 42\n",
    "overwrite = True\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\", \"f1\"]\n",
    "dir_path = os.path.abspath(os.getcwd())\n",
    "run_name = \"medium_net_mlp_balance_bsplit_baseline\" ## NO PERM, VERY ROBUST\n",
    "run_name = \"medium_mlp_var_balance_05weight_anova\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_var_balance_05weight_anova_longer\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_balance_lownoisefixparam_anova\"\n",
    "#run_name = \"medium_mlp_0.5static_balance_anova\"\n",
    "#run_name = \"large_mlp_fullbalance\"\n",
    "#run_name = \"small_net_mlp_var_imbalance_05weight\" ## WORKS VERY WELL WITH PERM\n",
    "#run_name = \"small_net_mlp_varbalance_weight_batchunisplit\"\n",
    "#run_name = \"small_net_mlp_variable_balance_weight\"\n",
    "#run_name = \"small_net_mlp_balance_minevalup\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "#path2 = dir_path + f\"/logs/trainrun_{run_name2}\"\n",
    "filename = \"model\"\n",
    "\n",
    "#all_data = all_data+1e-10\n",
    "#all_data = np.log(all_data)-np.mean(np.log(all_data), axis=0)\n",
    "#all_data = (all_data-np.mean(all_data,axis=0))/(np.std(all_data,axis=0)+1e-10)\n",
    "models = [\n",
    "    #CatBoostOptim(n_optim=cat_optim),\n",
    "    MajorityClass(),\n",
    "    XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "    #MedPFNClassifier(base_path=path2, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "    #MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process,\n",
    "    #                ft_epochs=ft_epochs, ft_lr=ft_lr),\n",
    "    RandomForestClassifier(),\n",
    "    #XGBoostOptim(n_optim=n_optim),\n",
    "    LogisticRegression(max_iter=1000), \n",
    "    TabPFNClassifier(device='cpu', N_ensemble_configurations=5, no_preprocess_mode=no_pre_process),\n",
    "    #TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\", max_epochs=ft_epochs)\n",
    "]\n",
    "\n",
    "for reducer in [AnovaSelect()]:#, RandomSelect(), NonZeroSelect(), MeanSelect(), StdSelect(), MaxSelect(), PCASelect()]:\n",
    "    results_mean = pd.DataFrame(np.zeros((len(models), len(metrics)+1)),\n",
    "                                index=[m.__class__.__name__ for m in models],\n",
    "                                columns=metrics+[\"runtime\"])\n",
    "    results_std = pd.DataFrame(np.zeros((len(models), len(metrics)+1)), \n",
    "                               index=[m.__class__.__name__ for m in models],\n",
    "                               columns=metrics+[\"runtime\"])\n",
    "    \n",
    "    for ii, model in enumerate(models):\n",
    "        results_mean.iloc[ii,:], results_std.iloc[ii,:] = cross_validate_sample(\n",
    "            model, all_data, labels, metrics, strat_split, cv, sampling, \n",
    "            reducer, max_samples, seed=seed, overwrite=overwrite, n_best_delete=best_delete)\n",
    "\n",
    "    results_mean = results_mean.add_suffix(\" mean\")\n",
    "    results_std = results_std.add_suffix(\" std\")\n",
    "    results_full = pd.concat((results_mean, results_std), axis=1)\n",
    "    cols = results_full.columns.tolist()\n",
    "    new_cols = []\n",
    "    for i in range(int(len(cols)/2)):\n",
    "        new_cols.append(cols[i])\n",
    "        new_cols.append(cols[i+int(len(cols)/2)])\n",
    "    results_full = results_full[new_cols]\n",
    "    red_name = reducer.__class__.__name__\n",
    "    if save:\n",
    "        directory = f\"results/{red_name}\"\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        save_path = f'results/{red_name}/baseline_cv{cv}_mxsamp{max_samples}_sd{seed}_ovrw{int(overwrite)}'\n",
    "        results_full.to_csv(save_path)\n",
    "    print(\"\\n\", \"\\n\", red_name, \"\\n\", results_full.round(3))\n",
    "    #print(results_full.sort_values(\"roc_auc mean\").round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51de1538-5dd7-40f8-9903-098b410b82cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = True\n",
    "sampling = None\n",
    "cv = 7\n",
    "best_delete = 0\n",
    "strat_split = True\n",
    "n_optim = 1000\n",
    "cat_optim = 10\n",
    "ft_epochs = 15\n",
    "ft_lr = 1e-7\n",
    "max_s = 1024\n",
    "max_q = 128\n",
    "max_samples = 1024\n",
    "no_pre_process = False\n",
    "multi_decoder = \"permutation\"\n",
    "N_ens = 7\n",
    "seed = 42\n",
    "overwrite = True\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\", \"f1\"]\n",
    "dir_path = os.path.abspath(os.getcwd())\n",
    "run_name = \"medium_net_mlp_balance_bsplit_baseline\" ## NO PERM, VERY ROBUST\n",
    "run_name = \"medium_mlp_var_balance_05weight_anova\" ## das aller bestigste\n",
    "#run_name = \"medium_mlp_balance_lownoisefixparam_anova\"\n",
    "#run_name = \"medium_mlp_0.5static_balance_anova\"\n",
    "#run_name = \"large_mlp_fullbalance\"\n",
    "#run_name = \"small_net_mlp_var_imbalance_05weight\" ## WORKS VERY WELL WITH PERM\n",
    "#run_name = \"small_net_mlp_varbalance_weight_batchunisplit\"\n",
    "#run_name = \"small_net_mlp_variable_balance_weight\"\n",
    "#run_name = \"small_net_mlp_balance_minevalup\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "filename = \"model\"\n",
    "models = [\n",
    "    #CatBoostOptim(n_optim=cat_optim),\n",
    "    MajorityClass(),\n",
    "    XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "    #MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process,\n",
    "    #                ft_epochs=ft_epochs, ft_lr=ft_lr),\n",
    "    RandomForestClassifier(),\n",
    "    #XGBoostOptim(n_optim=n_optim),\n",
    "    LogisticRegression(max_iter=1000), \n",
    "    TabPFNClassifier(device='cpu', N_ensemble_configurations=5, no_preprocess_mode=no_pre_process),\n",
    "    TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\", max_epochs=ft_epochs)\n",
    "]\n",
    "\n",
    "for reducer in [AnovaSelect()]:#, RandomSelect(), NonZeroSelect(), MeanSelect(), StdSelect(), MaxSelect(), PCASelect()]:\n",
    "    for std in [1e-9,1e-8,1e-7,1e-6,1e-5,1e-4]:\n",
    "        print(\"\\n\\n\", std)\n",
    "        noise_data = all_data + np.abs(np.random.default_rng(seed=42).normal(0,std,size=all_data.shape))\n",
    "        results_mean = pd.DataFrame(np.zeros((len(models), len(metrics)+1)),\n",
    "                                    index=[m.__class__.__name__ for m in models],\n",
    "                                    columns=metrics+[\"runtime\"])\n",
    "        results_std = pd.DataFrame(np.zeros((len(models), len(metrics)+1)), \n",
    "                                   index=[m.__class__.__name__ for m in models],\n",
    "                                   columns=metrics+[\"runtime\"])\n",
    "        \n",
    "        for ii, model in enumerate(models):\n",
    "            results_mean.iloc[ii,:], results_std.iloc[ii,:] = cross_validate_sample(\n",
    "                model, noise_data, labels, metrics, strat_split, cv, sampling, \n",
    "                reducer, max_samples, seed=seed, overwrite=overwrite, n_best_delete=best_delete)\n",
    "    \n",
    "        results_mean = results_mean.add_suffix(\" mean\")\n",
    "        results_std = results_std.add_suffix(\" std\")\n",
    "        results_full = pd.concat((results_mean, results_std), axis=1)\n",
    "        cols = results_full.columns.tolist()\n",
    "        new_cols = []\n",
    "        for i in range(int(len(cols)/2)):\n",
    "            new_cols.append(cols[i])\n",
    "            new_cols.append(cols[i+int(len(cols)/2)])\n",
    "        results_full = results_full[new_cols]\n",
    "        red_name = \"noise\"\n",
    "        if save:\n",
    "            directory = f\"results/{red_name}\"\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            save_path = f'results/{red_name}/n{std}_cv{cv}_mxsamp{max_samples}_sd{seed}_ovrw{int(overwrite)}'\n",
    "            results_full.to_csv(save_path)\n",
    "        #print(\"\\n\", \"\\n\", red_name, \"\\n\", results_full.round(3))\n",
    "        print(results_full.sort_values(\"roc_auc mean\").round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10eb424e-0b18-471f-9cf6-4b1ce31e88c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:08:27.968\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1040 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:08:32.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1250 | Train score: 0.9693 | Val loss: 0.1017 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:08:35.415\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1146 | Train score: 0.9571 | Val loss: 0.0962 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:08:37.564\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1738 | Train score: 0.9387 | Val loss: 0.0957 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:08:39.749\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1416 | Train score: 0.9571 | Val loss: 0.0968 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:08:41.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1361 | Train score: 0.9571 | Val loss: 0.0980 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:08:43.746\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.0892 | Train score: 0.9755 | Val loss: 0.0969 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:08:46.020\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1226 | Train score: 0.9509 | Val loss: 0.0954 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:08:47.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1018 | Train score: 0.9693 | Val loss: 0.0940 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:08:49.980\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1449 | Train score: 0.9571 | Val loss: 0.0935 | Val score: 0.9704\u001b[0m\n",
      "\u001b[32m2024-10-20 15:08:52.114\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1090 | Train score: 0.9693 | Val loss: 0.0936 | Val score: 0.9704\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:08:54.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1593 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:08:56.061\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1157 | Train score: 0.9448 | Val loss: 0.1760 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:08:58.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1371 | Train score: 0.9509 | Val loss: 0.1968 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:00.455\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1195 | Train score: 0.9509 | Val loss: 0.1921 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:02.571\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1338 | Train score: 0.9571 | Val loss: 0.1795 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:04.947\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1193 | Train score: 0.9693 | Val loss: 0.1679 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:06.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1188 | Train score: 0.9571 | Val loss: 0.1619 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:09.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.0990 | Train score: 0.9632 | Val loss: 0.1595 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:10.973\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1082 | Train score: 0.9632 | Val loss: 0.1599 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:13.147\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.0884 | Train score: 0.9755 | Val loss: 0.1616 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:15.403\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.0670 | Train score: 0.9693 | Val loss: 0.1649 | Val score: 0.9557\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:09:17.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1235 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:20.042\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1786 | Train score: 0.9448 | Val loss: 0.1392 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:21.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1303 | Train score: 0.9387 | Val loss: 0.1314 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:23.755\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1321 | Train score: 0.9509 | Val loss: 0.1270 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:25.642\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1120 | Train score: 0.9571 | Val loss: 0.1227 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:27.352\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1246 | Train score: 0.9509 | Val loss: 0.1188 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:28.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1454 | Train score: 0.9509 | Val loss: 0.1164 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:30.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.0728 | Train score: 0.9632 | Val loss: 0.1144 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:32.208\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.0775 | Train score: 0.9755 | Val loss: 0.1137 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:33.928\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1456 | Train score: 0.9632 | Val loss: 0.1132 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:35.486\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.0912 | Train score: 0.9571 | Val loss: 0.1135 | Val score: 0.9606\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:09:37.368\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1248 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:39.432\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1370 | Train score: 0.9509 | Val loss: 0.1184 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:41.185\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1698 | Train score: 0.9571 | Val loss: 0.1157 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:42.925\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.0775 | Train score: 0.9693 | Val loss: 0.1141 | Val score: 0.9704\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:44.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.0853 | Train score: 0.9755 | Val loss: 0.1144 | Val score: 0.9704\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:47.430\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1644 | Train score: 0.9571 | Val loss: 0.1155 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:49.978\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1784 | Train score: 0.9387 | Val loss: 0.1162 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:51.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1920 | Train score: 0.9448 | Val loss: 0.1172 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:53.825\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1386 | Train score: 0.9571 | Val loss: 0.1184 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:55.836\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.0982 | Train score: 0.9755 | Val loss: 0.1196 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:09:57.548\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1080 | Train score: 0.9816 | Val loss: 0.1204 | Val score: 0.9557\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:09:59.352\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.0860 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:01.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1437 | Train score: 0.9448 | Val loss: 0.0731 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:03.523\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1208 | Train score: 0.9387 | Val loss: 0.0676 | Val score: 0.9704\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:06.315\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1540 | Train score: 0.9509 | Val loss: 0.0693 | Val score: 0.9704\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:08.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.0893 | Train score: 0.9693 | Val loss: 0.0690 | Val score: 0.9704\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:11.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1141 | Train score: 0.9571 | Val loss: 0.0684 | Val score: 0.9754\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:14.122\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1020 | Train score: 0.9632 | Val loss: 0.0653 | Val score: 0.9754\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:16.989\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.0923 | Train score: 0.9571 | Val loss: 0.0609 | Val score: 0.9754\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:19.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1202 | Train score: 0.9509 | Val loss: 0.0578 | Val score: 0.9754\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:21.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1013 | Train score: 0.9571 | Val loss: 0.0554 | Val score: 0.9754\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:23.016\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.0705 | Train score: 0.9693 | Val loss: 0.0536 | Val score: 0.9754\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:10:24.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1229 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:26.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1235 | Train score: 0.9448 | Val loss: 0.1180 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:27.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1964 | Train score: 0.9448 | Val loss: 0.1174 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:28.841\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1419 | Train score: 0.9509 | Val loss: 0.1151 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:30.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1959 | Train score: 0.9325 | Val loss: 0.1162 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:31.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1099 | Train score: 0.9509 | Val loss: 0.1170 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:33.026\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1473 | Train score: 0.9509 | Val loss: 0.1154 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:34.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1185 | Train score: 0.9509 | Val loss: 0.1103 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:35.738\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1429 | Train score: 0.9387 | Val loss: 0.1088 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:37.137\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1422 | Train score: 0.9387 | Val loss: 0.1084 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:38.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.0853 | Train score: 0.9693 | Val loss: 0.1044 | Val score: 0.9606\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:10:40.104\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1007 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:41.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1382 | Train score: 0.9448 | Val loss: 0.0943 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:43.041\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1540 | Train score: 0.9509 | Val loss: 0.0985 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:44.558\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1095 | Train score: 0.9571 | Val loss: 0.0954 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:46.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1277 | Train score: 0.9632 | Val loss: 0.0953 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:47.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.0756 | Train score: 0.9816 | Val loss: 0.0948 | Val score: 0.9704\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:48.972\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1234 | Train score: 0.9632 | Val loss: 0.0954 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:50.570\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1218 | Train score: 0.9571 | Val loss: 0.0964 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:52.129\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1140 | Train score: 0.9693 | Val loss: 0.0982 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:53.613\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1070 | Train score: 0.9693 | Val loss: 0.1016 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:10:55.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1033 | Train score: 0.9693 | Val loss: 0.1032 | Val score: 0.9557\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.946         0.008           0.576          0.114        0.329       0.167         0.657        0.080    0.394   0.160         0.012        0.003\n",
      "MedPFNClassifier                0.959         0.008           0.651          0.058        0.629       0.128         0.804        0.064    0.637   0.094         1.942        0.035\n",
      "RandomForestClassifier          0.944         0.004           0.538          0.275        0.157       0.129         0.575        0.063    0.226   0.155         0.190        0.005\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.005        0.000\n",
      "TabPFNClassifier                0.953         0.009           0.714          0.160        0.371       0.148         0.680        0.073    0.468   0.130         3.124        0.520\n",
      "TabForestPFNClassifier          0.937         0.018           0.621          0.278        0.357       0.118         0.665        0.050    0.396   0.066        21.030        3.930\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:11:26.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1367 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:11:28.319\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1378 | Train score: 0.9448 | Val loss: 0.1411 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:11:29.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1799 | Train score: 0.9509 | Val loss: 0.1370 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:11:31.115\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.0837 | Train score: 0.9755 | Val loss: 0.1340 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:11:32.496\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1069 | Train score: 0.9448 | Val loss: 0.1360 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:11:33.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1845 | Train score: 0.9571 | Val loss: 0.1347 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:11:35.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1640 | Train score: 0.9448 | Val loss: 0.1334 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:11:36.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1224 | Train score: 0.9509 | Val loss: 0.1341 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:11:38.046\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1374 | Train score: 0.9448 | Val loss: 0.1348 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:11:39.450\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1045 | Train score: 0.9571 | Val loss: 0.1362 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:11:40.994\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1156 | Train score: 0.9632 | Val loss: 0.1413 | Val score: 0.9507\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:11:42.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1577 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:11:43.925\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1346 | Train score: 0.9693 | Val loss: 0.1572 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:11:45.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1497 | Train score: 0.9693 | Val loss: 0.1580 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:11:46.610\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1654 | Train score: 0.9571 | Val loss: 0.1545 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:11:47.955\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1592 | Train score: 0.9632 | Val loss: 0.1494 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:11:49.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2271 | Train score: 0.9387 | Val loss: 0.1459 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:11:50.607\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.0976 | Train score: 0.9693 | Val loss: 0.1451 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:11:51.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1013 | Train score: 0.9693 | Val loss: 0.1451 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:11:53.368\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1205 | Train score: 0.9632 | Val loss: 0.1453 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:11:54.680\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1202 | Train score: 0.9571 | Val loss: 0.1465 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:11:56.053\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1022 | Train score: 0.9755 | Val loss: 0.1509 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:11:57.649\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.0932 | Val score: 0.9704\u001b[0m\n",
      "\u001b[32m2024-10-20 15:11:59.146\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1499 | Train score: 0.9448 | Val loss: 0.0885 | Val score: 0.9704\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:00.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1377 | Train score: 0.9509 | Val loss: 0.0873 | Val score: 0.9704\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:01.965\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.0941 | Train score: 0.9632 | Val loss: 0.0824 | Val score: 0.9704\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:03.353\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1210 | Train score: 0.9571 | Val loss: 0.0812 | Val score: 0.9704\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:04.675\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1286 | Train score: 0.9448 | Val loss: 0.0821 | Val score: 0.9754\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:05.994\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.0729 | Train score: 0.9755 | Val loss: 0.0806 | Val score: 0.9754\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:07.374\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1557 | Train score: 0.9509 | Val loss: 0.0821 | Val score: 0.9754\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:08.713\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1521 | Train score: 0.9632 | Val loss: 0.0838 | Val score: 0.9704\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:10.024\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1760 | Train score: 0.9387 | Val loss: 0.0869 | Val score: 0.9704\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:11.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1744 | Train score: 0.9571 | Val loss: 0.0913 | Val score: 0.9704\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:12:12.929\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1512 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:14.367\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1303 | Train score: 0.9387 | Val loss: 0.1479 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:15.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1949 | Train score: 0.9387 | Val loss: 0.1485 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:17.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1423 | Train score: 0.9509 | Val loss: 0.1494 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:18.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1152 | Train score: 0.9693 | Val loss: 0.1514 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:19.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1371 | Train score: 0.9509 | Val loss: 0.1529 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:21.329\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1748 | Train score: 0.9571 | Val loss: 0.1528 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:22.746\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1621 | Train score: 0.9571 | Val loss: 0.1523 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:24.171\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.0995 | Train score: 0.9632 | Val loss: 0.1517 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:25.572\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1214 | Train score: 0.9509 | Val loss: 0.1512 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:27.066\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1104 | Train score: 0.9632 | Val loss: 0.1512 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:12:28.597\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1504 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:29.923\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1313 | Train score: 0.9571 | Val loss: 0.1501 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:31.245\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1742 | Train score: 0.9571 | Val loss: 0.1470 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:32.663\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1061 | Train score: 0.9693 | Val loss: 0.1502 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:34.352\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1130 | Train score: 0.9755 | Val loss: 0.1540 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:35.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1230 | Train score: 0.9632 | Val loss: 0.1561 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:37.199\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1521 | Train score: 0.9693 | Val loss: 0.1566 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:38.566\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1492 | Train score: 0.9571 | Val loss: 0.1561 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:40.062\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.0760 | Train score: 0.9755 | Val loss: 0.1575 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:41.403\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1317 | Train score: 0.9509 | Val loss: 0.1614 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:42.758\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1196 | Train score: 0.9509 | Val loss: 0.1625 | Val score: 0.9507\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:12:44.337\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1226 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:45.658\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1746 | Train score: 0.9387 | Val loss: 0.1093 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:46.989\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2077 | Train score: 0.9448 | Val loss: 0.1091 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:48.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1763 | Train score: 0.9632 | Val loss: 0.1108 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:49.853\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1482 | Train score: 0.9509 | Val loss: 0.1116 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:51.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1666 | Train score: 0.9509 | Val loss: 0.1124 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:52.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1494 | Train score: 0.9448 | Val loss: 0.1115 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:53.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1794 | Train score: 0.9509 | Val loss: 0.1135 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:55.299\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1479 | Train score: 0.9571 | Val loss: 0.1158 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:56.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1527 | Train score: 0.9509 | Val loss: 0.1170 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:12:58.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1862 | Train score: 0.9509 | Val loss: 0.1191 | Val score: 0.9507\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:12:59.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1087 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:13:01.162\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2107 | Train score: 0.9264 | Val loss: 0.1267 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:13:02.525\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1902 | Train score: 0.9387 | Val loss: 0.1372 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:13:03.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1459 | Train score: 0.9387 | Val loss: 0.1271 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:13:05.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1382 | Train score: 0.9387 | Val loss: 0.1124 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:13:06.647\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1275 | Train score: 0.9448 | Val loss: 0.1013 | Val score: 0.9704\u001b[0m\n",
      "\u001b[32m2024-10-20 15:13:08.013\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1015 | Train score: 0.9571 | Val loss: 0.0978 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:13:09.389\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1173 | Train score: 0.9632 | Val loss: 0.0988 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:13:10.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1656 | Train score: 0.9325 | Val loss: 0.1021 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:13:12.129\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1475 | Train score: 0.9693 | Val loss: 0.1050 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:13:13.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1506 | Train score: 0.9632 | Val loss: 0.1074 | Val score: 0.9606\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.960         0.006           0.814          0.171        0.500       0.131         0.745        0.061    0.589   0.085         0.010        0.001\n",
      "MedPFNClassifier                0.958         0.024           0.704          0.214        0.600       0.141         0.790        0.076    0.638   0.167         1.612        0.017\n",
      "RandomForestClassifier          0.947         0.004           0.667          0.318        0.171       0.103         0.583        0.050    0.259   0.141         0.182        0.005\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.004        0.000\n",
      "TabPFNClassifier                0.954         0.003           0.823          0.154        0.343       0.129         0.668        0.062    0.454   0.110         2.162        0.023\n",
      "TabForestPFNClassifier          0.932         0.009           0.436          0.104        0.300       0.131         0.636        0.057    0.324   0.082        15.153        0.247\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:13:45.611\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1212 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:13:46.928\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1781 | Train score: 0.9264 | Val loss: 0.1183 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:13:48.320\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1467 | Train score: 0.9509 | Val loss: 0.1177 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:13:49.859\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1685 | Train score: 0.9325 | Val loss: 0.1202 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:13:51.494\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1704 | Train score: 0.9387 | Val loss: 0.1248 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:13:52.925\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1407 | Train score: 0.9448 | Val loss: 0.1248 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:13:54.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1353 | Train score: 0.9448 | Val loss: 0.1211 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:13:55.798\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1467 | Train score: 0.9448 | Val loss: 0.1175 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:13:57.433\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1440 | Train score: 0.9448 | Val loss: 0.1158 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:13:58.869\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1400 | Train score: 0.9387 | Val loss: 0.1144 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:00.343\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1746 | Train score: 0.9509 | Val loss: 0.1144 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:14:02.086\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1809 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:03.454\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1304 | Train score: 0.9571 | Val loss: 0.1832 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:04.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1522 | Train score: 0.9571 | Val loss: 0.1867 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:06.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1033 | Train score: 0.9632 | Val loss: 0.1904 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:07.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1953 | Train score: 0.9448 | Val loss: 0.1855 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:09.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1234 | Train score: 0.9571 | Val loss: 0.1837 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:10.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1054 | Train score: 0.9693 | Val loss: 0.1842 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:12.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.0865 | Train score: 0.9755 | Val loss: 0.1848 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:13.447\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1359 | Train score: 0.9693 | Val loss: 0.1852 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:14.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1304 | Train score: 0.9632 | Val loss: 0.1844 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:16.228\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1268 | Train score: 0.9632 | Val loss: 0.1834 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:14:17.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1672 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:19.436\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1622 | Train score: 0.9387 | Val loss: 0.1686 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:21.018\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1432 | Train score: 0.9509 | Val loss: 0.1704 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:22.585\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1610 | Train score: 0.9448 | Val loss: 0.1685 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:24.199\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1428 | Train score: 0.9448 | Val loss: 0.1676 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:25.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1286 | Train score: 0.9632 | Val loss: 0.1679 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:27.335\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1717 | Train score: 0.9571 | Val loss: 0.1664 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:28.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.0955 | Train score: 0.9816 | Val loss: 0.1660 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:30.485\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1540 | Train score: 0.9509 | Val loss: 0.1659 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:32.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1671 | Train score: 0.9571 | Val loss: 0.1659 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:33.682\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1034 | Train score: 0.9693 | Val loss: 0.1662 | Val score: 0.9507\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:14:35.290\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1421 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:36.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1612 | Train score: 0.9387 | Val loss: 0.1443 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:38.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1992 | Train score: 0.9264 | Val loss: 0.1467 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:39.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1587 | Train score: 0.9448 | Val loss: 0.1464 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:41.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1286 | Train score: 0.9693 | Val loss: 0.1455 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:42.705\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1343 | Train score: 0.9632 | Val loss: 0.1448 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:44.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1214 | Train score: 0.9448 | Val loss: 0.1460 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:45.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.0965 | Train score: 0.9448 | Val loss: 0.1518 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:47.472\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1312 | Train score: 0.9571 | Val loss: 0.1576 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:49.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1242 | Train score: 0.9632 | Val loss: 0.1527 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:50.527\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1519 | Train score: 0.9571 | Val loss: 0.1510 | Val score: 0.9507\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:14:52.135\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1522 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:53.693\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1346 | Train score: 0.9387 | Val loss: 0.1545 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:55.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1210 | Train score: 0.9755 | Val loss: 0.1623 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:56.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1378 | Train score: 0.9693 | Val loss: 0.1643 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:58.427\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1048 | Train score: 0.9571 | Val loss: 0.1695 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:14:59.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1224 | Train score: 0.9632 | Val loss: 0.1730 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:15:01.541\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1619 | Train score: 0.9387 | Val loss: 0.1757 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:15:03.081\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.0858 | Train score: 0.9632 | Val loss: 0.1801 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:15:04.500\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1530 | Train score: 0.9448 | Val loss: 0.1813 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:15:06.054\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1372 | Train score: 0.9509 | Val loss: 0.1818 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:15:07.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1467 | Train score: 0.9387 | Val loss: 0.1810 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:15:09.322\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1361 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:15:10.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1312 | Train score: 0.9448 | Val loss: 0.1408 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:15:12.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1156 | Train score: 0.9387 | Val loss: 0.1368 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:15:13.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1426 | Train score: 0.9448 | Val loss: 0.1310 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:15:15.030\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1358 | Train score: 0.9509 | Val loss: 0.1255 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:15:16.424\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1702 | Train score: 0.9509 | Val loss: 0.1213 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:15:17.809\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1964 | Train score: 0.9509 | Val loss: 0.1167 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:15:19.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1659 | Train score: 0.9509 | Val loss: 0.1156 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:15:20.605\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1309 | Train score: 0.9509 | Val loss: 0.1155 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:15:21.975\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1523 | Train score: 0.9325 | Val loss: 0.1155 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:15:23.376\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1626 | Train score: 0.9387 | Val loss: 0.1165 | Val score: 0.9704\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:15:24.973\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1438 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:15:26.493\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1372 | Train score: 0.9387 | Val loss: 0.1382 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:15:28.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1271 | Train score: 0.9509 | Val loss: 0.1375 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:15:29.694\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.0994 | Train score: 0.9632 | Val loss: 0.1420 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:15:31.227\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2179 | Train score: 0.9448 | Val loss: 0.1415 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:15:32.760\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1354 | Train score: 0.9571 | Val loss: 0.1427 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:15:34.341\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1164 | Train score: 0.9632 | Val loss: 0.1439 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 15:15:35.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1204 | Train score: 0.9632 | Val loss: 0.1425 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:15:37.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1346 | Train score: 0.9387 | Val loss: 0.1405 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:15:38.894\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1630 | Train score: 0.9632 | Val loss: 0.1384 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:15:40.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1055 | Train score: 0.9632 | Val loss: 0.1367 | Val score: 0.9310\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.953         0.012           0.666          0.143        0.329       0.191         0.660        0.095    0.425   0.204         0.010        0.001\n",
      "MedPFNClassifier                0.959         0.018           0.708          0.186        0.557       0.159         0.771        0.084    0.619   0.161         1.629        0.019\n",
      "RandomForestClassifier          0.944         0.003           0.714          0.364        0.086       0.035         0.542        0.017    0.152   0.062         0.204        0.012\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.005        0.001\n",
      "TabPFNClassifier                0.954         0.010           0.810          0.208        0.314       0.136         0.654        0.068    0.437   0.149         2.164        0.010\n",
      "TabForestPFNClassifier          0.939         0.017           0.574          0.335        0.286       0.164         0.633        0.082    0.351   0.163        16.330        0.654\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:16:12.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1531 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:13.892\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1512 | Train score: 0.9202 | Val loss: 0.1549 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:15.216\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1296 | Train score: 0.9387 | Val loss: 0.1585 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:16.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1158 | Train score: 0.9693 | Val loss: 0.1649 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:17.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1567 | Train score: 0.9509 | Val loss: 0.1645 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:19.343\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.0914 | Train score: 0.9816 | Val loss: 0.1659 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:20.681\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1722 | Train score: 0.9509 | Val loss: 0.1589 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:22.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1122 | Train score: 0.9571 | Val loss: 0.1542 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:23.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1033 | Train score: 0.9448 | Val loss: 0.1516 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:24.758\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1271 | Train score: 0.9632 | Val loss: 0.1496 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:26.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1214 | Train score: 0.9571 | Val loss: 0.1479 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:16:27.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1501 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:29.311\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1435 | Train score: 0.9448 | Val loss: 0.1467 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:30.925\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1064 | Train score: 0.9693 | Val loss: 0.1491 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:32.464\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1379 | Train score: 0.9632 | Val loss: 0.1454 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:34.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1367 | Train score: 0.9571 | Val loss: 0.1426 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:35.826\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1604 | Train score: 0.9325 | Val loss: 0.1394 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:37.344\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1003 | Train score: 0.9632 | Val loss: 0.1373 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:38.956\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1510 | Train score: 0.9325 | Val loss: 0.1363 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:40.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.0958 | Train score: 0.9693 | Val loss: 0.1355 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:42.217\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1075 | Train score: 0.9509 | Val loss: 0.1346 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:43.854\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1312 | Train score: 0.9632 | Val loss: 0.1334 | Val score: 0.9557\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:16:45.470\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1498 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:46.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1869 | Train score: 0.9387 | Val loss: 0.1436 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:48.259\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1552 | Train score: 0.9387 | Val loss: 0.1399 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:49.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1992 | Train score: 0.9387 | Val loss: 0.1381 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:51.042\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1564 | Train score: 0.9571 | Val loss: 0.1357 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:52.438\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1591 | Train score: 0.9632 | Val loss: 0.1352 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:53.836\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1468 | Train score: 0.9509 | Val loss: 0.1342 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:55.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1665 | Train score: 0.9448 | Val loss: 0.1350 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:56.607\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1291 | Train score: 0.9571 | Val loss: 0.1347 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:58.053\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1822 | Train score: 0.9571 | Val loss: 0.1355 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:16:59.427\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1599 | Train score: 0.9571 | Val loss: 0.1355 | Val score: 0.9557\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:17:01.030\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1306 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:02.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1462 | Train score: 0.9387 | Val loss: 0.1320 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:03.772\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1654 | Train score: 0.9264 | Val loss: 0.1285 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:05.122\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1186 | Train score: 0.9448 | Val loss: 0.1257 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:06.525\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1812 | Train score: 0.9448 | Val loss: 0.1243 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:07.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1574 | Train score: 0.9571 | Val loss: 0.1239 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:09.283\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1650 | Train score: 0.9448 | Val loss: 0.1242 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:10.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1141 | Train score: 0.9693 | Val loss: 0.1242 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:12.054\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1201 | Train score: 0.9632 | Val loss: 0.1228 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:13.418\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.0987 | Train score: 0.9816 | Val loss: 0.1214 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:14.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1076 | Train score: 0.9755 | Val loss: 0.1200 | Val score: 0.9557\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:17:16.397\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1501 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:17.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1408 | Train score: 0.9387 | Val loss: 0.1347 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:19.193\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1290 | Train score: 0.9448 | Val loss: 0.1318 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:20.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1561 | Train score: 0.9571 | Val loss: 0.1298 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:21.993\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2097 | Train score: 0.9264 | Val loss: 0.1278 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:23.466\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1678 | Train score: 0.9387 | Val loss: 0.1263 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:24.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1174 | Train score: 0.9571 | Val loss: 0.1253 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:26.290\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1648 | Train score: 0.9571 | Val loss: 0.1245 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:27.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1701 | Train score: 0.9325 | Val loss: 0.1244 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:29.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1192 | Train score: 0.9693 | Val loss: 0.1239 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:30.603\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1148 | Train score: 0.9632 | Val loss: 0.1231 | Val score: 0.9655\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:17:32.199\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1410 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:33.591\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1238 | Train score: 0.9448 | Val loss: 0.1438 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:35.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1322 | Train score: 0.9571 | Val loss: 0.1551 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:36.418\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1861 | Train score: 0.9509 | Val loss: 0.1532 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:37.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1824 | Train score: 0.9325 | Val loss: 0.1437 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:39.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1130 | Train score: 0.9509 | Val loss: 0.1395 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:40.594\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1365 | Train score: 0.9387 | Val loss: 0.1339 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:41.940\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1146 | Train score: 0.9632 | Val loss: 0.1304 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:43.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.0957 | Train score: 0.9755 | Val loss: 0.1278 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:44.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1470 | Train score: 0.9509 | Val loss: 0.1267 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:46.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1161 | Train score: 0.9448 | Val loss: 0.1337 | Val score: 0.9507\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:17:47.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1632 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:49.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1597 | Train score: 0.9387 | Val loss: 0.1615 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:50.760\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1407 | Train score: 0.9448 | Val loss: 0.1640 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:52.278\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1308 | Train score: 0.9448 | Val loss: 0.1616 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:53.717\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.0980 | Train score: 0.9632 | Val loss: 0.1680 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:55.164\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.0805 | Train score: 0.9693 | Val loss: 0.1847 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:56.672\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1772 | Train score: 0.9448 | Val loss: 0.1817 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:58.154\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1214 | Train score: 0.9571 | Val loss: 0.1757 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:17:59.794\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1046 | Train score: 0.9632 | Val loss: 0.1698 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:18:01.424\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1422 | Train score: 0.9571 | Val loss: 0.1653 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:18:02.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1040 | Train score: 0.9755 | Val loss: 0.1641 | Val score: 0.9360\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.943         0.005           0.448          0.284        0.186       0.125         0.588        0.061    0.259   0.168         0.011        0.002\n",
      "MedPFNClassifier                0.946         0.014           0.544          0.115        0.514       0.125         0.744        0.065    0.528   0.119         1.618        0.016\n",
      "RandomForestClassifier          0.943         0.004           0.500          0.463        0.057       0.049         0.528        0.025    0.102   0.088         0.216        0.011\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.005        0.001\n",
      "TabPFNClassifier                0.945         0.010           0.657          0.290        0.186       0.136         0.589        0.068    0.270   0.167         2.158        0.022\n",
      "TabForestPFNClassifier          0.936         0.012           0.485          0.242        0.229       0.175         0.604        0.083    0.270   0.146        15.697        0.825\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:18:35.248\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1648 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:18:36.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1683 | Train score: 0.9387 | Val loss: 0.1640 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:18:38.147\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2636 | Train score: 0.9141 | Val loss: 0.1636 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:18:39.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1492 | Train score: 0.9387 | Val loss: 0.1639 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:18:41.101\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1849 | Train score: 0.9448 | Val loss: 0.1644 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:18:42.591\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1628 | Train score: 0.9387 | Val loss: 0.1641 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:18:43.929\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1742 | Train score: 0.9325 | Val loss: 0.1627 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:18:45.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1790 | Train score: 0.9387 | Val loss: 0.1621 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:18:46.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2316 | Train score: 0.9264 | Val loss: 0.1619 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:18:48.375\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1571 | Train score: 0.9509 | Val loss: 0.1612 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:18:49.992\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1767 | Train score: 0.9448 | Val loss: 0.1615 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:18:51.574\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1702 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:18:52.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1749 | Train score: 0.9387 | Val loss: 0.1683 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:18:54.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1294 | Train score: 0.9632 | Val loss: 0.1732 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:18:55.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1540 | Train score: 0.9509 | Val loss: 0.1784 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:18:57.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1433 | Train score: 0.9571 | Val loss: 0.1783 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:18:58.527\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1473 | Train score: 0.9632 | Val loss: 0.1775 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:00.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.0998 | Train score: 0.9632 | Val loss: 0.1802 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:01.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1364 | Train score: 0.9571 | Val loss: 0.1839 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:03.001\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1293 | Train score: 0.9571 | Val loss: 0.1874 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:04.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2215 | Train score: 0.9509 | Val loss: 0.1862 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:05.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1133 | Train score: 0.9632 | Val loss: 0.1852 | Val score: 0.9310\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:19:07.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1720 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:08.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1778 | Train score: 0.9325 | Val loss: 0.1679 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:10.454\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1238 | Train score: 0.9509 | Val loss: 0.1633 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:11.951\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1423 | Train score: 0.9325 | Val loss: 0.1614 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:13.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1176 | Train score: 0.9571 | Val loss: 0.1613 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:15.054\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.0809 | Train score: 0.9693 | Val loss: 0.1636 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:16.580\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1631 | Train score: 0.9387 | Val loss: 0.1636 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:18.196\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1090 | Train score: 0.9571 | Val loss: 0.1647 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:19.686\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1400 | Train score: 0.9448 | Val loss: 0.1660 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:21.236\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1111 | Train score: 0.9755 | Val loss: 0.1663 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:22.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1090 | Train score: 0.9693 | Val loss: 0.1674 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:19:24.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1526 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:25.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1505 | Train score: 0.9509 | Val loss: 0.1529 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:27.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1334 | Train score: 0.9571 | Val loss: 0.1568 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:28.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1697 | Train score: 0.9509 | Val loss: 0.1530 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:30.495\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1173 | Train score: 0.9387 | Val loss: 0.1522 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:31.986\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.0807 | Train score: 0.9632 | Val loss: 0.1547 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:33.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1175 | Train score: 0.9693 | Val loss: 0.1588 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:34.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1373 | Train score: 0.9448 | Val loss: 0.1633 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:36.266\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1242 | Train score: 0.9693 | Val loss: 0.1683 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:37.659\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1139 | Train score: 0.9755 | Val loss: 0.1726 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:39.123\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1169 | Train score: 0.9632 | Val loss: 0.1785 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:19:40.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1543 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:42.229\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1852 | Train score: 0.9387 | Val loss: 0.1559 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:43.642\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1404 | Train score: 0.9448 | Val loss: 0.1569 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:45.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1431 | Train score: 0.9387 | Val loss: 0.1586 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:46.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1541 | Train score: 0.9448 | Val loss: 0.1593 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:47.907\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1667 | Train score: 0.9387 | Val loss: 0.1591 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:49.344\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1150 | Train score: 0.9509 | Val loss: 0.1594 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:50.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1465 | Train score: 0.9509 | Val loss: 0.1594 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:52.180\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1113 | Train score: 0.9571 | Val loss: 0.1598 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:53.630\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1525 | Train score: 0.9325 | Val loss: 0.1603 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:54.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1695 | Train score: 0.9387 | Val loss: 0.1621 | Val score: 0.9507\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:19:56.580\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1509 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:58.103\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1800 | Train score: 0.9448 | Val loss: 0.1475 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:19:59.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1561 | Train score: 0.9387 | Val loss: 0.1387 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:20:00.959\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1198 | Train score: 0.9509 | Val loss: 0.1363 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:20:02.404\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1387 | Train score: 0.9571 | Val loss: 0.1360 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:20:03.923\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1299 | Train score: 0.9693 | Val loss: 0.1363 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:20:05.312\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1654 | Train score: 0.9571 | Val loss: 0.1354 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:20:06.856\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.0837 | Train score: 0.9755 | Val loss: 0.1351 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:20:08.319\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1926 | Train score: 0.9509 | Val loss: 0.1347 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:20:09.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2036 | Train score: 0.9325 | Val loss: 0.1346 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:20:11.325\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1367 | Train score: 0.9571 | Val loss: 0.1351 | Val score: 0.9507\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:20:12.940\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1983 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:20:14.298\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1574 | Train score: 0.9387 | Val loss: 0.2033 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:20:15.680\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1733 | Train score: 0.9509 | Val loss: 0.1958 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:20:17.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1674 | Train score: 0.9448 | Val loss: 0.1961 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:20:18.431\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1561 | Train score: 0.9448 | Val loss: 0.2009 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:20:19.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1453 | Train score: 0.9632 | Val loss: 0.2032 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:20:21.180\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.0979 | Train score: 0.9693 | Val loss: 0.2096 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:20:22.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1579 | Train score: 0.9632 | Val loss: 0.2156 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:20:24.003\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1545 | Train score: 0.9509 | Val loss: 0.2208 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:20:25.387\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1663 | Train score: 0.9509 | Val loss: 0.2202 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:20:26.754\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1331 | Train score: 0.9571 | Val loss: 0.2189 | Val score: 0.9409\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.944         0.010           0.405          0.352        0.200       0.177         0.596        0.088    0.265   0.231         0.011        0.001\n",
      "MedPFNClassifier                0.942         0.014           0.529          0.126        0.443       0.129         0.708        0.064    0.470   0.111         1.624        0.015\n",
      "RandomForestClassifier          0.942         0.006           0.500          0.463        0.057       0.049         0.527        0.025    0.102   0.088         0.235        0.006\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.005        0.001\n",
      "TabPFNClassifier                0.937         0.013           0.524          0.382        0.114       0.064         0.551        0.033    0.176   0.101         2.179        0.025\n",
      "TabForestPFNClassifier          0.934         0.012           0.410          0.164        0.214       0.112         0.597        0.056    0.271   0.125        15.871        0.462\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:20:59.310\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1655 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:00.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2003 | Train score: 0.9325 | Val loss: 0.1672 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:02.150\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1723 | Train score: 0.9387 | Val loss: 0.1557 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:03.562\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2078 | Train score: 0.9325 | Val loss: 0.1540 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:04.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1934 | Train score: 0.9325 | Val loss: 0.1549 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:06.337\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2790 | Train score: 0.9264 | Val loss: 0.1586 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:07.723\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1605 | Train score: 0.9448 | Val loss: 0.1610 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:09.121\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1715 | Train score: 0.9387 | Val loss: 0.1621 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:10.480\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1639 | Train score: 0.9448 | Val loss: 0.1615 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:11.823\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1575 | Train score: 0.9509 | Val loss: 0.1599 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:13.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1423 | Train score: 0.9571 | Val loss: 0.1585 | Val score: 0.9507\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:21:14.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1899 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:16.106\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1601 | Train score: 0.9325 | Val loss: 0.1939 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:17.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1515 | Train score: 0.9571 | Val loss: 0.2007 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:19.107\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1283 | Train score: 0.9632 | Val loss: 0.1979 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:20.447\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1522 | Train score: 0.9387 | Val loss: 0.1900 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:22.034\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1111 | Train score: 0.9632 | Val loss: 0.1881 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:23.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1436 | Train score: 0.9448 | Val loss: 0.1872 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:24.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.0984 | Train score: 0.9632 | Val loss: 0.1880 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:26.289\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1692 | Train score: 0.9509 | Val loss: 0.1867 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:27.696\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1172 | Train score: 0.9509 | Val loss: 0.1850 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:29.061\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1022 | Train score: 0.9571 | Val loss: 0.1856 | Val score: 0.9507\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:21:30.666\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1779 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:32.226\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1695 | Train score: 0.9387 | Val loss: 0.1708 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:33.748\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1465 | Train score: 0.9325 | Val loss: 0.1741 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:35.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1546 | Train score: 0.9509 | Val loss: 0.1786 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:36.736\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1725 | Train score: 0.9632 | Val loss: 0.1785 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:38.262\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1315 | Train score: 0.9693 | Val loss: 0.1801 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:39.760\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1737 | Train score: 0.9387 | Val loss: 0.1792 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:41.299\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1387 | Train score: 0.9632 | Val loss: 0.1793 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:42.839\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1621 | Train score: 0.9632 | Val loss: 0.1805 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:44.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1603 | Train score: 0.9571 | Val loss: 0.1816 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:46.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1381 | Train score: 0.9571 | Val loss: 0.1831 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:21:47.663\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1980 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:49.059\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1747 | Train score: 0.9387 | Val loss: 0.1994 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:50.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1411 | Train score: 0.9387 | Val loss: 0.2160 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:51.900\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1530 | Train score: 0.9509 | Val loss: 0.2189 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:53.267\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1418 | Train score: 0.9632 | Val loss: 0.2214 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:54.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1691 | Train score: 0.9325 | Val loss: 0.2163 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:56.000\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1172 | Train score: 0.9509 | Val loss: 0.2154 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:57.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1491 | Train score: 0.9325 | Val loss: 0.2142 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:21:58.926\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1482 | Train score: 0.9448 | Val loss: 0.2129 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:00.392\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1091 | Train score: 0.9509 | Val loss: 0.2130 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:01.826\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1171 | Train score: 0.9571 | Val loss: 0.2162 | Val score: 0.9557\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:22:03.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1691 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:05.021\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1688 | Train score: 0.9387 | Val loss: 0.1724 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:06.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1256 | Train score: 0.9571 | Val loss: 0.1808 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:07.859\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1461 | Train score: 0.9509 | Val loss: 0.1859 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:09.264\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1911 | Train score: 0.9264 | Val loss: 0.1811 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:10.799\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1314 | Train score: 0.9571 | Val loss: 0.1822 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:12.212\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1729 | Train score: 0.9509 | Val loss: 0.1813 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:13.612\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1211 | Train score: 0.9387 | Val loss: 0.1803 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:14.981\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1304 | Train score: 0.9448 | Val loss: 0.1783 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:16.340\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1484 | Train score: 0.9448 | Val loss: 0.1785 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:17.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1584 | Train score: 0.9448 | Val loss: 0.1777 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:22:19.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1603 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:20.744\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1994 | Train score: 0.9387 | Val loss: 0.1589 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:22.131\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1808 | Train score: 0.9387 | Val loss: 0.1431 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:23.575\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1615 | Train score: 0.9325 | Val loss: 0.1325 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:24.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1332 | Train score: 0.9387 | Val loss: 0.1273 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:26.492\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1761 | Train score: 0.9325 | Val loss: 0.1258 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:27.917\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1160 | Train score: 0.9509 | Val loss: 0.1245 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:29.390\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1179 | Train score: 0.9448 | Val loss: 0.1230 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:30.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2128 | Train score: 0.9509 | Val loss: 0.1230 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:32.174\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1736 | Train score: 0.9509 | Val loss: 0.1234 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:33.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1798 | Train score: 0.9448 | Val loss: 0.1235 | Val score: 0.9557\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:22:35.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1941 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:36.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1717 | Train score: 0.9387 | Val loss: 0.2035 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:38.062\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2254 | Train score: 0.9080 | Val loss: 0.1982 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:39.646\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1496 | Train score: 0.9325 | Val loss: 0.1959 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:41.173\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2186 | Train score: 0.9325 | Val loss: 0.1919 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:42.731\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1757 | Train score: 0.9325 | Val loss: 0.1897 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:44.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1996 | Train score: 0.9387 | Val loss: 0.1900 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:45.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1972 | Train score: 0.9387 | Val loss: 0.1938 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:47.248\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1611 | Train score: 0.9387 | Val loss: 0.1935 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:48.696\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1756 | Train score: 0.9387 | Val loss: 0.1893 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:22:50.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1482 | Train score: 0.9387 | Val loss: 0.1885 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.942         0.005           0.400          0.256        0.186       0.125         0.587        0.061    0.253   0.167         0.012        0.001\n",
      "MedPFNClassifier                0.937         0.005           0.456          0.055        0.429       0.175         0.699        0.083    0.429   0.101         1.621        0.019\n",
      "RandomForestClassifier          0.939         0.003           0.190          0.226        0.043       0.049         0.519        0.023    0.070   0.080         0.254        0.016\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.005        0.001\n",
      "TabPFNClassifier                0.944         0.005           0.429          0.290        0.129       0.116         0.562        0.057    0.194   0.164         2.178        0.034\n",
      "TabForestPFNClassifier          0.940         0.007           0.375          0.205        0.157       0.150         0.573        0.073    0.211   0.163        15.763        0.491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:23:22.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1685 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:23:24.264\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1897 | Train score: 0.9387 | Val loss: 0.1619 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:23:25.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1609 | Train score: 0.9387 | Val loss: 0.1554 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:23:26.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2151 | Train score: 0.9448 | Val loss: 0.1534 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:23:28.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1693 | Train score: 0.9325 | Val loss: 0.1517 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:23:29.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1736 | Train score: 0.9387 | Val loss: 0.1507 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:23:31.075\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2174 | Train score: 0.9387 | Val loss: 0.1517 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:23:32.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1635 | Train score: 0.9571 | Val loss: 0.1515 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:23:33.780\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1837 | Train score: 0.9325 | Val loss: 0.1511 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:23:35.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1800 | Train score: 0.9509 | Val loss: 0.1515 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:23:36.620\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1611 | Train score: 0.9448 | Val loss: 0.1494 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:23:38.193\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1680 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:23:39.578\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1553 | Train score: 0.9387 | Val loss: 0.1682 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:23:40.956\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1733 | Train score: 0.9325 | Val loss: 0.1750 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:23:42.428\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1313 | Train score: 0.9571 | Val loss: 0.1799 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:23:43.798\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2130 | Train score: 0.9325 | Val loss: 0.1716 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:23:45.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1347 | Train score: 0.9571 | Val loss: 0.1649 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:23:46.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1070 | Train score: 0.9448 | Val loss: 0.1618 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:23:47.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1326 | Train score: 0.9571 | Val loss: 0.1603 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:23:49.394\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1722 | Train score: 0.9387 | Val loss: 0.1599 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:23:50.760\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1349 | Train score: 0.9509 | Val loss: 0.1612 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:23:52.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1987 | Train score: 0.9387 | Val loss: 0.1590 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:23:53.759\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1615 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:23:55.188\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1778 | Train score: 0.9387 | Val loss: 0.1595 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:23:56.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1662 | Train score: 0.9509 | Val loss: 0.1619 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:23:57.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1645 | Train score: 0.9448 | Val loss: 0.1636 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:23:59.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1588 | Train score: 0.9509 | Val loss: 0.1637 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:00.578\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1518 | Train score: 0.9571 | Val loss: 0.1620 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:01.957\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1459 | Train score: 0.9325 | Val loss: 0.1593 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:03.280\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1939 | Train score: 0.9387 | Val loss: 0.1580 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:04.682\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1306 | Train score: 0.9632 | Val loss: 0.1570 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:06.010\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1438 | Train score: 0.9448 | Val loss: 0.1562 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:07.376\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1694 | Train score: 0.9571 | Val loss: 0.1555 | Val score: 0.9310\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:24:08.957\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1573 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:10.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2078 | Train score: 0.9325 | Val loss: 0.1435 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:11.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1757 | Train score: 0.9509 | Val loss: 0.1382 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:13.390\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1469 | Train score: 0.9387 | Val loss: 0.1317 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:14.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1548 | Train score: 0.9448 | Val loss: 0.1292 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:16.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1306 | Train score: 0.9325 | Val loss: 0.1265 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:17.648\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1753 | Train score: 0.9387 | Val loss: 0.1271 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:19.080\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1257 | Train score: 0.9509 | Val loss: 0.1258 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:20.527\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1616 | Train score: 0.9448 | Val loss: 0.1260 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:22.069\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1216 | Train score: 0.9509 | Val loss: 0.1271 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:23.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1222 | Train score: 0.9509 | Val loss: 0.1266 | Val score: 0.9606\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:24:25.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1458 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:26.580\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1884 | Train score: 0.9325 | Val loss: 0.1427 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:28.015\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1922 | Train score: 0.9448 | Val loss: 0.1433 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:29.472\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1851 | Train score: 0.9448 | Val loss: 0.1424 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:30.798\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1801 | Train score: 0.9387 | Val loss: 0.1411 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:32.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1805 | Train score: 0.9387 | Val loss: 0.1412 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:33.502\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1828 | Train score: 0.9448 | Val loss: 0.1431 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:34.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1593 | Train score: 0.9448 | Val loss: 0.1455 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:36.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1830 | Train score: 0.9387 | Val loss: 0.1491 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:37.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1556 | Train score: 0.9509 | Val loss: 0.1523 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:38.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1489 | Train score: 0.9448 | Val loss: 0.1578 | Val score: 0.9507\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:24:40.559\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1951 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:41.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1410 | Train score: 0.9571 | Val loss: 0.1932 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:43.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1108 | Train score: 0.9387 | Val loss: 0.1988 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:44.715\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1530 | Train score: 0.9571 | Val loss: 0.1981 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:46.054\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2000 | Train score: 0.9264 | Val loss: 0.1928 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:47.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1416 | Train score: 0.9571 | Val loss: 0.1896 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:48.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1824 | Train score: 0.9509 | Val loss: 0.1860 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:50.150\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1183 | Train score: 0.9632 | Val loss: 0.1859 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:51.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1284 | Train score: 0.9693 | Val loss: 0.1871 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:52.864\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1362 | Train score: 0.9387 | Val loss: 0.1889 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:54.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1221 | Train score: 0.9509 | Val loss: 0.1916 | Val score: 0.9261\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:24:55.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1546 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:57.172\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1654 | Train score: 0.9387 | Val loss: 0.1429 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:58.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2115 | Train score: 0.9325 | Val loss: 0.1453 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:24:59.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1989 | Train score: 0.9387 | Val loss: 0.1481 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:25:01.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2138 | Train score: 0.9325 | Val loss: 0.1522 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:25:02.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1691 | Train score: 0.9387 | Val loss: 0.1532 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:25:03.955\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1649 | Train score: 0.9387 | Val loss: 0.1479 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:25:05.360\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1860 | Train score: 0.9325 | Val loss: 0.1448 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:25:06.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1790 | Train score: 0.9387 | Val loss: 0.1476 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:25:08.066\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1794 | Train score: 0.9448 | Val loss: 0.1486 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:25:09.457\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2099 | Train score: 0.9325 | Val loss: 0.1512 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.942         0.012           0.343          0.253        0.257       0.232         0.621        0.113    0.290   0.241         0.013        0.001\n",
      "MedPFNClassifier                0.941         0.015           0.486          0.142        0.429       0.175         0.701        0.090    0.454   0.160         1.613        0.017\n",
      "RandomForestClassifier          0.941         0.000           0.143          0.226        0.029       0.045         0.513        0.021    0.048   0.075         0.284        0.012\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.005        0.000\n",
      "TabPFNClassifier                0.951         0.017           0.721          0.256        0.300       0.256         0.646        0.128    0.374   0.258         2.172        0.024\n",
      "TabForestPFNClassifier          0.939         0.009           0.481          0.301        0.143       0.090         0.566        0.043    0.204   0.119        15.177        0.360\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:25:42.263\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1741 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:25:43.836\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1610 | Train score: 0.9387 | Val loss: 0.1669 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:25:45.396\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1967 | Train score: 0.9325 | Val loss: 0.1653 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:25:47.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1890 | Train score: 0.9325 | Val loss: 0.1676 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:25:48.571\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1258 | Train score: 0.9387 | Val loss: 0.1663 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:25:50.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1767 | Train score: 0.9509 | Val loss: 0.1655 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:25:51.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1469 | Train score: 0.9509 | Val loss: 0.1657 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:25:53.308\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1537 | Train score: 0.9509 | Val loss: 0.1658 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:25:54.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1834 | Train score: 0.9448 | Val loss: 0.1674 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:25:56.498\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1218 | Train score: 0.9632 | Val loss: 0.1694 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:25:58.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1595 | Train score: 0.9264 | Val loss: 0.1718 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:25:59.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1636 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:01.224\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1489 | Train score: 0.9387 | Val loss: 0.1650 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:02.586\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1666 | Train score: 0.9571 | Val loss: 0.1626 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:04.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1596 | Train score: 0.9448 | Val loss: 0.1591 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:05.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1120 | Train score: 0.9509 | Val loss: 0.1581 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:06.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2298 | Train score: 0.9448 | Val loss: 0.1573 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:08.372\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1193 | Train score: 0.9448 | Val loss: 0.1575 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:09.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1658 | Train score: 0.9448 | Val loss: 0.1576 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:11.088\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1594 | Train score: 0.9509 | Val loss: 0.1582 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:12.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1930 | Train score: 0.9325 | Val loss: 0.1579 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:14.045\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1926 | Train score: 0.9387 | Val loss: 0.1581 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:26:15.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1505 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:17.214\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1734 | Train score: 0.9387 | Val loss: 0.1424 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:18.542\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1819 | Train score: 0.9325 | Val loss: 0.1404 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:19.887\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1604 | Train score: 0.9448 | Val loss: 0.1398 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:21.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1909 | Train score: 0.9202 | Val loss: 0.1423 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:23.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1724 | Train score: 0.9325 | Val loss: 0.1432 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:24.586\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1331 | Train score: 0.9387 | Val loss: 0.1426 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:26.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1478 | Train score: 0.9325 | Val loss: 0.1435 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:27.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1414 | Train score: 0.9448 | Val loss: 0.1476 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:28.826\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1852 | Train score: 0.9264 | Val loss: 0.1498 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:30.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1154 | Train score: 0.9571 | Val loss: 0.1512 | Val score: 0.9507\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:26:31.950\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1369 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:33.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1651 | Train score: 0.9387 | Val loss: 0.1356 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:34.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1739 | Train score: 0.9387 | Val loss: 0.1274 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:36.271\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1414 | Train score: 0.9448 | Val loss: 0.1228 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:37.625\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1669 | Train score: 0.9509 | Val loss: 0.1220 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:39.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1090 | Train score: 0.9755 | Val loss: 0.1207 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:40.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1248 | Train score: 0.9693 | Val loss: 0.1206 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:42.103\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1193 | Train score: 0.9632 | Val loss: 0.1212 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:43.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1628 | Train score: 0.9571 | Val loss: 0.1230 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:45.016\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1967 | Train score: 0.9448 | Val loss: 0.1252 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:46.562\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1512 | Train score: 0.9387 | Val loss: 0.1258 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:26:48.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1598 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:49.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1536 | Train score: 0.9387 | Val loss: 0.1628 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:50.986\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1426 | Train score: 0.9509 | Val loss: 0.1736 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:52.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1949 | Train score: 0.9448 | Val loss: 0.1787 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:53.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1235 | Train score: 0.9509 | Val loss: 0.1820 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:55.374\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2122 | Train score: 0.9202 | Val loss: 0.1805 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:56.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1690 | Train score: 0.9448 | Val loss: 0.1766 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:58.258\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1607 | Train score: 0.9571 | Val loss: 0.1718 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:26:59.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1524 | Train score: 0.9387 | Val loss: 0.1705 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:27:01.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1327 | Train score: 0.9509 | Val loss: 0.1709 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:27:02.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1540 | Train score: 0.9509 | Val loss: 0.1719 | Val score: 0.9507\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:27:04.175\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1781 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:27:05.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1621 | Train score: 0.9448 | Val loss: 0.1788 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:27:06.882\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1569 | Train score: 0.9509 | Val loss: 0.1772 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:27:08.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1186 | Train score: 0.9509 | Val loss: 0.1809 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:27:09.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2081 | Train score: 0.9387 | Val loss: 0.1812 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:27:10.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1248 | Train score: 0.9509 | Val loss: 0.1842 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:27:12.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1580 | Train score: 0.9387 | Val loss: 0.1866 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:27:13.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1253 | Train score: 0.9571 | Val loss: 0.1906 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:27:15.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1173 | Train score: 0.9571 | Val loss: 0.1927 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:27:16.514\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1632 | Train score: 0.9387 | Val loss: 0.1942 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:27:17.890\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1522 | Train score: 0.9571 | Val loss: 0.1954 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:27:19.496\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1739 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:27:20.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1553 | Train score: 0.9387 | Val loss: 0.1717 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:27:22.311\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1545 | Train score: 0.9325 | Val loss: 0.1709 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:27:23.714\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1871 | Train score: 0.9387 | Val loss: 0.1683 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:27:25.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1346 | Train score: 0.9509 | Val loss: 0.1675 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:27:26.537\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2005 | Train score: 0.9387 | Val loss: 0.1658 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:27:27.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1493 | Train score: 0.9509 | Val loss: 0.1649 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:27:29.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1577 | Train score: 0.9448 | Val loss: 0.1649 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:27:30.804\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1167 | Train score: 0.9571 | Val loss: 0.1650 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:27:32.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1388 | Train score: 0.9509 | Val loss: 0.1654 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:27:33.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1624 | Train score: 0.9387 | Val loss: 0.1656 | Val score: 0.9360\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.938         0.012           0.408          0.276        0.229       0.158         0.606        0.079    0.288   0.194         0.014        0.001\n",
      "MedPFNClassifier                0.936         0.012           0.441          0.097        0.429       0.175         0.698        0.088    0.430   0.133         1.608        0.018\n",
      "RandomForestClassifier          0.943         0.007           0.413          0.379        0.171       0.191         0.582        0.092    0.217   0.216         0.314        0.008\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.005        0.001\n",
      "TabPFNClassifier                0.948         0.011           0.595          0.151        0.271       0.167         0.631        0.083    0.357   0.190         2.166        0.020\n",
      "TabForestPFNClassifier          0.940         0.007           0.446          0.200        0.271       0.167         0.627        0.079    0.318   0.169        15.840        0.612\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:28:06.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1679 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:07.919\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1839 | Train score: 0.9387 | Val loss: 0.1588 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:09.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1721 | Train score: 0.9387 | Val loss: 0.1550 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:10.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1595 | Train score: 0.9325 | Val loss: 0.1531 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:12.178\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1993 | Train score: 0.9448 | Val loss: 0.1517 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:13.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1630 | Train score: 0.9387 | Val loss: 0.1508 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:15.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1579 | Train score: 0.9387 | Val loss: 0.1504 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:16.432\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1463 | Train score: 0.9387 | Val loss: 0.1497 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:17.966\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1433 | Train score: 0.9448 | Val loss: 0.1493 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:19.397\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1752 | Train score: 0.9387 | Val loss: 0.1492 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:20.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1294 | Train score: 0.9387 | Val loss: 0.1491 | Val score: 0.9557\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:28:22.387\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1620 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:23.737\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1553 | Train score: 0.9325 | Val loss: 0.1548 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:25.074\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2203 | Train score: 0.9387 | Val loss: 0.1543 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:26.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1584 | Train score: 0.9448 | Val loss: 0.1555 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:27.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1680 | Train score: 0.9509 | Val loss: 0.1562 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:29.155\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1810 | Train score: 0.9387 | Val loss: 0.1577 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:30.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1696 | Train score: 0.9571 | Val loss: 0.1595 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:31.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1532 | Train score: 0.9571 | Val loss: 0.1608 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:33.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1521 | Train score: 0.9509 | Val loss: 0.1617 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:34.567\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1885 | Train score: 0.9387 | Val loss: 0.1622 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:36.059\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1308 | Train score: 0.9571 | Val loss: 0.1621 | Val score: 0.9261\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:28:37.685\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1661 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:39.066\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1076 | Train score: 0.9693 | Val loss: 0.1693 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:40.714\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1610 | Train score: 0.9448 | Val loss: 0.1716 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:42.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2157 | Train score: 0.9509 | Val loss: 0.1661 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:43.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1464 | Train score: 0.9448 | Val loss: 0.1628 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:45.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1343 | Train score: 0.9448 | Val loss: 0.1616 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:46.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.0934 | Train score: 0.9632 | Val loss: 0.1621 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:48.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1081 | Train score: 0.9755 | Val loss: 0.1633 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:49.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1104 | Train score: 0.9632 | Val loss: 0.1651 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:51.093\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1145 | Train score: 0.9571 | Val loss: 0.1671 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:52.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.0939 | Train score: 0.9632 | Val loss: 0.1702 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:28:54.131\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1610 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:55.986\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1706 | Train score: 0.9387 | Val loss: 0.1536 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:57.737\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1602 | Train score: 0.9387 | Val loss: 0.1543 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:28:59.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1654 | Train score: 0.9509 | Val loss: 0.1542 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:01.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1343 | Train score: 0.9387 | Val loss: 0.1555 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:02.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1123 | Train score: 0.9632 | Val loss: 0.1579 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:04.375\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1582 | Train score: 0.9448 | Val loss: 0.1611 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:06.140\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1208 | Train score: 0.9448 | Val loss: 0.1670 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:07.728\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1406 | Train score: 0.9509 | Val loss: 0.1712 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:09.304\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1471 | Train score: 0.9448 | Val loss: 0.1730 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:10.853\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1449 | Train score: 0.9509 | Val loss: 0.1743 | Val score: 0.9507\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:29:12.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1404 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:14.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1446 | Train score: 0.9509 | Val loss: 0.1395 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:15.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1643 | Train score: 0.9509 | Val loss: 0.1374 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:17.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1674 | Train score: 0.9448 | Val loss: 0.1369 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:18.833\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1400 | Train score: 0.9509 | Val loss: 0.1351 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:20.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1087 | Train score: 0.9571 | Val loss: 0.1331 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:22.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1037 | Train score: 0.9693 | Val loss: 0.1329 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:23.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1350 | Train score: 0.9509 | Val loss: 0.1349 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:25.205\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1848 | Train score: 0.9448 | Val loss: 0.1366 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:26.758\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1231 | Train score: 0.9755 | Val loss: 0.1390 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:28.376\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1282 | Train score: 0.9693 | Val loss: 0.1412 | Val score: 0.9606\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:29:29.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1828 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:31.430\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1983 | Train score: 0.9325 | Val loss: 0.1791 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:32.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1813 | Train score: 0.9387 | Val loss: 0.1775 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:34.341\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1556 | Train score: 0.9387 | Val loss: 0.1754 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:35.760\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2014 | Train score: 0.9264 | Val loss: 0.1739 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:37.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1789 | Train score: 0.9448 | Val loss: 0.1727 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:38.610\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1458 | Train score: 0.9448 | Val loss: 0.1719 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:40.008\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1458 | Train score: 0.9448 | Val loss: 0.1717 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:41.469\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1411 | Train score: 0.9571 | Val loss: 0.1721 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:42.891\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1549 | Train score: 0.9387 | Val loss: 0.1735 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:44.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1754 | Train score: 0.9448 | Val loss: 0.1732 | Val score: 0.9507\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:29:45.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1806 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:47.465\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1708 | Train score: 0.9387 | Val loss: 0.1728 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:48.860\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1785 | Train score: 0.9448 | Val loss: 0.1727 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:50.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1913 | Train score: 0.9448 | Val loss: 0.1732 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:51.754\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1586 | Train score: 0.9509 | Val loss: 0.1741 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:53.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1798 | Train score: 0.9448 | Val loss: 0.1749 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:54.643\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1580 | Train score: 0.9509 | Val loss: 0.1754 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:56.103\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1904 | Train score: 0.9387 | Val loss: 0.1754 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:57.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1270 | Train score: 0.9448 | Val loss: 0.1746 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:29:59.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1462 | Train score: 0.9509 | Val loss: 0.1734 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:30:00.567\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1177 | Train score: 0.9387 | Val loss: 0.1725 | Val score: 0.9360\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.938         0.010           0.401          0.259        0.257       0.176         0.619        0.087    0.305   0.200         0.014        0.001\n",
      "MedPFNClassifier                0.934         0.015           0.448          0.112        0.429       0.116         0.697        0.061    0.435   0.108         1.605        0.012\n",
      "RandomForestClassifier          0.939         0.004           0.214          0.259        0.100       0.120         0.546        0.056    0.132   0.155         0.335        0.006\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.005        0.001\n",
      "TabPFNClassifier                0.946         0.008           0.586          0.136        0.243       0.129         0.616        0.064    0.331   0.154         2.163        0.024\n",
      "TabForestPFNClassifier          0.939         0.013           0.624          0.265        0.300       0.141         0.640        0.066    0.350   0.131        16.227        0.987\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:30:33.382\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2165 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:30:34.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1458 | Train score: 0.9264 | Val loss: 0.2309 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:30:36.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1505 | Train score: 0.9325 | Val loss: 0.2341 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:30:37.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1573 | Train score: 0.9387 | Val loss: 0.2327 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:30:39.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1484 | Train score: 0.9325 | Val loss: 0.2309 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:30:40.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1533 | Train score: 0.9325 | Val loss: 0.2293 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:30:41.912\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1173 | Train score: 0.9693 | Val loss: 0.2297 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:30:43.411\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1480 | Train score: 0.9448 | Val loss: 0.2287 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 15:30:44.866\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1323 | Train score: 0.9571 | Val loss: 0.2288 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 15:30:46.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1512 | Train score: 0.9325 | Val loss: 0.2283 | Val score: 0.9163\u001b[0m\n",
      "\u001b[32m2024-10-20 15:30:47.822\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1473 | Train score: 0.9509 | Val loss: 0.2286 | Val score: 0.9212\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:30:49.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1438 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:30:50.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1493 | Train score: 0.9387 | Val loss: 0.1381 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:30:52.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1610 | Train score: 0.9509 | Val loss: 0.1349 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:30:54.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1679 | Train score: 0.9448 | Val loss: 0.1337 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:30:55.724\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1796 | Train score: 0.9264 | Val loss: 0.1347 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:30:57.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1788 | Train score: 0.9387 | Val loss: 0.1361 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:30:58.797\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1427 | Train score: 0.9448 | Val loss: 0.1337 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:00.294\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1623 | Train score: 0.9448 | Val loss: 0.1316 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:01.878\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1389 | Train score: 0.9387 | Val loss: 0.1299 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:03.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1561 | Train score: 0.9387 | Val loss: 0.1298 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:05.018\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1386 | Train score: 0.9448 | Val loss: 0.1304 | Val score: 0.9606\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:31:06.675\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1208 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:08.093\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1675 | Train score: 0.9387 | Val loss: 0.1234 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:09.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1662 | Train score: 0.9387 | Val loss: 0.1264 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:10.981\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1407 | Train score: 0.9387 | Val loss: 0.1204 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:12.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1250 | Train score: 0.9448 | Val loss: 0.1150 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:13.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1752 | Train score: 0.9448 | Val loss: 0.1164 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:15.554\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1549 | Train score: 0.9571 | Val loss: 0.1162 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:17.036\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1554 | Train score: 0.9448 | Val loss: 0.1168 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:18.573\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1868 | Train score: 0.9264 | Val loss: 0.1169 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:20.034\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1904 | Train score: 0.9264 | Val loss: 0.1211 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:21.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1396 | Train score: 0.9448 | Val loss: 0.1270 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:31:23.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1499 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:24.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1525 | Train score: 0.9509 | Val loss: 0.1507 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:25.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1790 | Train score: 0.9325 | Val loss: 0.1512 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:27.277\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1318 | Train score: 0.9509 | Val loss: 0.1516 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:28.677\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1372 | Train score: 0.9571 | Val loss: 0.1523 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:30.048\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1699 | Train score: 0.9448 | Val loss: 0.1541 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:31.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1044 | Train score: 0.9693 | Val loss: 0.1572 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:32.798\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1227 | Train score: 0.9448 | Val loss: 0.1636 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:34.206\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1251 | Train score: 0.9509 | Val loss: 0.1708 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:35.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1091 | Train score: 0.9755 | Val loss: 0.1775 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:37.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1459 | Train score: 0.9387 | Val loss: 0.1828 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:31:38.781\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1215 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:40.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1501 | Train score: 0.9448 | Val loss: 0.1204 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:41.663\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1449 | Train score: 0.9509 | Val loss: 0.1183 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:43.163\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1154 | Train score: 0.9509 | Val loss: 0.1141 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:44.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1265 | Train score: 0.9632 | Val loss: 0.1102 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:46.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1889 | Train score: 0.9509 | Val loss: 0.1115 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:47.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2076 | Train score: 0.9325 | Val loss: 0.1139 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:49.213\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1214 | Train score: 0.9509 | Val loss: 0.1153 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:50.643\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1611 | Train score: 0.9448 | Val loss: 0.1153 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:52.140\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1633 | Train score: 0.9325 | Val loss: 0.1168 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:53.589\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1523 | Train score: 0.9509 | Val loss: 0.1191 | Val score: 0.9557\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:31:55.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1524 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:56.833\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1742 | Train score: 0.9448 | Val loss: 0.1499 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:31:58.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1554 | Train score: 0.9387 | Val loss: 0.1480 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:32:00.008\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1611 | Train score: 0.9387 | Val loss: 0.1491 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:32:01.686\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1386 | Train score: 0.9509 | Val loss: 0.1504 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:32:03.137\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1797 | Train score: 0.9264 | Val loss: 0.1496 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:32:04.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1838 | Train score: 0.9325 | Val loss: 0.1502 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:32:06.620\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1201 | Train score: 0.9325 | Val loss: 0.1516 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:32:08.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1731 | Train score: 0.9325 | Val loss: 0.1521 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:32:09.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1362 | Train score: 0.9509 | Val loss: 0.1512 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:32:11.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1829 | Train score: 0.9325 | Val loss: 0.1502 | Val score: 0.9606\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:32:12.922\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1832 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:32:14.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1632 | Train score: 0.9448 | Val loss: 0.1829 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:32:15.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1952 | Train score: 0.9387 | Val loss: 0.1816 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:32:17.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2170 | Train score: 0.9387 | Val loss: 0.1785 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:32:18.864\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1552 | Train score: 0.9387 | Val loss: 0.1775 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:32:20.298\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1492 | Train score: 0.9325 | Val loss: 0.1768 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:32:21.731\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1549 | Train score: 0.9325 | Val loss: 0.1760 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:32:23.182\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1338 | Train score: 0.9571 | Val loss: 0.1754 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:32:24.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1639 | Train score: 0.9509 | Val loss: 0.1753 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:32:26.153\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1429 | Train score: 0.9448 | Val loss: 0.1765 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:32:27.672\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1477 | Train score: 0.9448 | Val loss: 0.1782 | Val score: 0.9409\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.938         0.008           0.342          0.232        0.286       0.203         0.633        0.098    0.311   0.216         0.017        0.003\n",
      "MedPFNClassifier                0.928         0.016           0.405          0.126        0.429       0.116         0.694        0.062    0.416   0.121         1.595        0.012\n",
      "RandomForestClassifier          0.942         0.006           0.296          0.371        0.114       0.146         0.554        0.070    0.156   0.187         0.344        0.014\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.005        0.000\n",
      "TabPFNClassifier                0.948         0.007           0.800          0.207        0.243       0.129         0.618        0.062    0.337   0.147         2.144        0.031\n",
      "TabForestPFNClassifier          0.939         0.008           0.613          0.254        0.300       0.141         0.640        0.065    0.346   0.118        16.267        0.639\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:33:00.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1844 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:02.054\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1499 | Train score: 0.9509 | Val loss: 0.1848 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:03.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1998 | Train score: 0.9264 | Val loss: 0.1834 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:04.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1646 | Train score: 0.9509 | Val loss: 0.1824 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:06.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1472 | Train score: 0.9387 | Val loss: 0.1831 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:07.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1901 | Train score: 0.9202 | Val loss: 0.1830 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:09.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1554 | Train score: 0.9509 | Val loss: 0.1847 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:10.902\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1377 | Train score: 0.9632 | Val loss: 0.1847 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:12.345\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1566 | Train score: 0.9448 | Val loss: 0.1964 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:13.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2118 | Train score: 0.9509 | Val loss: 0.1910 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:15.376\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1160 | Train score: 0.9632 | Val loss: 0.1942 | Val score: 0.9212\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:33:17.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1474 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:18.469\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1779 | Train score: 0.9448 | Val loss: 0.1440 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:19.959\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2091 | Train score: 0.9264 | Val loss: 0.1465 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:21.486\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1727 | Train score: 0.9264 | Val loss: 0.1469 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:22.991\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1549 | Train score: 0.9448 | Val loss: 0.1452 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:24.492\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2100 | Train score: 0.9387 | Val loss: 0.1470 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:25.976\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1703 | Train score: 0.9387 | Val loss: 0.1486 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:27.472\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1463 | Train score: 0.9387 | Val loss: 0.1475 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:29.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1305 | Train score: 0.9448 | Val loss: 0.1466 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:30.507\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1745 | Train score: 0.9387 | Val loss: 0.1476 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:32.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1725 | Train score: 0.9448 | Val loss: 0.1495 | Val score: 0.9507\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:33:33.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1249 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:35.188\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1530 | Train score: 0.9387 | Val loss: 0.1286 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:36.570\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1537 | Train score: 0.9448 | Val loss: 0.1339 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:37.996\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1810 | Train score: 0.9509 | Val loss: 0.1332 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:39.398\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1547 | Train score: 0.9325 | Val loss: 0.1326 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:40.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1228 | Train score: 0.9755 | Val loss: 0.1332 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:42.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2246 | Train score: 0.9387 | Val loss: 0.1332 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:43.615\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1466 | Train score: 0.9571 | Val loss: 0.1328 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:45.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1654 | Train score: 0.9387 | Val loss: 0.1323 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:46.398\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1540 | Train score: 0.9448 | Val loss: 0.1318 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:47.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1511 | Train score: 0.9387 | Val loss: 0.1311 | Val score: 0.9606\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:33:49.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1375 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:50.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1674 | Train score: 0.9325 | Val loss: 0.1333 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:52.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1591 | Train score: 0.9509 | Val loss: 0.1318 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:53.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1489 | Train score: 0.9448 | Val loss: 0.1281 | Val score: 0.9606\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:55.018\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1502 | Train score: 0.9509 | Val loss: 0.1239 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:56.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1492 | Train score: 0.9387 | Val loss: 0.1209 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:57.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1239 | Train score: 0.9571 | Val loss: 0.1191 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:33:59.300\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1524 | Train score: 0.9448 | Val loss: 0.1184 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:00.729\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1242 | Train score: 0.9448 | Val loss: 0.1185 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:02.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1624 | Train score: 0.9509 | Val loss: 0.1192 | Val score: 0.9655\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:03.429\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1391 | Train score: 0.9509 | Val loss: 0.1201 | Val score: 0.9655\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:34:05.059\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1396 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:06.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1280 | Train score: 0.9693 | Val loss: 0.1390 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:07.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1712 | Train score: 0.9509 | Val loss: 0.1377 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:09.351\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.0867 | Train score: 0.9755 | Val loss: 0.1405 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:10.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1115 | Train score: 0.9632 | Val loss: 0.1424 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:12.080\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1190 | Train score: 0.9571 | Val loss: 0.1447 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:13.559\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1601 | Train score: 0.9632 | Val loss: 0.1460 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:15.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1668 | Train score: 0.9448 | Val loss: 0.1477 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:16.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1580 | Train score: 0.9509 | Val loss: 0.1509 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:17.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1096 | Train score: 0.9632 | Val loss: 0.1563 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:19.447\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.0909 | Train score: 0.9571 | Val loss: 0.1639 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:34:21.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1333 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:22.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2185 | Train score: 0.9387 | Val loss: 0.1434 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:24.027\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1919 | Train score: 0.9387 | Val loss: 0.1421 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:25.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1761 | Train score: 0.9387 | Val loss: 0.1350 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:26.881\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1735 | Train score: 0.9387 | Val loss: 0.1287 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:28.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1850 | Train score: 0.9325 | Val loss: 0.1262 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:29.751\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1430 | Train score: 0.9448 | Val loss: 0.1237 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:31.306\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1804 | Train score: 0.9387 | Val loss: 0.1241 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:32.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2107 | Train score: 0.9325 | Val loss: 0.1253 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:34.162\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1653 | Train score: 0.9387 | Val loss: 0.1273 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:35.608\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2189 | Train score: 0.9387 | Val loss: 0.1300 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:34:37.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2261 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:38.736\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1569 | Train score: 0.9571 | Val loss: 0.2245 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:40.302\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1433 | Train score: 0.9571 | Val loss: 0.2277 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:41.794\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1903 | Train score: 0.9264 | Val loss: 0.2241 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:43.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1310 | Train score: 0.9571 | Val loss: 0.2262 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:44.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1584 | Train score: 0.9509 | Val loss: 0.2280 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:46.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1732 | Train score: 0.9325 | Val loss: 0.2290 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:47.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1388 | Train score: 0.9448 | Val loss: 0.2320 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:49.310\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1542 | Train score: 0.9325 | Val loss: 0.2345 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:50.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1306 | Train score: 0.9571 | Val loss: 0.2350 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:34:52.213\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1524 | Train score: 0.9571 | Val loss: 0.2344 | Val score: 0.9212\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.938         0.007           0.363          0.236        0.229       0.158         0.606        0.078    0.280   0.188         0.016        0.001\n",
      "MedPFNClassifier                0.925         0.006           0.380          0.041        0.443       0.118         0.699        0.056    0.405   0.070         1.603        0.021\n",
      "RandomForestClassifier          0.939         0.003           0.109          0.174        0.057       0.105         0.526        0.048    0.072   0.126         0.361        0.017\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.005        0.001\n",
      "TabPFNClassifier                0.947         0.008           0.738          0.276        0.186       0.099         0.590        0.050    0.284   0.135         2.148        0.030\n",
      "TabForestPFNClassifier          0.937         0.014           0.354          0.274        0.257       0.168         0.619        0.080    0.293   0.202        15.881        0.393\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:35:25.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1655 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:35:26.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1705 | Train score: 0.9264 | Val loss: 0.1645 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:35:28.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1689 | Train score: 0.9264 | Val loss: 0.1643 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:35:29.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2111 | Train score: 0.9325 | Val loss: 0.1655 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:35:31.457\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1856 | Train score: 0.9387 | Val loss: 0.1662 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:35:33.041\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1876 | Train score: 0.9387 | Val loss: 0.1661 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:35:34.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1579 | Train score: 0.9387 | Val loss: 0.1673 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:35:36.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1976 | Train score: 0.9387 | Val loss: 0.1646 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:35:37.634\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2118 | Train score: 0.9387 | Val loss: 0.1661 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:35:39.146\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1523 | Train score: 0.9387 | Val loss: 0.1638 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:35:40.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1543 | Train score: 0.9387 | Val loss: 0.1614 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:35:42.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2070 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:35:43.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1533 | Train score: 0.9325 | Val loss: 0.2190 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:35:45.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1605 | Train score: 0.9509 | Val loss: 0.2267 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:35:46.410\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1378 | Train score: 0.9448 | Val loss: 0.2350 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:35:47.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1708 | Train score: 0.9387 | Val loss: 0.2357 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:35:49.371\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1491 | Train score: 0.9509 | Val loss: 0.2361 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:35:50.892\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1317 | Train score: 0.9509 | Val loss: 0.2377 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:35:52.284\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1411 | Train score: 0.9387 | Val loss: 0.2383 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:35:53.807\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1224 | Train score: 0.9448 | Val loss: 0.2425 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:35:55.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1557 | Train score: 0.9509 | Val loss: 0.2425 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:35:56.604\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1465 | Train score: 0.9509 | Val loss: 0.2393 | Val score: 0.9310\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:35:58.220\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1619 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:35:59.724\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1748 | Train score: 0.9387 | Val loss: 0.1650 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:01.314\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1404 | Train score: 0.9509 | Val loss: 0.1665 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:02.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1565 | Train score: 0.9387 | Val loss: 0.1676 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:04.289\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1520 | Train score: 0.9509 | Val loss: 0.1690 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:05.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1777 | Train score: 0.9325 | Val loss: 0.1694 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:07.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1318 | Train score: 0.9632 | Val loss: 0.1710 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:08.728\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1428 | Train score: 0.9509 | Val loss: 0.1717 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:10.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1415 | Train score: 0.9509 | Val loss: 0.1737 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:11.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1212 | Train score: 0.9632 | Val loss: 0.1762 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:13.340\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1394 | Train score: 0.9509 | Val loss: 0.1784 | Val score: 0.9261\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:36:14.992\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1800 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:16.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2053 | Train score: 0.9448 | Val loss: 0.1801 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:17.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1533 | Train score: 0.9387 | Val loss: 0.1826 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:19.302\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1593 | Train score: 0.9448 | Val loss: 0.1877 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:20.755\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1549 | Train score: 0.9448 | Val loss: 0.1945 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:22.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1813 | Train score: 0.9448 | Val loss: 0.1971 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:23.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1420 | Train score: 0.9509 | Val loss: 0.1990 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:25.272\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1483 | Train score: 0.9509 | Val loss: 0.2012 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:26.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1525 | Train score: 0.9448 | Val loss: 0.2050 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:28.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1061 | Train score: 0.9693 | Val loss: 0.2120 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:29.633\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1784 | Train score: 0.9325 | Val loss: 0.2201 | Val score: 0.9261\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:36:31.239\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1460 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:32.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1577 | Train score: 0.9387 | Val loss: 0.1373 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:34.398\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1357 | Train score: 0.9387 | Val loss: 0.1343 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:36.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2024 | Train score: 0.9387 | Val loss: 0.1329 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:37.716\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1749 | Train score: 0.9448 | Val loss: 0.1337 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:39.309\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1189 | Train score: 0.9693 | Val loss: 0.1328 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:40.918\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1456 | Train score: 0.9448 | Val loss: 0.1325 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:42.450\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1617 | Train score: 0.9325 | Val loss: 0.1330 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:44.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1580 | Train score: 0.9264 | Val loss: 0.1330 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:45.634\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2106 | Train score: 0.9325 | Val loss: 0.1337 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:47.285\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1685 | Train score: 0.9325 | Val loss: 0.1376 | Val score: 0.9606\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:36:48.927\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2003 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:50.438\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1504 | Train score: 0.9448 | Val loss: 0.2078 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:52.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1824 | Train score: 0.9448 | Val loss: 0.2082 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:53.701\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1561 | Train score: 0.9571 | Val loss: 0.2083 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:55.325\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2043 | Train score: 0.9264 | Val loss: 0.2061 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:56.857\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1380 | Train score: 0.9509 | Val loss: 0.2055 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:36:58.371\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1445 | Train score: 0.9387 | Val loss: 0.2074 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:37:00.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1678 | Train score: 0.9387 | Val loss: 0.2092 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:37:01.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1527 | Train score: 0.9448 | Val loss: 0.2123 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:37:03.387\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1639 | Train score: 0.9325 | Val loss: 0.2150 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:37:05.176\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2676 | Train score: 0.9202 | Val loss: 0.2120 | Val score: 0.9261\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:37:06.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1898 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:37:08.376\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2116 | Train score: 0.9325 | Val loss: 0.1804 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:37:09.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1684 | Train score: 0.9387 | Val loss: 0.1772 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:37:11.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1696 | Train score: 0.9509 | Val loss: 0.1765 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:37:12.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1891 | Train score: 0.9325 | Val loss: 0.1761 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:37:14.225\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1608 | Train score: 0.9448 | Val loss: 0.1761 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:37:15.880\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1784 | Train score: 0.9509 | Val loss: 0.1759 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:37:17.304\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1793 | Train score: 0.9387 | Val loss: 0.1760 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:37:18.693\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1503 | Train score: 0.9448 | Val loss: 0.1766 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:37:20.241\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1346 | Train score: 0.9509 | Val loss: 0.1778 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:37:21.829\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1933 | Train score: 0.9509 | Val loss: 0.1783 | Val score: 0.9409\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.934         0.009           0.278          0.249        0.171       0.167         0.577        0.083    0.211   0.198         0.017        0.001\n",
      "MedPFNClassifier                0.928         0.010           0.397          0.075        0.400       0.076         0.681        0.039    0.397   0.073         1.599        0.026\n",
      "RandomForestClassifier          0.938         0.004           0.093          0.152        0.043       0.073         0.519        0.032    0.059   0.098         0.377        0.021\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.005        0.000\n",
      "TabPFNClassifier                0.948         0.005           0.786          0.208        0.214       0.064         0.604        0.031    0.325   0.080         2.149        0.025\n",
      "TabForestPFNClassifier          0.941         0.006           0.524          0.279        0.214       0.146         0.600        0.068    0.272   0.150        16.584        0.653\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:37:54.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1804 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:37:56.292\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1920 | Train score: 0.9387 | Val loss: 0.1836 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:37:57.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1588 | Train score: 0.9509 | Val loss: 0.1822 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:37:59.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1843 | Train score: 0.9509 | Val loss: 0.1834 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:00.359\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1941 | Train score: 0.9387 | Val loss: 0.1838 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:01.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1723 | Train score: 0.9264 | Val loss: 0.1838 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:03.254\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1455 | Train score: 0.9509 | Val loss: 0.1841 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:04.604\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1453 | Train score: 0.9509 | Val loss: 0.1852 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:05.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1667 | Train score: 0.9325 | Val loss: 0.1860 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:07.404\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1473 | Train score: 0.9387 | Val loss: 0.1871 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:08.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1363 | Train score: 0.9509 | Val loss: 0.1879 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:38:10.394\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1727 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:11.896\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2353 | Train score: 0.9202 | Val loss: 0.1822 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:13.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1764 | Train score: 0.9387 | Val loss: 0.1803 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:14.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1906 | Train score: 0.9448 | Val loss: 0.1773 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:16.213\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2055 | Train score: 0.9387 | Val loss: 0.1771 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:17.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1909 | Train score: 0.9387 | Val loss: 0.1768 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:19.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1722 | Train score: 0.9387 | Val loss: 0.1755 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:20.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1689 | Train score: 0.9387 | Val loss: 0.1742 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:21.925\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1892 | Train score: 0.9387 | Val loss: 0.1735 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:23.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1893 | Train score: 0.9571 | Val loss: 0.1730 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:24.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1447 | Train score: 0.9387 | Val loss: 0.1725 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:38:26.396\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2023 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:27.804\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1438 | Train score: 0.9509 | Val loss: 0.2164 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:29.148\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1795 | Train score: 0.9509 | Val loss: 0.2220 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:30.594\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1508 | Train score: 0.9509 | Val loss: 0.2243 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:32.004\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1631 | Train score: 0.9448 | Val loss: 0.2264 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:33.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1503 | Train score: 0.9387 | Val loss: 0.2308 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:35.008\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1264 | Train score: 0.9571 | Val loss: 0.2412 | Val score: 0.9163\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:36.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1529 | Train score: 0.9509 | Val loss: 0.2510 | Val score: 0.9163\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:37.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1097 | Train score: 0.9755 | Val loss: 0.2577 | Val score: 0.9163\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:39.248\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1085 | Train score: 0.9632 | Val loss: 0.2680 | Val score: 0.9163\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:40.649\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1009 | Train score: 0.9632 | Val loss: 0.2843 | Val score: 0.9163\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:38:42.315\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1612 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:43.865\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1630 | Train score: 0.9448 | Val loss: 0.1665 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:45.486\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1424 | Train score: 0.9509 | Val loss: 0.1687 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:47.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1648 | Train score: 0.9325 | Val loss: 0.1717 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:48.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2039 | Train score: 0.9509 | Val loss: 0.1686 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:49.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1279 | Train score: 0.9448 | Val loss: 0.1683 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:51.080\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1668 | Train score: 0.9387 | Val loss: 0.1685 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:52.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1820 | Train score: 0.9387 | Val loss: 0.1687 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:53.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2040 | Train score: 0.9509 | Val loss: 0.1689 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:55.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1480 | Train score: 0.9448 | Val loss: 0.1698 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:56.867\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1825 | Train score: 0.9448 | Val loss: 0.1705 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:38:58.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1495 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:38:59.892\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2348 | Train score: 0.9325 | Val loss: 0.1592 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:01.315\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1767 | Train score: 0.9387 | Val loss: 0.1599 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:02.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1675 | Train score: 0.9387 | Val loss: 0.1590 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:04.149\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1700 | Train score: 0.9448 | Val loss: 0.1582 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:05.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2112 | Train score: 0.9448 | Val loss: 0.1579 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:07.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1781 | Train score: 0.9387 | Val loss: 0.1577 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:08.418\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1456 | Train score: 0.9509 | Val loss: 0.1568 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:09.880\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2090 | Train score: 0.9325 | Val loss: 0.1581 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:11.473\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1271 | Train score: 0.9448 | Val loss: 0.1602 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:12.912\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1377 | Train score: 0.9448 | Val loss: 0.1634 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:39:14.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1917 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:15.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1600 | Train score: 0.9509 | Val loss: 0.2065 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:17.266\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2102 | Train score: 0.9387 | Val loss: 0.2026 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:18.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1915 | Train score: 0.9387 | Val loss: 0.1976 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:20.019\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1802 | Train score: 0.9325 | Val loss: 0.1955 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:21.411\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1506 | Train score: 0.9509 | Val loss: 0.1957 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:22.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1459 | Train score: 0.9448 | Val loss: 0.1977 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:24.148\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1736 | Train score: 0.9387 | Val loss: 0.2015 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:25.553\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1927 | Train score: 0.9202 | Val loss: 0.2041 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:26.894\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1248 | Train score: 0.9693 | Val loss: 0.2099 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:28.298\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1455 | Train score: 0.9448 | Val loss: 0.2147 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:39:29.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1693 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:31.261\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1460 | Train score: 0.9387 | Val loss: 0.1711 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:32.694\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1798 | Train score: 0.9325 | Val loss: 0.1706 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:34.230\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1752 | Train score: 0.9325 | Val loss: 0.1706 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:35.615\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1820 | Train score: 0.9325 | Val loss: 0.1746 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:36.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1771 | Train score: 0.9325 | Val loss: 0.1778 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:38.360\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1387 | Train score: 0.9325 | Val loss: 0.1796 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:39.867\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1533 | Train score: 0.9264 | Val loss: 0.1819 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:41.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1637 | Train score: 0.9325 | Val loss: 0.1832 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:42.608\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1649 | Train score: 0.9448 | Val loss: 0.1817 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:39:44.060\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1238 | Train score: 0.9509 | Val loss: 0.1854 | Val score: 0.9458\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.934         0.004           0.432          0.033        0.329       0.148         0.650        0.068    0.350   0.107         0.017        0.002\n",
      "MedPFNClassifier                0.946         0.005           0.544          0.042        0.500       0.076         0.737        0.038    0.520   0.059         1.602        0.021\n",
      "RandomForestClassifier          0.941         0.003           0.139          0.224        0.086       0.146         0.540        0.069    0.105   0.175         0.372        0.024\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.004        0.001\n",
      "TabPFNClassifier                0.948         0.005           0.690          0.159        0.200       0.076         0.597        0.038    0.306   0.100         2.157        0.016\n",
      "TabForestPFNClassifier          0.942         0.004           0.435          0.185        0.214       0.136         0.601        0.065    0.278   0.153        15.534        0.269\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:40:16.990\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1669 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:40:18.448\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2063 | Train score: 0.9387 | Val loss: 0.1666 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:40:19.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1378 | Train score: 0.9448 | Val loss: 0.1644 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:40:21.392\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1608 | Train score: 0.9448 | Val loss: 0.1637 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:40:22.864\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1623 | Train score: 0.9509 | Val loss: 0.1639 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:40:24.399\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1397 | Train score: 0.9509 | Val loss: 0.1669 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:40:25.983\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1476 | Train score: 0.9448 | Val loss: 0.1737 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:40:27.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1649 | Train score: 0.9387 | Val loss: 0.1795 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:40:29.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.0805 | Train score: 0.9816 | Val loss: 0.1865 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:40:30.672\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1518 | Train score: 0.9387 | Val loss: 0.1927 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:40:32.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2078 | Train score: 0.9387 | Val loss: 0.1917 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:40:33.852\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1575 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:40:35.297\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1858 | Train score: 0.9448 | Val loss: 0.1541 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:40:36.790\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2298 | Train score: 0.9325 | Val loss: 0.1576 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:40:38.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1746 | Train score: 0.9387 | Val loss: 0.1587 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:40:39.634\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1884 | Train score: 0.9325 | Val loss: 0.1600 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:40:41.122\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2044 | Train score: 0.9387 | Val loss: 0.1611 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:40:42.595\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2037 | Train score: 0.9387 | Val loss: 0.1625 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:40:44.115\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1647 | Train score: 0.9387 | Val loss: 0.1618 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:40:45.525\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1947 | Train score: 0.9325 | Val loss: 0.1619 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:40:47.007\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1724 | Train score: 0.9509 | Val loss: 0.1619 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:40:48.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1472 | Train score: 0.9448 | Val loss: 0.1620 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:40:50.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1542 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:40:51.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1560 | Train score: 0.9387 | Val loss: 0.1557 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:40:53.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1664 | Train score: 0.9325 | Val loss: 0.1599 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:40:54.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1557 | Train score: 0.9387 | Val loss: 0.1584 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:40:56.172\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1450 | Train score: 0.9571 | Val loss: 0.1597 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:40:57.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1405 | Train score: 0.9509 | Val loss: 0.1642 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:40:59.214\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1536 | Train score: 0.9448 | Val loss: 0.1644 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:00.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1759 | Train score: 0.9387 | Val loss: 0.1634 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:02.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1818 | Train score: 0.9448 | Val loss: 0.1638 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:04.066\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1889 | Train score: 0.9509 | Val loss: 0.1666 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:05.614\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1672 | Train score: 0.9448 | Val loss: 0.1700 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:41:07.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1653 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:08.717\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1475 | Train score: 0.9448 | Val loss: 0.1742 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:10.206\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1557 | Train score: 0.9018 | Val loss: 0.1767 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:11.717\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1737 | Train score: 0.9448 | Val loss: 0.1761 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:13.279\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1603 | Train score: 0.9387 | Val loss: 0.1756 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:14.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1626 | Train score: 0.9325 | Val loss: 0.1747 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:16.214\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1282 | Train score: 0.9325 | Val loss: 0.1772 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:17.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1715 | Train score: 0.9387 | Val loss: 0.1756 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:19.081\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1853 | Train score: 0.9387 | Val loss: 0.1749 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:20.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1084 | Train score: 0.9693 | Val loss: 0.1792 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:21.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2023 | Train score: 0.9387 | Val loss: 0.1812 | Val score: 0.9507\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:41:23.579\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1669 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:25.003\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1426 | Train score: 0.9509 | Val loss: 0.1780 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:26.475\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1873 | Train score: 0.9387 | Val loss: 0.1804 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:27.919\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1456 | Train score: 0.9509 | Val loss: 0.1793 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:29.320\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1618 | Train score: 0.9509 | Val loss: 0.1783 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:30.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1908 | Train score: 0.9448 | Val loss: 0.1787 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:32.322\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1600 | Train score: 0.9571 | Val loss: 0.1790 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:33.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1436 | Train score: 0.9448 | Val loss: 0.1813 | Val score: 0.9163\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:35.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1484 | Train score: 0.9387 | Val loss: 0.1831 | Val score: 0.9163\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:36.685\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1296 | Train score: 0.9509 | Val loss: 0.1858 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:38.105\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1445 | Train score: 0.9509 | Val loss: 0.1876 | Val score: 0.9212\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:41:39.746\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1646 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:41.146\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1838 | Train score: 0.9325 | Val loss: 0.1618 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:42.569\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1898 | Train score: 0.9325 | Val loss: 0.1646 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:43.989\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1697 | Train score: 0.9325 | Val loss: 0.1701 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:45.471\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1706 | Train score: 0.9325 | Val loss: 0.1758 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:46.929\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.3305 | Train score: 0.9080 | Val loss: 0.1746 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:48.548\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2249 | Train score: 0.9325 | Val loss: 0.1756 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:49.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2198 | Train score: 0.9325 | Val loss: 0.1768 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:51.383\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1680 | Train score: 0.9387 | Val loss: 0.1772 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:52.854\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1831 | Train score: 0.9387 | Val loss: 0.1770 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:54.245\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1766 | Train score: 0.9387 | Val loss: 0.1776 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:41:55.894\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1536 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:57.405\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1588 | Train score: 0.9448 | Val loss: 0.1561 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:41:58.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1507 | Train score: 0.9448 | Val loss: 0.1576 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:42:00.441\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1763 | Train score: 0.9448 | Val loss: 0.1543 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:42:02.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1361 | Train score: 0.9571 | Val loss: 0.1533 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:42:03.599\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1386 | Train score: 0.9448 | Val loss: 0.1523 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:42:05.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1475 | Train score: 0.9387 | Val loss: 0.1526 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:42:06.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1243 | Train score: 0.9571 | Val loss: 0.1541 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:42:08.149\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1661 | Train score: 0.9264 | Val loss: 0.1560 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:42:09.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1309 | Train score: 0.9509 | Val loss: 0.1579 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:42:11.219\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1369 | Train score: 0.9571 | Val loss: 0.1619 | Val score: 0.9557\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.926         0.010           0.289          0.143        0.157       0.090         0.566        0.046    0.196   0.100         0.017        0.001\n",
      "MedPFNClassifier                0.947         0.005           0.559          0.067        0.500       0.076         0.737        0.037    0.525   0.055         1.587        0.012\n",
      "RandomForestClassifier          0.942         0.002           0.153          0.243        0.100       0.160         0.547        0.076    0.121   0.193         0.371        0.030\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.005        0.000\n",
      "TabPFNClassifier                0.943         0.003           0.619          0.172        0.129       0.045         0.562        0.022    0.209   0.063         2.132        0.012\n",
      "TabForestPFNClassifier          0.943         0.005           0.447          0.203        0.243       0.168         0.615        0.081    0.303   0.185        16.259        0.386\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:42:44.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1885 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:42:45.591\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1624 | Train score: 0.9448 | Val loss: 0.2055 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:42:47.272\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2030 | Train score: 0.9141 | Val loss: 0.2163 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:42:48.938\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1626 | Train score: 0.9387 | Val loss: 0.2152 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:42:50.658\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1655 | Train score: 0.9202 | Val loss: 0.2120 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:42:52.415\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1562 | Train score: 0.9448 | Val loss: 0.2094 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:42:54.021\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1627 | Train score: 0.9387 | Val loss: 0.2065 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:42:55.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1658 | Train score: 0.9448 | Val loss: 0.2014 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:42:57.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1205 | Train score: 0.9448 | Val loss: 0.2039 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:42:59.099\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1661 | Train score: 0.9448 | Val loss: 0.2036 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:00.883\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1618 | Train score: 0.9387 | Val loss: 0.2018 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:43:02.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1822 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:03.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2006 | Train score: 0.9387 | Val loss: 0.1804 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:05.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1780 | Train score: 0.9387 | Val loss: 0.1799 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:06.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1931 | Train score: 0.9325 | Val loss: 0.1798 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:08.397\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2097 | Train score: 0.9325 | Val loss: 0.1789 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:09.901\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1531 | Train score: 0.9264 | Val loss: 0.1789 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:11.402\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1653 | Train score: 0.9325 | Val loss: 0.1804 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:12.895\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1960 | Train score: 0.9448 | Val loss: 0.1823 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:14.351\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1622 | Train score: 0.9325 | Val loss: 0.1853 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:15.836\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1559 | Train score: 0.9448 | Val loss: 0.1893 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:17.341\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1794 | Train score: 0.9325 | Val loss: 0.1912 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:43:18.975\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1690 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:20.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1620 | Train score: 0.9448 | Val loss: 0.1733 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:21.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1053 | Train score: 0.9571 | Val loss: 0.1859 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:23.234\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1559 | Train score: 0.9448 | Val loss: 0.1949 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:24.728\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2156 | Train score: 0.9141 | Val loss: 0.1927 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:26.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1349 | Train score: 0.9632 | Val loss: 0.1866 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:27.564\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1580 | Train score: 0.9387 | Val loss: 0.1800 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:29.030\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1632 | Train score: 0.9387 | Val loss: 0.1755 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:30.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1564 | Train score: 0.9509 | Val loss: 0.1730 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:31.904\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1765 | Train score: 0.9264 | Val loss: 0.1716 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:33.399\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1404 | Train score: 0.9387 | Val loss: 0.1706 | Val score: 0.9557\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:43:35.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1663 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:36.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1782 | Train score: 0.9264 | Val loss: 0.1677 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:37.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1729 | Train score: 0.9387 | Val loss: 0.1682 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:39.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2018 | Train score: 0.9325 | Val loss: 0.1698 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:40.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1961 | Train score: 0.9387 | Val loss: 0.1722 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:42.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1477 | Train score: 0.9387 | Val loss: 0.1753 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:43.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1464 | Train score: 0.9509 | Val loss: 0.1796 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:45.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1227 | Train score: 0.9387 | Val loss: 0.1857 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:46.839\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1400 | Train score: 0.9325 | Val loss: 0.1944 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:48.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1841 | Train score: 0.9448 | Val loss: 0.1997 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:50.115\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1084 | Train score: 0.9387 | Val loss: 0.2073 | Val score: 0.9310\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:43:51.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1904 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:53.155\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1694 | Train score: 0.9387 | Val loss: 0.1861 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:54.591\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1501 | Train score: 0.9509 | Val loss: 0.1873 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:56.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1296 | Train score: 0.9571 | Val loss: 0.1899 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:57.583\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1356 | Train score: 0.9448 | Val loss: 0.1924 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:43:59.094\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1491 | Train score: 0.9448 | Val loss: 0.1938 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:44:00.559\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1712 | Train score: 0.9448 | Val loss: 0.1970 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:44:01.972\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1289 | Train score: 0.9509 | Val loss: 0.1995 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:44:03.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1116 | Train score: 0.9632 | Val loss: 0.2015 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:44:04.857\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1387 | Train score: 0.9509 | Val loss: 0.2039 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:44:06.293\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1500 | Train score: 0.9509 | Val loss: 0.2064 | Val score: 0.9261\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:44:07.954\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1712 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:44:09.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1736 | Train score: 0.9387 | Val loss: 0.1737 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:44:10.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1731 | Train score: 0.9325 | Val loss: 0.1818 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:44:12.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1823 | Train score: 0.9202 | Val loss: 0.1865 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:44:13.869\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1602 | Train score: 0.9509 | Val loss: 0.1841 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:44:15.498\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1898 | Train score: 0.9387 | Val loss: 0.1800 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:44:16.983\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2040 | Train score: 0.9448 | Val loss: 0.1794 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:44:18.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1848 | Train score: 0.9387 | Val loss: 0.1800 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:44:20.228\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1902 | Train score: 0.9448 | Val loss: 0.1813 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:44:21.895\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1791 | Train score: 0.9387 | Val loss: 0.1813 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:44:23.603\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1831 | Train score: 0.9387 | Val loss: 0.1836 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:44:25.272\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1634 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:44:26.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2046 | Train score: 0.9387 | Val loss: 0.1745 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:44:28.432\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2252 | Train score: 0.9387 | Val loss: 0.1795 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:44:29.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1931 | Train score: 0.9387 | Val loss: 0.1776 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:44:31.574\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1812 | Train score: 0.9387 | Val loss: 0.1720 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:44:33.106\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2195 | Train score: 0.9325 | Val loss: 0.1691 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:44:34.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1870 | Train score: 0.9387 | Val loss: 0.1659 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:44:36.251\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1607 | Train score: 0.9387 | Val loss: 0.1618 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:44:37.799\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1903 | Train score: 0.9387 | Val loss: 0.1596 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:44:39.343\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1732 | Train score: 0.9325 | Val loss: 0.1571 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:44:40.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1626 | Train score: 0.9387 | Val loss: 0.1555 | Val score: 0.9360\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.937         0.015           0.429          0.216        0.229       0.148         0.605        0.077    0.294   0.179         0.019        0.004\n",
      "MedPFNClassifier                0.937         0.010           0.471          0.088        0.457       0.090         0.712        0.047    0.463   0.088         1.589        0.012\n",
      "RandomForestClassifier          0.944         0.006           0.200          0.321        0.100       0.160         0.549        0.078    0.133   0.214         0.357        0.021\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.005        0.000\n",
      "TabPFNClassifier                0.944         0.003           0.476          0.431        0.086       0.083         0.542        0.040    0.140   0.130         2.145        0.029\n",
      "TabForestPFNClassifier          0.939         0.009           0.378          0.360        0.157       0.168         0.573        0.082    0.201   0.198        16.637        0.766\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:45:13.851\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1718 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:15.310\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2153 | Train score: 0.9387 | Val loss: 0.1807 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:16.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1827 | Train score: 0.9387 | Val loss: 0.1785 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:18.292\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2197 | Train score: 0.9387 | Val loss: 0.1812 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:19.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1943 | Train score: 0.9325 | Val loss: 0.1793 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:21.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1867 | Train score: 0.9387 | Val loss: 0.1767 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:23.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1727 | Train score: 0.9387 | Val loss: 0.1749 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:24.585\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1729 | Train score: 0.9387 | Val loss: 0.1733 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:26.122\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1841 | Train score: 0.9387 | Val loss: 0.1723 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:27.680\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2248 | Train score: 0.9202 | Val loss: 0.1712 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:29.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1611 | Train score: 0.9387 | Val loss: 0.1703 | Val score: 0.9507\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:45:30.852\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1881 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:32.454\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1740 | Train score: 0.9387 | Val loss: 0.1896 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:33.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1751 | Train score: 0.9448 | Val loss: 0.1911 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:35.390\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2028 | Train score: 0.9325 | Val loss: 0.1923 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:36.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2015 | Train score: 0.9387 | Val loss: 0.1918 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:38.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1943 | Train score: 0.9387 | Val loss: 0.1906 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:39.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1727 | Train score: 0.9448 | Val loss: 0.1913 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:41.140\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1943 | Train score: 0.9202 | Val loss: 0.1917 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:42.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1663 | Train score: 0.9448 | Val loss: 0.1923 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:43.981\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1930 | Train score: 0.9264 | Val loss: 0.1922 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:45.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1753 | Train score: 0.9448 | Val loss: 0.1925 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:45:47.155\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2143 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:48.584\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1829 | Train score: 0.9509 | Val loss: 0.2076 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:50.195\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1615 | Train score: 0.9571 | Val loss: 0.2077 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:51.608\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1691 | Train score: 0.9509 | Val loss: 0.2083 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:53.008\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1583 | Train score: 0.9509 | Val loss: 0.2110 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:54.431\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1729 | Train score: 0.9509 | Val loss: 0.2135 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:56.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2060 | Train score: 0.9325 | Val loss: 0.2206 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:57.431\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1508 | Train score: 0.9509 | Val loss: 0.2237 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 15:45:58.852\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1735 | Train score: 0.9509 | Val loss: 0.2249 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:00.292\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1299 | Train score: 0.9571 | Val loss: 0.2290 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:01.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1223 | Train score: 0.9632 | Val loss: 0.2386 | Val score: 0.9212\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:46:03.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1677 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:04.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1856 | Train score: 0.9387 | Val loss: 0.1711 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:06.332\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1933 | Train score: 0.9325 | Val loss: 0.1699 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:07.799\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2425 | Train score: 0.9264 | Val loss: 0.1692 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:09.259\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1703 | Train score: 0.9387 | Val loss: 0.1686 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:10.755\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1894 | Train score: 0.9448 | Val loss: 0.1689 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:12.212\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1626 | Train score: 0.9387 | Val loss: 0.1683 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:13.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1894 | Train score: 0.9448 | Val loss: 0.1666 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:15.351\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1513 | Train score: 0.9448 | Val loss: 0.1655 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:16.836\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1616 | Train score: 0.9264 | Val loss: 0.1678 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:18.353\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1661 | Train score: 0.9448 | Val loss: 0.1680 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:46:19.989\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1458 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:21.448\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1722 | Train score: 0.9509 | Val loss: 0.1487 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:22.873\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1734 | Train score: 0.9448 | Val loss: 0.1491 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:24.287\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1376 | Train score: 0.9448 | Val loss: 0.1480 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:25.705\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1938 | Train score: 0.9448 | Val loss: 0.1476 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:27.121\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1462 | Train score: 0.9571 | Val loss: 0.1490 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:28.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1724 | Train score: 0.9448 | Val loss: 0.1506 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:29.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1271 | Train score: 0.9509 | Val loss: 0.1527 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:31.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1599 | Train score: 0.9448 | Val loss: 0.1537 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:32.797\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1370 | Train score: 0.9387 | Val loss: 0.1549 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:34.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1464 | Train score: 0.9509 | Val loss: 0.1561 | Val score: 0.9557\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:46:35.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1935 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:37.280\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1793 | Train score: 0.9387 | Val loss: 0.1976 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:38.864\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1603 | Train score: 0.9387 | Val loss: 0.2086 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:40.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1473 | Train score: 0.9387 | Val loss: 0.2228 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:41.945\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1904 | Train score: 0.9387 | Val loss: 0.2250 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:43.415\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1621 | Train score: 0.9448 | Val loss: 0.2279 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:45.066\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1821 | Train score: 0.9387 | Val loss: 0.2271 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:46.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1391 | Train score: 0.9632 | Val loss: 0.2272 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:48.193\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1595 | Train score: 0.9387 | Val loss: 0.2267 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:49.762\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1591 | Train score: 0.9325 | Val loss: 0.2260 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:51.227\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1895 | Train score: 0.9325 | Val loss: 0.2244 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:46:52.913\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1680 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:54.532\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1778 | Train score: 0.9387 | Val loss: 0.1657 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:56.110\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1904 | Train score: 0.9264 | Val loss: 0.1688 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:57.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1717 | Train score: 0.9387 | Val loss: 0.1699 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:46:59.125\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1986 | Train score: 0.9264 | Val loss: 0.1703 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:47:00.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2606 | Train score: 0.9387 | Val loss: 0.1675 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:47:02.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1830 | Train score: 0.9387 | Val loss: 0.1681 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:47:03.950\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1730 | Train score: 0.9448 | Val loss: 0.1694 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:47:05.538\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1670 | Train score: 0.9387 | Val loss: 0.1701 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:47:07.154\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1627 | Train score: 0.9387 | Val loss: 0.1705 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:47:08.634\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1853 | Train score: 0.9325 | Val loss: 0.1707 | Val score: 0.9409\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.925         0.006           0.224          0.093        0.143       0.073         0.558        0.031    0.172   0.080         0.017        0.001\n",
      "MedPFNClassifier                0.937         0.011           0.456          0.096        0.414       0.125         0.692        0.063    0.432   0.110         1.594        0.029\n",
      "RandomForestClassifier          0.943         0.004           0.179          0.290        0.071       0.116         0.534        0.056    0.102   0.166         0.364        0.036\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.004        0.000\n",
      "TabPFNClassifier                0.944         0.003           0.381          0.330        0.114       0.099         0.555        0.048    0.176   0.152         2.143        0.025\n",
      "TabForestPFNClassifier          0.946         0.007           0.514          0.380        0.171       0.128         0.583        0.062    0.248   0.178        16.348        0.515\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:47:41.262\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1748 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:47:42.681\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1858 | Train score: 0.9448 | Val loss: 0.1752 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:47:44.140\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1767 | Train score: 0.9448 | Val loss: 0.1759 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:47:45.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1897 | Train score: 0.9448 | Val loss: 0.1757 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:47:47.182\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1553 | Train score: 0.9387 | Val loss: 0.1777 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:47:48.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1610 | Train score: 0.9387 | Val loss: 0.1822 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:47:50.230\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1539 | Train score: 0.9387 | Val loss: 0.1870 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:47:51.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1459 | Train score: 0.9387 | Val loss: 0.1942 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:47:53.229\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1679 | Train score: 0.9264 | Val loss: 0.1957 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:47:54.817\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2103 | Train score: 0.9448 | Val loss: 0.1915 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:47:56.466\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1335 | Train score: 0.9387 | Val loss: 0.1907 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:47:58.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1815 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:47:59.731\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1679 | Train score: 0.9387 | Val loss: 0.1791 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:01.297\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2195 | Train score: 0.9264 | Val loss: 0.1798 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:02.859\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2280 | Train score: 0.9448 | Val loss: 0.1813 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:04.432\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1812 | Train score: 0.9509 | Val loss: 0.1825 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:06.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1767 | Train score: 0.9448 | Val loss: 0.1825 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:07.804\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1835 | Train score: 0.9448 | Val loss: 0.1821 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:09.561\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2012 | Train score: 0.9264 | Val loss: 0.1826 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:11.107\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2080 | Train score: 0.9387 | Val loss: 0.1846 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:12.744\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1851 | Train score: 0.9387 | Val loss: 0.1864 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:14.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1605 | Train score: 0.9448 | Val loss: 0.1862 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:48:15.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1857 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:17.507\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1606 | Train score: 0.9387 | Val loss: 0.1842 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:18.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1769 | Train score: 0.9387 | Val loss: 0.1837 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:20.319\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1498 | Train score: 0.9325 | Val loss: 0.1894 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:21.723\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1779 | Train score: 0.9325 | Val loss: 0.1925 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:23.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1532 | Train score: 0.9387 | Val loss: 0.1985 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:24.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1435 | Train score: 0.9632 | Val loss: 0.2041 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:25.951\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1377 | Train score: 0.9509 | Val loss: 0.2093 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:27.485\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1694 | Train score: 0.9448 | Val loss: 0.2099 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:29.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1345 | Train score: 0.9325 | Val loss: 0.2104 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:30.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1148 | Train score: 0.9448 | Val loss: 0.2152 | Val score: 0.9261\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:48:32.095\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1834 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:33.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2029 | Train score: 0.9264 | Val loss: 0.1836 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:35.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1811 | Train score: 0.9387 | Val loss: 0.1829 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:36.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1997 | Train score: 0.9325 | Val loss: 0.1830 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:38.308\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1699 | Train score: 0.9387 | Val loss: 0.1842 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:39.729\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2046 | Train score: 0.9325 | Val loss: 0.1846 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:41.193\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1567 | Train score: 0.9448 | Val loss: 0.1867 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:42.662\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1558 | Train score: 0.9387 | Val loss: 0.1886 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:44.137\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1608 | Train score: 0.9387 | Val loss: 0.1919 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:45.612\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2654 | Train score: 0.9387 | Val loss: 0.1920 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:47.079\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1493 | Train score: 0.9509 | Val loss: 0.1922 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:48:48.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1505 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:50.174\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1776 | Train score: 0.9325 | Val loss: 0.1480 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:51.544\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1452 | Train score: 0.9387 | Val loss: 0.1505 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:52.925\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1555 | Train score: 0.9387 | Val loss: 0.1559 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:54.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1656 | Train score: 0.9325 | Val loss: 0.1676 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:55.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2155 | Train score: 0.9387 | Val loss: 0.1713 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:57.229\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1426 | Train score: 0.9448 | Val loss: 0.1744 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:48:58.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1510 | Train score: 0.9448 | Val loss: 0.1768 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:49:00.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1662 | Train score: 0.9448 | Val loss: 0.1787 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:49:01.441\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1146 | Train score: 0.9571 | Val loss: 0.1819 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:49:02.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1610 | Train score: 0.9264 | Val loss: 0.1834 | Val score: 0.9310\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:49:04.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1677 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:49:05.880\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2362 | Train score: 0.9387 | Val loss: 0.1732 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:49:07.274\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1936 | Train score: 0.9387 | Val loss: 0.1726 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:49:08.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1626 | Train score: 0.9387 | Val loss: 0.1697 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:49:10.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1922 | Train score: 0.9325 | Val loss: 0.1694 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:49:11.693\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2091 | Train score: 0.9387 | Val loss: 0.1699 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:49:13.069\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1968 | Train score: 0.9325 | Val loss: 0.1700 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:49:14.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1650 | Train score: 0.9448 | Val loss: 0.1697 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:49:16.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1617 | Train score: 0.9448 | Val loss: 0.1700 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:49:17.615\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1687 | Train score: 0.9509 | Val loss: 0.1711 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:49:19.026\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1936 | Train score: 0.9325 | Val loss: 0.1733 | Val score: 0.9507\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:49:20.671\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2039 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:49:22.106\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1958 | Train score: 0.9387 | Val loss: 0.2012 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:49:23.541\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1836 | Train score: 0.9325 | Val loss: 0.2036 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:49:24.992\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1587 | Train score: 0.9387 | Val loss: 0.2131 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:49:26.526\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1490 | Train score: 0.9325 | Val loss: 0.2240 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:49:27.992\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1729 | Train score: 0.9448 | Val loss: 0.2327 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:49:29.445\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1460 | Train score: 0.9448 | Val loss: 0.2357 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:49:30.912\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2193 | Train score: 0.9202 | Val loss: 0.2352 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:49:32.400\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1286 | Train score: 0.9509 | Val loss: 0.2402 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:49:33.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1755 | Train score: 0.9387 | Val loss: 0.2424 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:49:35.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1736 | Train score: 0.9448 | Val loss: 0.2439 | Val score: 0.9310\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.933         0.006           0.337          0.168        0.143       0.105         0.563        0.050    0.189   0.116         0.019        0.002\n",
      "MedPFNClassifier                0.934         0.005           0.433          0.046        0.357       0.049         0.664        0.024    0.390   0.042         1.581        0.012\n",
      "RandomForestClassifier          0.941         0.000           0.143          0.226        0.029       0.045         0.513        0.021    0.048   0.075         0.346        0.019\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.005        0.000\n",
      "TabPFNClassifier                0.944         0.003           0.476          0.431        0.086       0.083         0.542        0.040    0.140   0.130         2.137        0.023\n",
      "TabForestPFNClassifier          0.943         0.003           0.529          0.271        0.229       0.158         0.608        0.074    0.289   0.163        16.242        0.627\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:50:08.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2037 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:10.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2070 | Train score: 0.9448 | Val loss: 0.1997 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:11.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1947 | Train score: 0.9325 | Val loss: 0.1997 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:13.309\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1900 | Train score: 0.9387 | Val loss: 0.1975 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:14.875\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1877 | Train score: 0.9448 | Val loss: 0.1961 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:16.406\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1843 | Train score: 0.9325 | Val loss: 0.1953 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:17.992\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2052 | Train score: 0.9325 | Val loss: 0.1949 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:19.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1863 | Train score: 0.9387 | Val loss: 0.1949 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:20.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1938 | Train score: 0.9387 | Val loss: 0.1957 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:22.485\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2109 | Train score: 0.9387 | Val loss: 0.1954 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:23.895\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1961 | Train score: 0.9325 | Val loss: 0.1949 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:50:25.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1849 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:26.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2301 | Train score: 0.9141 | Val loss: 0.1868 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:28.268\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1861 | Train score: 0.9387 | Val loss: 0.1912 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:29.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1629 | Train score: 0.9387 | Val loss: 0.1890 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:31.225\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2060 | Train score: 0.9325 | Val loss: 0.1853 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:32.591\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1737 | Train score: 0.9387 | Val loss: 0.1881 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:34.008\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1985 | Train score: 0.9264 | Val loss: 0.1904 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:35.399\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1949 | Train score: 0.9325 | Val loss: 0.1907 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:36.821\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1519 | Train score: 0.9448 | Val loss: 0.1906 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:38.177\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1954 | Train score: 0.9448 | Val loss: 0.1896 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:39.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1740 | Train score: 0.9325 | Val loss: 0.1890 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:50:41.352\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1957 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:42.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2252 | Train score: 0.9325 | Val loss: 0.1946 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:44.182\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2173 | Train score: 0.9387 | Val loss: 0.1901 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:45.673\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1935 | Train score: 0.9387 | Val loss: 0.1899 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:47.105\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1873 | Train score: 0.9387 | Val loss: 0.1950 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:48.580\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2047 | Train score: 0.9325 | Val loss: 0.2002 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:49.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1535 | Train score: 0.9387 | Val loss: 0.2027 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:51.326\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1657 | Train score: 0.9448 | Val loss: 0.2076 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:52.729\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1981 | Train score: 0.9387 | Val loss: 0.2097 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:54.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1809 | Train score: 0.9141 | Val loss: 0.2096 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:55.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2325 | Train score: 0.9325 | Val loss: 0.2047 | Val score: 0.9310\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:50:57.266\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1945 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:50:58.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1764 | Train score: 0.9387 | Val loss: 0.1990 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:00.239\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1884 | Train score: 0.9387 | Val loss: 0.2028 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:01.643\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2289 | Train score: 0.9387 | Val loss: 0.2009 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:03.150\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1829 | Train score: 0.9448 | Val loss: 0.2010 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:04.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1714 | Train score: 0.9448 | Val loss: 0.2017 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:06.148\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1942 | Train score: 0.9448 | Val loss: 0.2014 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:07.594\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2260 | Train score: 0.9387 | Val loss: 0.2006 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:09.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1691 | Train score: 0.9448 | Val loss: 0.2010 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:10.434\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1701 | Train score: 0.9264 | Val loss: 0.2020 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:11.832\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2464 | Train score: 0.9387 | Val loss: 0.2021 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:51:13.459\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1776 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:14.846\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1973 | Train score: 0.9387 | Val loss: 0.1755 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:16.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2363 | Train score: 0.9325 | Val loss: 0.1750 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:17.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1837 | Train score: 0.9448 | Val loss: 0.1726 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:19.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1915 | Train score: 0.9264 | Val loss: 0.1711 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:20.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2197 | Train score: 0.9325 | Val loss: 0.1710 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:21.938\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2415 | Train score: 0.9264 | Val loss: 0.1726 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:23.336\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1822 | Train score: 0.9387 | Val loss: 0.1743 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:24.807\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1780 | Train score: 0.9387 | Val loss: 0.1736 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:26.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1813 | Train score: 0.9387 | Val loss: 0.1721 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:27.602\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1702 | Train score: 0.9387 | Val loss: 0.1695 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:51:29.211\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1804 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:30.596\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1793 | Train score: 0.9509 | Val loss: 0.1803 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:32.095\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1417 | Train score: 0.9387 | Val loss: 0.1897 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:33.553\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2565 | Train score: 0.9141 | Val loss: 0.1837 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:34.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1848 | Train score: 0.9448 | Val loss: 0.1816 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:36.553\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1762 | Train score: 0.9387 | Val loss: 0.1817 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:37.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2049 | Train score: 0.9325 | Val loss: 0.1826 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:39.428\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1867 | Train score: 0.9509 | Val loss: 0.1836 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:40.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1734 | Train score: 0.9387 | Val loss: 0.1851 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:42.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1809 | Train score: 0.9387 | Val loss: 0.1861 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:43.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1517 | Train score: 0.9387 | Val loss: 0.1879 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:51:45.326\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1793 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:46.693\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1675 | Train score: 0.9448 | Val loss: 0.1777 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:48.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2043 | Train score: 0.9325 | Val loss: 0.1788 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:49.870\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2257 | Train score: 0.9387 | Val loss: 0.1832 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:51.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1920 | Train score: 0.9387 | Val loss: 0.1846 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:52.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1804 | Train score: 0.9387 | Val loss: 0.1885 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:54.054\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1809 | Train score: 0.9387 | Val loss: 0.1944 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:55.461\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1965 | Train score: 0.9325 | Val loss: 0.1993 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:56.865\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1887 | Train score: 0.9448 | Val loss: 0.2044 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:58.410\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1549 | Train score: 0.9387 | Val loss: 0.2128 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:51:59.822\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1680 | Train score: 0.9509 | Val loss: 0.2192 | Val score: 0.9261\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.927         0.007           0.167          0.148        0.057       0.049         0.520        0.026    0.085   0.074         0.019        0.002\n",
      "MedPFNClassifier                0.927         0.009           0.327          0.145        0.257       0.129         0.613        0.063    0.282   0.131         1.583        0.016\n",
      "RandomForestClassifier          0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.334        0.013\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.004        0.000\n",
      "TabPFNClassifier                0.943         0.003           0.190          0.301        0.057       0.090         0.528        0.044    0.088   0.139         2.242        0.143\n",
      "TabForestPFNClassifier          0.944         0.006           0.357          0.440        0.071       0.103         0.535        0.051    0.116   0.160        15.826        0.341\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:52:32.284\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1973 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:52:33.618\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1750 | Train score: 0.9325 | Val loss: 0.2071 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:52:34.965\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1822 | Train score: 0.9325 | Val loss: 0.2121 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:52:36.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1979 | Train score: 0.9202 | Val loss: 0.2086 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:52:37.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1443 | Train score: 0.9448 | Val loss: 0.2093 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 15:52:39.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1506 | Train score: 0.9448 | Val loss: 0.2115 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 15:52:40.438\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1671 | Train score: 0.9387 | Val loss: 0.2132 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 15:52:41.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1804 | Train score: 0.9509 | Val loss: 0.2141 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 15:52:43.201\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1788 | Train score: 0.9202 | Val loss: 0.2137 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:52:44.558\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1765 | Train score: 0.9448 | Val loss: 0.2131 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:52:45.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1757 | Train score: 0.9387 | Val loss: 0.2124 | Val score: 0.9261\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:52:47.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2040 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:52:48.893\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2093 | Train score: 0.9325 | Val loss: 0.2045 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:52:50.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2073 | Train score: 0.9387 | Val loss: 0.1990 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:52:51.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1898 | Train score: 0.9387 | Val loss: 0.2027 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:52:53.147\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2096 | Train score: 0.9387 | Val loss: 0.2059 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:52:54.744\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2131 | Train score: 0.9264 | Val loss: 0.2076 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:52:56.088\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1854 | Train score: 0.9448 | Val loss: 0.2097 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:52:57.495\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1588 | Train score: 0.9448 | Val loss: 0.2151 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:52:58.975\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1644 | Train score: 0.9448 | Val loss: 0.2272 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:00.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1751 | Train score: 0.9448 | Val loss: 0.2309 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:01.973\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1641 | Train score: 0.9264 | Val loss: 0.2423 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:53:03.595\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1841 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:04.966\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1839 | Train score: 0.9387 | Val loss: 0.1845 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:06.340\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2053 | Train score: 0.9387 | Val loss: 0.1858 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:07.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1929 | Train score: 0.9387 | Val loss: 0.1866 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:09.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1884 | Train score: 0.9325 | Val loss: 0.1933 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:10.529\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1322 | Train score: 0.9571 | Val loss: 0.2033 | Val score: 0.9163\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:11.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1608 | Train score: 0.9325 | Val loss: 0.2077 | Val score: 0.9064\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:13.292\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2004 | Train score: 0.9387 | Val loss: 0.2047 | Val score: 0.9064\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:14.697\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1368 | Train score: 0.9509 | Val loss: 0.2005 | Val score: 0.9113\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:16.117\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2174 | Train score: 0.9080 | Val loss: 0.1936 | Val score: 0.9163\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:17.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1640 | Train score: 0.9509 | Val loss: 0.1908 | Val score: 0.9212\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:53:19.114\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1845 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:20.717\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2001 | Train score: 0.9387 | Val loss: 0.1870 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:22.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2002 | Train score: 0.9387 | Val loss: 0.1901 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:23.632\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1596 | Train score: 0.9448 | Val loss: 0.1934 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:25.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2296 | Train score: 0.9387 | Val loss: 0.1936 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:26.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1961 | Train score: 0.9509 | Val loss: 0.1919 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:27.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2520 | Train score: 0.9448 | Val loss: 0.1892 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:29.329\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2031 | Train score: 0.9325 | Val loss: 0.1873 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:30.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1728 | Train score: 0.9509 | Val loss: 0.1863 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:32.358\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1741 | Train score: 0.9387 | Val loss: 0.1859 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:33.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2284 | Train score: 0.9325 | Val loss: 0.1864 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:53:35.418\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1796 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:36.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1921 | Train score: 0.9387 | Val loss: 0.1782 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:38.335\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1987 | Train score: 0.9325 | Val loss: 0.1773 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:39.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1842 | Train score: 0.9387 | Val loss: 0.1772 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:41.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1743 | Train score: 0.9325 | Val loss: 0.1781 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:42.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2435 | Train score: 0.9264 | Val loss: 0.1764 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:44.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1810 | Train score: 0.9387 | Val loss: 0.1754 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:45.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1907 | Train score: 0.9325 | Val loss: 0.1750 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:47.390\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1837 | Train score: 0.9509 | Val loss: 0.1768 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:48.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1712 | Train score: 0.9264 | Val loss: 0.1802 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:50.375\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1260 | Train score: 0.9448 | Val loss: 0.1841 | Val score: 0.9310\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:53:51.984\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1893 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:53.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2152 | Train score: 0.9387 | Val loss: 0.1749 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:54.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2064 | Train score: 0.9387 | Val loss: 0.1734 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:56.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2080 | Train score: 0.9264 | Val loss: 0.1700 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:58.217\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2196 | Train score: 0.9387 | Val loss: 0.1705 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:53:59.644\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1966 | Train score: 0.9448 | Val loss: 0.1658 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:54:01.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2353 | Train score: 0.9325 | Val loss: 0.1734 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:54:02.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2121 | Train score: 0.9448 | Val loss: 0.1763 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:54:04.387\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1870 | Train score: 0.9387 | Val loss: 0.1760 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:54:05.869\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2155 | Train score: 0.9387 | Val loss: 0.1754 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:54:07.393\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1818 | Train score: 0.9448 | Val loss: 0.1734 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:54:09.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1787 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:54:10.496\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2160 | Train score: 0.9387 | Val loss: 0.1857 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:54:11.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1984 | Train score: 0.9387 | Val loss: 0.1869 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:54:13.549\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1776 | Train score: 0.9387 | Val loss: 0.1918 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:54:14.909\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1873 | Train score: 0.9387 | Val loss: 0.1845 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:54:16.382\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2049 | Train score: 0.9387 | Val loss: 0.1839 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:54:17.833\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1838 | Train score: 0.9325 | Val loss: 0.1832 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:54:19.429\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1833 | Train score: 0.9448 | Val loss: 0.1829 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:54:20.869\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1830 | Train score: 0.9325 | Val loss: 0.1827 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:54:22.348\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1866 | Train score: 0.9448 | Val loss: 0.1825 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:54:23.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1668 | Train score: 0.9509 | Val loss: 0.1820 | Val score: 0.9360\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.926         0.011           0.238          0.196        0.129       0.116         0.553        0.059    0.165   0.144         0.018        0.002\n",
      "MedPFNClassifier                0.942         0.017           0.531          0.348        0.286       0.146         0.634        0.076    0.365   0.198         1.585        0.016\n",
      "RandomForestClassifier          0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.329        0.012\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.005        0.000\n",
      "TabPFNClassifier                0.943         0.003           0.286          0.452        0.029       0.045         0.514        0.023    0.052   0.082         2.132        0.021\n",
      "TabForestPFNClassifier          0.942         0.005           0.500          0.463        0.057       0.049         0.527        0.025    0.102   0.088        15.867        0.566\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:54:56.145\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1467 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:54:57.471\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2117 | Train score: 0.9448 | Val loss: 0.1672 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:54:58.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2145 | Train score: 0.9387 | Val loss: 0.1587 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:00.182\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1955 | Train score: 0.9387 | Val loss: 0.1481 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:01.548\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1936 | Train score: 0.9387 | Val loss: 0.1401 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:02.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1972 | Train score: 0.9325 | Val loss: 0.1364 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:04.254\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2277 | Train score: 0.9448 | Val loss: 0.1377 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:05.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2036 | Train score: 0.9325 | Val loss: 0.1396 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:07.340\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1786 | Train score: 0.9387 | Val loss: 0.1395 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:08.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1804 | Train score: 0.9387 | Val loss: 0.1386 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:10.262\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1846 | Train score: 0.9387 | Val loss: 0.1382 | Val score: 0.9507\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:55:11.877\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1991 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:13.204\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2187 | Train score: 0.9387 | Val loss: 0.1665 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:14.544\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1900 | Train score: 0.9387 | Val loss: 0.1613 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:16.046\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1929 | Train score: 0.9325 | Val loss: 0.1586 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:17.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2167 | Train score: 0.9202 | Val loss: 0.1576 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:18.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1970 | Train score: 0.9448 | Val loss: 0.1583 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:20.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2431 | Train score: 0.9264 | Val loss: 0.1602 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:21.716\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1868 | Train score: 0.9387 | Val loss: 0.1625 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:23.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1824 | Train score: 0.9509 | Val loss: 0.1636 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:24.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1817 | Train score: 0.9387 | Val loss: 0.1621 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:25.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1785 | Train score: 0.9448 | Val loss: 0.1595 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:55:27.427\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1783 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:28.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1929 | Train score: 0.9387 | Val loss: 0.1691 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:30.185\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1923 | Train score: 0.9325 | Val loss: 0.1718 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:31.591\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2065 | Train score: 0.9387 | Val loss: 0.1774 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:32.954\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1788 | Train score: 0.9325 | Val loss: 0.1811 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:34.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1723 | Train score: 0.9448 | Val loss: 0.1821 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:35.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1843 | Train score: 0.9448 | Val loss: 0.1805 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:37.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1793 | Train score: 0.9387 | Val loss: 0.1801 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:38.644\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1663 | Train score: 0.9448 | Val loss: 0.1806 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:40.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2106 | Train score: 0.9325 | Val loss: 0.1788 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:41.628\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1707 | Train score: 0.9448 | Val loss: 0.1788 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:55:43.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2151 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:44.620\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1970 | Train score: 0.9387 | Val loss: 0.2219 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:46.015\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1746 | Train score: 0.9509 | Val loss: 0.2370 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:47.397\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2279 | Train score: 0.9509 | Val loss: 0.2394 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:48.798\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1870 | Train score: 0.9387 | Val loss: 0.2460 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:50.180\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1908 | Train score: 0.9448 | Val loss: 0.2464 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:51.571\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1847 | Train score: 0.9448 | Val loss: 0.2454 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:52.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1901 | Train score: 0.9387 | Val loss: 0.2425 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:54.372\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1924 | Train score: 0.9448 | Val loss: 0.2404 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:55.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1532 | Train score: 0.9448 | Val loss: 0.2416 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:55:57.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1952 | Train score: 0.9448 | Val loss: 0.2407 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:55:58.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1957 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:00.155\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2212 | Train score: 0.9387 | Val loss: 0.1878 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:01.537\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1796 | Train score: 0.9387 | Val loss: 0.1832 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:03.013\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2130 | Train score: 0.9264 | Val loss: 0.1819 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:04.411\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2211 | Train score: 0.9264 | Val loss: 0.1830 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:05.859\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1623 | Train score: 0.9448 | Val loss: 0.1818 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:07.266\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1722 | Train score: 0.9387 | Val loss: 0.1801 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:08.871\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1893 | Train score: 0.9387 | Val loss: 0.1809 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:10.454\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1696 | Train score: 0.9387 | Val loss: 0.1831 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:12.030\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1787 | Train score: 0.9325 | Val loss: 0.1874 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:13.430\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1572 | Train score: 0.9202 | Val loss: 0.1940 | Val score: 0.9261\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:56:15.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1978 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:16.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1812 | Train score: 0.9387 | Val loss: 0.2075 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:17.791\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1555 | Train score: 0.9387 | Val loss: 0.2211 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:19.166\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1805 | Train score: 0.9325 | Val loss: 0.2298 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:20.561\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2048 | Train score: 0.9202 | Val loss: 0.2299 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:21.946\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1626 | Train score: 0.9387 | Val loss: 0.2277 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:23.332\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1516 | Train score: 0.9264 | Val loss: 0.2246 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:24.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1974 | Train score: 0.9325 | Val loss: 0.2192 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:26.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1618 | Train score: 0.9387 | Val loss: 0.2154 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:27.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1863 | Train score: 0.9448 | Val loss: 0.2127 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:28.954\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1574 | Train score: 0.9325 | Val loss: 0.2118 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:56:30.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2303 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:32.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1876 | Train score: 0.9509 | Val loss: 0.2248 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:33.584\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1663 | Train score: 0.9448 | Val loss: 0.2266 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:34.951\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1784 | Train score: 0.9448 | Val loss: 0.2320 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:36.497\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1643 | Train score: 0.9387 | Val loss: 0.2432 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:38.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1851 | Train score: 0.9264 | Val loss: 0.2560 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:39.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1843 | Train score: 0.9325 | Val loss: 0.2568 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:41.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1291 | Train score: 0.9448 | Val loss: 0.2651 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:42.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2126 | Train score: 0.9448 | Val loss: 0.2628 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:44.080\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1604 | Train score: 0.9387 | Val loss: 0.2600 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:56:45.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1559 | Train score: 0.9448 | Val loss: 0.2579 | Val score: 0.9360\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.933         0.011           0.306          0.261        0.129       0.088         0.556        0.043    0.176   0.126         0.018        0.002\n",
      "MedPFNClassifier                0.944         0.020           0.565          0.212        0.400       0.141         0.689        0.075    0.463   0.170         1.575        0.019\n",
      "RandomForestClassifier          0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.317        0.004\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.005        0.001\n",
      "TabPFNClassifier                0.943         0.003           0.190          0.301        0.057       0.090         0.528        0.044    0.088   0.139         2.128        0.021\n",
      "TabForestPFNClassifier          0.938         0.003           0.214          0.247        0.043       0.049         0.519        0.024    0.071   0.082        15.577        0.399\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:57:17.883\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2037 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:57:19.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1616 | Train score: 0.9448 | Val loss: 0.2130 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:57:20.839\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1664 | Train score: 0.9448 | Val loss: 0.2238 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:57:22.284\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2160 | Train score: 0.9325 | Val loss: 0.2222 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:57:23.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2661 | Train score: 0.9141 | Val loss: 0.2160 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:57:25.213\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1817 | Train score: 0.9325 | Val loss: 0.2134 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:57:26.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1852 | Train score: 0.9202 | Val loss: 0.2123 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:57:28.210\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1833 | Train score: 0.9448 | Val loss: 0.2116 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:57:29.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2064 | Train score: 0.9202 | Val loss: 0.2106 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:57:31.214\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1626 | Train score: 0.9448 | Val loss: 0.2106 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:57:32.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1769 | Train score: 0.9325 | Val loss: 0.2117 | Val score: 0.9310\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:57:34.391\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1945 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:57:35.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2008 | Train score: 0.9387 | Val loss: 0.2040 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:57:37.177\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1943 | Train score: 0.9264 | Val loss: 0.2047 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:57:38.587\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2185 | Train score: 0.9387 | Val loss: 0.2020 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:57:39.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1808 | Train score: 0.9387 | Val loss: 0.2036 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:57:41.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1649 | Train score: 0.9387 | Val loss: 0.2094 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:57:42.738\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1964 | Train score: 0.9264 | Val loss: 0.2135 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:57:44.135\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1273 | Train score: 0.9387 | Val loss: 0.2243 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:57:45.526\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1900 | Train score: 0.9141 | Val loss: 0.2329 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:57:46.925\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1699 | Train score: 0.9387 | Val loss: 0.2391 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:57:48.298\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1424 | Train score: 0.9325 | Val loss: 0.2457 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:57:49.920\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1923 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:57:51.602\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1999 | Train score: 0.9387 | Val loss: 0.1987 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:57:53.185\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1736 | Train score: 0.9325 | Val loss: 0.2113 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:57:54.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2488 | Train score: 0.9141 | Val loss: 0.2087 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:57:56.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1932 | Train score: 0.9387 | Val loss: 0.2092 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:57:58.249\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1854 | Train score: 0.9448 | Val loss: 0.2120 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:57:59.859\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1589 | Train score: 0.9448 | Val loss: 0.2164 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:01.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1469 | Train score: 0.9387 | Val loss: 0.2286 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:03.105\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1883 | Train score: 0.9509 | Val loss: 0.2372 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:04.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1822 | Train score: 0.9387 | Val loss: 0.2433 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:06.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1497 | Train score: 0.9571 | Val loss: 0.2474 | Val score: 0.9310\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:58:08.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2111 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:09.672\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2405 | Train score: 0.9387 | Val loss: 0.2110 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:11.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2183 | Train score: 0.9387 | Val loss: 0.2120 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:12.673\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2099 | Train score: 0.9387 | Val loss: 0.2138 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:14.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1991 | Train score: 0.9387 | Val loss: 0.2152 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:15.486\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1881 | Train score: 0.9387 | Val loss: 0.2180 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:16.913\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1941 | Train score: 0.9387 | Val loss: 0.2224 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:18.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1862 | Train score: 0.9387 | Val loss: 0.2261 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:19.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1965 | Train score: 0.9448 | Val loss: 0.2318 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:21.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2341 | Train score: 0.9387 | Val loss: 0.2328 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:22.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2068 | Train score: 0.9448 | Val loss: 0.2306 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:58:24.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1845 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:26.147\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1913 | Train score: 0.9387 | Val loss: 0.1758 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:27.889\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1861 | Train score: 0.9387 | Val loss: 0.1739 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:29.480\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2368 | Train score: 0.9202 | Val loss: 0.1743 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:31.058\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1668 | Train score: 0.9448 | Val loss: 0.1762 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:32.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1606 | Train score: 0.9387 | Val loss: 0.1781 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:34.210\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2000 | Train score: 0.9325 | Val loss: 0.1796 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:35.867\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2008 | Train score: 0.9387 | Val loss: 0.1812 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:37.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2201 | Train score: 0.9264 | Val loss: 0.1808 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:39.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1574 | Train score: 0.9387 | Val loss: 0.1803 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:40.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1728 | Train score: 0.9448 | Val loss: 0.1791 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:58:42.470\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1950 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:43.806\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2314 | Train score: 0.9387 | Val loss: 0.1984 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:45.182\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2222 | Train score: 0.9387 | Val loss: 0.2047 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:46.579\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2056 | Train score: 0.9387 | Val loss: 0.1971 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:47.932\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2043 | Train score: 0.9387 | Val loss: 0.1961 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:49.377\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1998 | Train score: 0.9387 | Val loss: 0.1941 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:50.761\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1878 | Train score: 0.9387 | Val loss: 0.1945 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:52.137\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1789 | Train score: 0.9387 | Val loss: 0.1973 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:53.542\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2080 | Train score: 0.9141 | Val loss: 0.2028 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:54.925\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2199 | Train score: 0.9509 | Val loss: 0.2074 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:56.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1768 | Train score: 0.9387 | Val loss: 0.2143 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:58:57.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1943 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:58:59.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1858 | Train score: 0.9387 | Val loss: 0.2025 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:59:01.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2072 | Train score: 0.9448 | Val loss: 0.2050 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:59:02.736\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1949 | Train score: 0.9325 | Val loss: 0.2034 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:59:04.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1672 | Train score: 0.9387 | Val loss: 0.2101 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:59:05.738\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1686 | Train score: 0.9448 | Val loss: 0.2133 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:59:07.198\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1680 | Train score: 0.9264 | Val loss: 0.2148 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:59:08.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1488 | Train score: 0.9448 | Val loss: 0.2156 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:59:10.061\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1872 | Train score: 0.9325 | Val loss: 0.2148 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 15:59:11.790\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1796 | Train score: 0.9325 | Val loss: 0.2272 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:59:13.486\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1972 | Train score: 0.9325 | Val loss: 0.2143 | Val score: 0.9458\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.933         0.006           0.329          0.164        0.143       0.073         0.563        0.037    0.199   0.100         0.017        0.002\n",
      "MedPFNClassifier                0.948         0.013           0.607          0.276        0.300       0.177         0.645        0.089    0.388   0.203         1.576        0.011\n",
      "RandomForestClassifier          0.942         0.002           0.143          0.350        0.014       0.035         0.507        0.017    0.026   0.064         0.306        0.006\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.005        0.000\n",
      "TabPFNClassifier                0.941         0.000           0.143          0.226        0.029       0.045         0.513        0.021    0.048   0.075         2.142        0.027\n",
      "TabForestPFNClassifier          0.943         0.009           0.571          0.495        0.086       0.083         0.542        0.043    0.147   0.140        16.460        1.001\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 15:59:45.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1844 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:59:47.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2073 | Train score: 0.9387 | Val loss: 0.1660 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 15:59:48.507\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1817 | Train score: 0.9448 | Val loss: 0.1667 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:59:49.901\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1871 | Train score: 0.9387 | Val loss: 0.1687 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:59:51.302\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2443 | Train score: 0.9325 | Val loss: 0.1687 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:59:52.694\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1818 | Train score: 0.9325 | Val loss: 0.1689 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:59:54.082\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1551 | Train score: 0.9387 | Val loss: 0.1699 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:59:55.469\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1845 | Train score: 0.9325 | Val loss: 0.1704 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:59:56.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1738 | Train score: 0.9387 | Val loss: 0.1712 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 15:59:58.348\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1605 | Train score: 0.9325 | Val loss: 0.1727 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 15:59:59.731\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1603 | Train score: 0.9387 | Val loss: 0.1759 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:00:01.344\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1977 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:02.883\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1717 | Train score: 0.9387 | Val loss: 0.2047 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:04.384\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1629 | Train score: 0.9387 | Val loss: 0.2134 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:05.929\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2499 | Train score: 0.9325 | Val loss: 0.2113 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:07.272\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2341 | Train score: 0.9325 | Val loss: 0.2073 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:08.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1563 | Train score: 0.9387 | Val loss: 0.2057 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:10.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1575 | Train score: 0.9387 | Val loss: 0.2054 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:12.078\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1983 | Train score: 0.9387 | Val loss: 0.2049 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:13.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1647 | Train score: 0.9448 | Val loss: 0.2047 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:14.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1898 | Train score: 0.9387 | Val loss: 0.2038 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:16.474\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1804 | Train score: 0.9325 | Val loss: 0.2028 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:00:18.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1981 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:19.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1829 | Train score: 0.9387 | Val loss: 0.2045 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:20.891\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2023 | Train score: 0.9325 | Val loss: 0.2088 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:22.258\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2168 | Train score: 0.9202 | Val loss: 0.2098 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:23.705\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1740 | Train score: 0.9387 | Val loss: 0.2110 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:25.137\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1714 | Train score: 0.9509 | Val loss: 0.2122 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:26.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1688 | Train score: 0.9448 | Val loss: 0.2132 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:28.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1780 | Train score: 0.9387 | Val loss: 0.2138 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:29.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2030 | Train score: 0.9387 | Val loss: 0.2111 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:30.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1475 | Train score: 0.9387 | Val loss: 0.2083 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:32.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1642 | Train score: 0.9509 | Val loss: 0.2098 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:00:33.979\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2200 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:35.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2162 | Train score: 0.9387 | Val loss: 0.2261 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:37.140\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1936 | Train score: 0.9387 | Val loss: 0.2352 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:38.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1697 | Train score: 0.9387 | Val loss: 0.2457 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:40.415\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2115 | Train score: 0.9509 | Val loss: 0.2506 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:41.961\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2364 | Train score: 0.9325 | Val loss: 0.2441 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:43.555\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2335 | Train score: 0.9387 | Val loss: 0.2363 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:44.984\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1878 | Train score: 0.9264 | Val loss: 0.2333 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:46.569\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2097 | Train score: 0.9325 | Val loss: 0.2312 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:48.214\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1697 | Train score: 0.9509 | Val loss: 0.2295 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:49.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2219 | Train score: 0.9387 | Val loss: 0.2279 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:00:51.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1873 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:52.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1927 | Train score: 0.9387 | Val loss: 0.1881 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:54.445\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1799 | Train score: 0.9387 | Val loss: 0.1921 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:55.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1459 | Train score: 0.9387 | Val loss: 0.1947 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:57.393\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1894 | Train score: 0.9325 | Val loss: 0.1947 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:00:58.846\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2108 | Train score: 0.9264 | Val loss: 0.1938 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:01:00.287\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2258 | Train score: 0.9325 | Val loss: 0.1912 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:01:01.806\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1896 | Train score: 0.9387 | Val loss: 0.1900 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:01:03.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2041 | Train score: 0.9264 | Val loss: 0.1890 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:01:04.689\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1547 | Train score: 0.9387 | Val loss: 0.1891 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:01:06.150\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1701 | Train score: 0.9325 | Val loss: 0.1883 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:01:07.799\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1900 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:01:09.175\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1967 | Train score: 0.9387 | Val loss: 0.1854 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:01:10.746\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2094 | Train score: 0.9325 | Val loss: 0.1868 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:01:12.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2158 | Train score: 0.9264 | Val loss: 0.1849 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:01:13.954\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1765 | Train score: 0.9509 | Val loss: 0.1839 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:01:15.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1911 | Train score: 0.9448 | Val loss: 0.1825 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:01:17.122\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2204 | Train score: 0.9387 | Val loss: 0.1815 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:01:18.772\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1889 | Train score: 0.9387 | Val loss: 0.1808 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:01:20.346\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1885 | Train score: 0.9387 | Val loss: 0.1801 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:01:21.923\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2255 | Train score: 0.9387 | Val loss: 0.1805 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:01:23.495\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1991 | Train score: 0.9325 | Val loss: 0.1810 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:01:25.105\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1955 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:01:26.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2039 | Train score: 0.9387 | Val loss: 0.2010 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 16:01:28.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1867 | Train score: 0.9387 | Val loss: 0.2068 | Val score: 0.9163\u001b[0m\n",
      "\u001b[32m2024-10-20 16:01:29.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1812 | Train score: 0.9509 | Val loss: 0.2115 | Val score: 0.9163\u001b[0m\n",
      "\u001b[32m2024-10-20 16:01:30.900\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2153 | Train score: 0.9264 | Val loss: 0.2113 | Val score: 0.9163\u001b[0m\n",
      "\u001b[32m2024-10-20 16:01:32.350\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1883 | Train score: 0.9387 | Val loss: 0.2104 | Val score: 0.9163\u001b[0m\n",
      "\u001b[32m2024-10-20 16:01:33.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1993 | Train score: 0.9325 | Val loss: 0.2090 | Val score: 0.9163\u001b[0m\n",
      "\u001b[32m2024-10-20 16:01:35.349\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2082 | Train score: 0.9509 | Val loss: 0.2085 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 16:01:36.836\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1511 | Train score: 0.9448 | Val loss: 0.2095 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:01:38.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1890 | Train score: 0.9387 | Val loss: 0.2116 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:01:39.746\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1738 | Train score: 0.9325 | Val loss: 0.2134 | Val score: 0.9310\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.927         0.017           0.405          0.397        0.071       0.045         0.526        0.029    0.114   0.074         0.015        0.001\n",
      "MedPFNClassifier                0.951         0.013           0.656          0.261        0.329       0.158         0.659        0.080    0.429   0.187         1.590        0.019\n",
      "RandomForestClassifier          0.940         0.002           0.000          0.000        0.000       0.000         0.500        0.001    0.000   0.000         0.291        0.014\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.005        0.001\n",
      "TabPFNClassifier                0.939         0.003           0.000          0.000        0.000       0.000         0.499        0.001    0.000   0.000         2.144        0.028\n",
      "TabForestPFNClassifier          0.947         0.009           0.631          0.321        0.171       0.128         0.583        0.064    0.260   0.177        16.228        0.685\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:02:12.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1904 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:13.398\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2380 | Train score: 0.9387 | Val loss: 0.2189 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:14.749\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2287 | Train score: 0.9387 | Val loss: 0.2176 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:16.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2287 | Train score: 0.9387 | Val loss: 0.2149 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:17.496\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2249 | Train score: 0.9387 | Val loss: 0.2070 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:18.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2198 | Train score: 0.9387 | Val loss: 0.1984 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:20.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2091 | Train score: 0.9387 | Val loss: 0.1945 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:21.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1980 | Train score: 0.9387 | Val loss: 0.1934 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:22.954\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1848 | Train score: 0.9387 | Val loss: 0.1921 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:24.329\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1991 | Train score: 0.9387 | Val loss: 0.1915 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:25.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2093 | Train score: 0.9387 | Val loss: 0.1912 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:02:27.314\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1987 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:28.693\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2035 | Train score: 0.9387 | Val loss: 0.1955 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:30.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2080 | Train score: 0.9387 | Val loss: 0.1956 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:31.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2176 | Train score: 0.9387 | Val loss: 0.1923 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:32.956\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2028 | Train score: 0.9264 | Val loss: 0.1909 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:34.349\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2044 | Train score: 0.9325 | Val loss: 0.1894 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:35.792\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1948 | Train score: 0.9448 | Val loss: 0.1890 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:37.403\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2092 | Train score: 0.9387 | Val loss: 0.1887 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:38.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2106 | Train score: 0.9387 | Val loss: 0.1898 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:40.241\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1984 | Train score: 0.9387 | Val loss: 0.1896 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:41.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2156 | Train score: 0.9387 | Val loss: 0.1904 | Val score: 0.9310\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:02:43.261\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2129 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:44.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2094 | Train score: 0.9387 | Val loss: 0.2128 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:46.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1748 | Train score: 0.9387 | Val loss: 0.2234 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:47.468\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1551 | Train score: 0.9509 | Val loss: 0.2311 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:48.901\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2038 | Train score: 0.9387 | Val loss: 0.2308 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:50.336\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1711 | Train score: 0.9325 | Val loss: 0.2322 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:51.737\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1606 | Train score: 0.9325 | Val loss: 0.2358 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:53.147\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1896 | Train score: 0.9448 | Val loss: 0.2373 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:54.561\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2307 | Train score: 0.9325 | Val loss: 0.2320 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:55.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1588 | Train score: 0.9509 | Val loss: 0.2310 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:02:57.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1402 | Train score: 0.9509 | Val loss: 0.2316 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:02:59.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1972 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:00.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2142 | Train score: 0.9325 | Val loss: 0.2023 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:01.962\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2027 | Train score: 0.9387 | Val loss: 0.1999 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:03.402\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2210 | Train score: 0.9387 | Val loss: 0.2015 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:05.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2064 | Train score: 0.9387 | Val loss: 0.2033 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:06.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2008 | Train score: 0.9387 | Val loss: 0.2050 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:08.043\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1806 | Train score: 0.9387 | Val loss: 0.2082 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:09.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1990 | Train score: 0.9387 | Val loss: 0.2129 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:11.003\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2177 | Train score: 0.9387 | Val loss: 0.2144 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:12.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2071 | Train score: 0.9448 | Val loss: 0.2156 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:13.978\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1763 | Train score: 0.9325 | Val loss: 0.2148 | Val score: 0.9261\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:03:15.605\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2081 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:17.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1893 | Train score: 0.9387 | Val loss: 0.2000 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:18.437\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2153 | Train score: 0.9325 | Val loss: 0.1967 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:19.870\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1934 | Train score: 0.9387 | Val loss: 0.1991 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:21.358\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2522 | Train score: 0.9202 | Val loss: 0.2021 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:22.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2378 | Train score: 0.9325 | Val loss: 0.2057 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:24.384\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1876 | Train score: 0.9509 | Val loss: 0.2068 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:25.870\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1495 | Train score: 0.9509 | Val loss: 0.2075 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:27.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1894 | Train score: 0.9509 | Val loss: 0.2079 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:28.874\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2238 | Train score: 0.9387 | Val loss: 0.2076 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:30.357\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1778 | Train score: 0.9387 | Val loss: 0.2078 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:03:31.994\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1891 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:33.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2073 | Train score: 0.9387 | Val loss: 0.1789 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:34.891\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2298 | Train score: 0.9202 | Val loss: 0.1793 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:36.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2150 | Train score: 0.9325 | Val loss: 0.1809 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:37.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2202 | Train score: 0.9387 | Val loss: 0.1833 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:39.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2147 | Train score: 0.9387 | Val loss: 0.1858 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:40.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2391 | Train score: 0.9264 | Val loss: 0.1889 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:42.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1895 | Train score: 0.9387 | Val loss: 0.1901 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:43.514\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2085 | Train score: 0.9387 | Val loss: 0.1903 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:44.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2059 | Train score: 0.9387 | Val loss: 0.1896 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:46.368\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1919 | Train score: 0.9387 | Val loss: 0.1884 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:03:47.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1981 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:49.599\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2069 | Train score: 0.9387 | Val loss: 0.2023 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:50.978\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2144 | Train score: 0.9387 | Val loss: 0.2006 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:52.391\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1923 | Train score: 0.9387 | Val loss: 0.1991 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:53.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1719 | Train score: 0.9448 | Val loss: 0.1989 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:55.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1866 | Train score: 0.9448 | Val loss: 0.1993 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:56.730\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2215 | Train score: 0.9448 | Val loss: 0.1977 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:58.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2375 | Train score: 0.9325 | Val loss: 0.1968 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:03:59.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1775 | Train score: 0.9509 | Val loss: 0.1969 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:04:01.075\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1697 | Train score: 0.9448 | Val loss: 0.1975 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:04:02.486\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2010 | Train score: 0.9387 | Val loss: 0.1987 | Val score: 0.9360\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.941         0.011           0.503          0.341        0.200       0.120         0.594        0.060    0.277   0.163         0.016        0.001\n",
      "MedPFNClassifier                0.941         0.008           0.398          0.253        0.257       0.176         0.620        0.087    0.310   0.206         1.608        0.059\n",
      "RandomForestClassifier          0.940         0.002           0.000          0.000        0.000       0.000         0.500        0.001    0.000   0.000         0.288        0.011\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.005        0.001\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.133        0.024\n",
      "TabForestPFNClassifier          0.943         0.004           0.405          0.376        0.086       0.083         0.541        0.041    0.138   0.130        15.718        0.396\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:04:34.576\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2375 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 16:04:35.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1796 | Train score: 0.9448 | Val loss: 0.2689 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 16:04:37.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1983 | Train score: 0.9325 | Val loss: 0.2747 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 16:04:38.848\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2216 | Train score: 0.9325 | Val loss: 0.2627 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:04:40.448\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2014 | Train score: 0.9325 | Val loss: 0.2533 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:04:41.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1696 | Train score: 0.9387 | Val loss: 0.2481 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:04:43.624\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2481 | Train score: 0.9387 | Val loss: 0.2396 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:04:45.093\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1767 | Train score: 0.9387 | Val loss: 0.2363 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:04:46.584\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1677 | Train score: 0.9448 | Val loss: 0.2385 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:04:48.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1854 | Train score: 0.9448 | Val loss: 0.2408 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:04:49.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1751 | Train score: 0.9387 | Val loss: 0.2474 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:04:51.309\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2124 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:04:52.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2211 | Train score: 0.9387 | Val loss: 0.1822 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:04:54.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2260 | Train score: 0.9387 | Val loss: 0.1834 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:04:55.615\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2219 | Train score: 0.9325 | Val loss: 0.1879 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:04:57.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2502 | Train score: 0.9387 | Val loss: 0.1947 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:04:58.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2011 | Train score: 0.9387 | Val loss: 0.1993 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:04:59.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2011 | Train score: 0.9509 | Val loss: 0.2004 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:01.266\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1758 | Train score: 0.9325 | Val loss: 0.1994 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:02.615\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2067 | Train score: 0.9325 | Val loss: 0.1971 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:03.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1860 | Train score: 0.9325 | Val loss: 0.1955 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:05.318\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1879 | Train score: 0.9448 | Val loss: 0.1947 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:05:06.927\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1960 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:08.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2332 | Train score: 0.9387 | Val loss: 0.2222 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:09.686\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2267 | Train score: 0.9387 | Val loss: 0.2218 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:11.106\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2278 | Train score: 0.9387 | Val loss: 0.2199 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:12.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2229 | Train score: 0.9387 | Val loss: 0.2155 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:13.891\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2207 | Train score: 0.9387 | Val loss: 0.2076 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:15.248\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2161 | Train score: 0.9387 | Val loss: 0.1991 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:16.626\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2120 | Train score: 0.9387 | Val loss: 0.1923 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:18.004\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2083 | Train score: 0.9387 | Val loss: 0.1881 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:19.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2009 | Train score: 0.9387 | Val loss: 0.1856 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:20.772\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1972 | Train score: 0.9448 | Val loss: 0.1836 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:05:22.400\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1960 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:23.848\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2183 | Train score: 0.9387 | Val loss: 0.1908 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:25.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2511 | Train score: 0.9387 | Val loss: 0.1899 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:26.835\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1875 | Train score: 0.9387 | Val loss: 0.1904 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:28.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2334 | Train score: 0.9325 | Val loss: 0.1922 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:29.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2590 | Train score: 0.9387 | Val loss: 0.1950 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:31.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2063 | Train score: 0.9387 | Val loss: 0.1970 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:32.679\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2176 | Train score: 0.9387 | Val loss: 0.1981 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:34.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1934 | Train score: 0.9387 | Val loss: 0.1981 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:35.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2086 | Train score: 0.9387 | Val loss: 0.1972 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:37.325\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1942 | Train score: 0.9387 | Val loss: 0.1969 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:05:38.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2045 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:40.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1954 | Train score: 0.9387 | Val loss: 0.2119 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:41.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1853 | Train score: 0.9387 | Val loss: 0.2266 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:43.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1882 | Train score: 0.9264 | Val loss: 0.2288 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:44.539\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1719 | Train score: 0.9509 | Val loss: 0.2273 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:45.893\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2324 | Train score: 0.9264 | Val loss: 0.2218 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:47.348\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1657 | Train score: 0.9387 | Val loss: 0.2192 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:48.798\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1464 | Train score: 0.9509 | Val loss: 0.2190 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:50.291\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1698 | Train score: 0.9509 | Val loss: 0.2191 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:51.649\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1364 | Train score: 0.9509 | Val loss: 0.2209 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:53.077\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1411 | Train score: 0.9509 | Val loss: 0.2236 | Val score: 0.9261\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:05:54.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2182 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:56.361\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2103 | Train score: 0.9387 | Val loss: 0.2201 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:58.104\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2058 | Train score: 0.9325 | Val loss: 0.2216 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:05:59.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1691 | Train score: 0.9387 | Val loss: 0.2265 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:06:01.167\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2007 | Train score: 0.9387 | Val loss: 0.2248 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:06:02.576\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2400 | Train score: 0.9387 | Val loss: 0.2184 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:06:04.396\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2120 | Train score: 0.9202 | Val loss: 0.2153 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:06:06.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1782 | Train score: 0.9387 | Val loss: 0.2141 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:06:07.625\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1852 | Train score: 0.9387 | Val loss: 0.2138 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:06:09.345\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2114 | Train score: 0.9325 | Val loss: 0.2129 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:06:10.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2066 | Train score: 0.9325 | Val loss: 0.2121 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:06:12.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.1791 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:06:13.972\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2582 | Train score: 0.9264 | Val loss: 0.2210 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:06:15.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2293 | Train score: 0.9387 | Val loss: 0.2239 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:06:17.017\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2306 | Train score: 0.9387 | Val loss: 0.2239 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:06:18.525\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2299 | Train score: 0.9387 | Val loss: 0.2231 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:06:20.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2285 | Train score: 0.9387 | Val loss: 0.2218 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:06:21.908\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2280 | Train score: 0.9387 | Val loss: 0.2200 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:06:23.403\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2267 | Train score: 0.9387 | Val loss: 0.2183 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:06:25.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2245 | Train score: 0.9387 | Val loss: 0.2163 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:06:26.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2225 | Train score: 0.9387 | Val loss: 0.2133 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:06:28.182\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2211 | Train score: 0.9387 | Val loss: 0.2091 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.927         0.006           0.281          0.107        0.129       0.045         0.553        0.022    0.171   0.051         0.015        0.002\n",
      "MedPFNClassifier                0.945         0.013           0.523          0.200        0.314       0.164         0.650        0.084    0.390   0.185         1.580        0.017\n",
      "RandomForestClassifier          0.943         0.003           0.286          0.452        0.029       0.045         0.514        0.023    0.052   0.082         0.276        0.013\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.004        0.000\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.142        0.024\n",
      "TabForestPFNClassifier          0.945         0.005           0.393          0.460        0.086       0.112         0.542        0.055    0.135   0.169        16.171        0.827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:07:00.125\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2100 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:01.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2176 | Train score: 0.9387 | Val loss: 0.1965 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:02.915\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2045 | Train score: 0.9448 | Val loss: 0.1989 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:04.300\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2039 | Train score: 0.9448 | Val loss: 0.1994 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:05.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1773 | Train score: 0.9448 | Val loss: 0.1996 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:07.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1935 | Train score: 0.9387 | Val loss: 0.1981 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:08.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2347 | Train score: 0.9325 | Val loss: 0.1962 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:09.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1914 | Train score: 0.9448 | Val loss: 0.1948 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:11.239\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1975 | Train score: 0.9448 | Val loss: 0.1935 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:12.676\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1942 | Train score: 0.9325 | Val loss: 0.1925 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:14.028\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1826 | Train score: 0.9387 | Val loss: 0.1915 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:07:15.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2051 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:17.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2065 | Train score: 0.9387 | Val loss: 0.2057 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:18.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2112 | Train score: 0.9202 | Val loss: 0.2099 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:19.880\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2294 | Train score: 0.9387 | Val loss: 0.2065 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:21.277\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1818 | Train score: 0.9387 | Val loss: 0.2046 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:22.687\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1897 | Train score: 0.9387 | Val loss: 0.2033 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:24.076\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2057 | Train score: 0.9202 | Val loss: 0.2022 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:25.492\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1856 | Train score: 0.9325 | Val loss: 0.2014 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:26.870\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2123 | Train score: 0.9387 | Val loss: 0.2012 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:28.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1917 | Train score: 0.9387 | Val loss: 0.2008 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:29.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1894 | Train score: 0.9387 | Val loss: 0.2003 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:07:31.383\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2034 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:32.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.1972 | Train score: 0.9387 | Val loss: 0.2048 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:34.437\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1917 | Train score: 0.9387 | Val loss: 0.2083 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:35.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2245 | Train score: 0.9202 | Val loss: 0.2068 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:37.532\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2212 | Train score: 0.9325 | Val loss: 0.2037 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:38.927\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1667 | Train score: 0.9448 | Val loss: 0.2028 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:40.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2002 | Train score: 0.9387 | Val loss: 0.2021 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:41.833\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1681 | Train score: 0.9387 | Val loss: 0.2016 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:43.358\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2021 | Train score: 0.9448 | Val loss: 0.2005 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:44.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1959 | Train score: 0.9387 | Val loss: 0.2001 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:46.400\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1785 | Train score: 0.9387 | Val loss: 0.2002 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:07:48.015\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2116 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:49.372\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2215 | Train score: 0.9387 | Val loss: 0.1931 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:50.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1871 | Train score: 0.9387 | Val loss: 0.1976 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:52.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1984 | Train score: 0.9387 | Val loss: 0.2006 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:53.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2040 | Train score: 0.9325 | Val loss: 0.1999 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:54.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2064 | Train score: 0.9448 | Val loss: 0.1977 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:56.257\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2362 | Train score: 0.9387 | Val loss: 0.1954 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:57.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1961 | Train score: 0.9448 | Val loss: 0.1945 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 16:07:59.026\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1914 | Train score: 0.9448 | Val loss: 0.1954 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:00.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1976 | Train score: 0.9387 | Val loss: 0.1955 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:01.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1965 | Train score: 0.9387 | Val loss: 0.1948 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:08:03.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2182 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:04.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2107 | Train score: 0.9387 | Val loss: 0.2207 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:06.357\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2236 | Train score: 0.9387 | Val loss: 0.2178 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:07.794\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2223 | Train score: 0.9387 | Val loss: 0.2165 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:09.217\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2139 | Train score: 0.9387 | Val loss: 0.2155 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:10.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2074 | Train score: 0.9387 | Val loss: 0.2151 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:12.000\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2297 | Train score: 0.9387 | Val loss: 0.2140 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:13.384\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1924 | Train score: 0.9387 | Val loss: 0.2153 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:14.794\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2258 | Train score: 0.9387 | Val loss: 0.2151 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:16.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1796 | Train score: 0.9387 | Val loss: 0.2170 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:17.624\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1815 | Train score: 0.9387 | Val loss: 0.2202 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:08:19.229\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2087 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:20.659\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2189 | Train score: 0.9325 | Val loss: 0.2089 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:22.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1982 | Train score: 0.9387 | Val loss: 0.2183 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:23.575\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2476 | Train score: 0.9325 | Val loss: 0.2194 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:24.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1727 | Train score: 0.9325 | Val loss: 0.2202 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:26.433\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2093 | Train score: 0.9387 | Val loss: 0.2196 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:28.007\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1907 | Train score: 0.9387 | Val loss: 0.2187 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:29.353\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1852 | Train score: 0.9264 | Val loss: 0.2181 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:30.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2187 | Train score: 0.9325 | Val loss: 0.2180 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:32.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2058 | Train score: 0.9264 | Val loss: 0.2183 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:33.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1911 | Train score: 0.9387 | Val loss: 0.2203 | Val score: 0.9310\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:08:35.234\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2036 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:36.585\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2103 | Train score: 0.9387 | Val loss: 0.2080 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:37.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1799 | Train score: 0.9387 | Val loss: 0.2182 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:39.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1558 | Train score: 0.9387 | Val loss: 0.2350 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:40.790\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1790 | Train score: 0.9448 | Val loss: 0.2555 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:42.175\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1983 | Train score: 0.9264 | Val loss: 0.2598 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:43.596\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1781 | Train score: 0.9448 | Val loss: 0.2516 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:44.978\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2545 | Train score: 0.9325 | Val loss: 0.2383 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:46.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2985 | Train score: 0.9264 | Val loss: 0.2254 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:47.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1703 | Train score: 0.9509 | Val loss: 0.2167 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:08:49.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1999 | Train score: 0.9448 | Val loss: 0.2094 | Val score: 0.9360\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.929         0.012           0.184          0.209        0.057       0.049         0.520        0.022    0.081   0.073         0.015        0.001\n",
      "MedPFNClassifier                0.946         0.012           0.479          0.318        0.271       0.212         0.630        0.105    0.335   0.247         1.582        0.013\n",
      "RandomForestClassifier          0.943         0.003           0.429          0.495        0.043       0.049         0.521        0.025    0.078   0.090         0.269        0.015\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.005        0.000\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.123        0.024\n",
      "TabForestPFNClassifier          0.945         0.003           0.619          0.415        0.100       0.076         0.549        0.037    0.166   0.117        15.511        0.379\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:09:21.155\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2188 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:09:22.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2291 | Train score: 0.9387 | Val loss: 0.2028 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:09:23.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2269 | Train score: 0.9387 | Val loss: 0.1915 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:09:25.282\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2119 | Train score: 0.9387 | Val loss: 0.1965 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:09:26.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2176 | Train score: 0.9387 | Val loss: 0.2044 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:09:28.012\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2009 | Train score: 0.9448 | Val loss: 0.2061 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:09:29.391\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2252 | Train score: 0.9387 | Val loss: 0.2053 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:09:30.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2457 | Train score: 0.9202 | Val loss: 0.2044 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:09:32.297\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2052 | Train score: 0.9387 | Val loss: 0.2050 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:09:33.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2074 | Train score: 0.9387 | Val loss: 0.2057 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:09:35.404\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2137 | Train score: 0.9387 | Val loss: 0.2062 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:09:37.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2054 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:09:38.475\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2171 | Train score: 0.9387 | Val loss: 0.1942 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:09:40.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2119 | Train score: 0.9448 | Val loss: 0.1966 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:09:41.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2148 | Train score: 0.9448 | Val loss: 0.1959 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:09:42.972\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1808 | Train score: 0.9387 | Val loss: 0.1966 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:09:44.377\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1735 | Train score: 0.9325 | Val loss: 0.1974 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:09:45.791\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1921 | Train score: 0.9448 | Val loss: 0.1978 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:09:47.251\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2068 | Train score: 0.9387 | Val loss: 0.1960 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:09:48.721\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1914 | Train score: 0.9264 | Val loss: 0.1971 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:09:50.171\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1825 | Train score: 0.9448 | Val loss: 0.1973 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:09:51.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2026 | Train score: 0.9202 | Val loss: 0.1970 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:09:53.415\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2132 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:09:54.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2207 | Train score: 0.9387 | Val loss: 0.1986 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:09:56.227\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1918 | Train score: 0.9387 | Val loss: 0.2095 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:09:57.646\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1752 | Train score: 0.9387 | Val loss: 0.2137 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:09:59.086\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2002 | Train score: 0.9387 | Val loss: 0.2072 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:00.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1937 | Train score: 0.9325 | Val loss: 0.2000 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:01.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2006 | Train score: 0.9325 | Val loss: 0.1953 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:03.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2040 | Train score: 0.9387 | Val loss: 0.1917 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:04.758\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1388 | Train score: 0.9448 | Val loss: 0.1903 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:06.226\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2418 | Train score: 0.9202 | Val loss: 0.1909 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:07.694\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2056 | Train score: 0.9325 | Val loss: 0.1917 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:10:09.315\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2227 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:10.794\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2302 | Train score: 0.9387 | Val loss: 0.2202 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:12.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2245 | Train score: 0.9387 | Val loss: 0.2187 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:13.717\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2079 | Train score: 0.9387 | Val loss: 0.2196 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:15.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2500 | Train score: 0.9387 | Val loss: 0.2175 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:16.586\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2061 | Train score: 0.9387 | Val loss: 0.2170 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:18.229\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2121 | Train score: 0.9387 | Val loss: 0.2171 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:19.693\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1763 | Train score: 0.9387 | Val loss: 0.2188 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:21.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2110 | Train score: 0.9387 | Val loss: 0.2200 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:22.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1942 | Train score: 0.9509 | Val loss: 0.2213 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:24.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1906 | Train score: 0.9325 | Val loss: 0.2228 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:10:25.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2083 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:27.212\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2157 | Train score: 0.9387 | Val loss: 0.2027 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:28.659\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1535 | Train score: 0.9448 | Val loss: 0.2112 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:30.093\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2713 | Train score: 0.9141 | Val loss: 0.2046 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:31.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2408 | Train score: 0.9264 | Val loss: 0.2021 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:32.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2125 | Train score: 0.9387 | Val loss: 0.2046 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:34.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2160 | Train score: 0.9325 | Val loss: 0.2063 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:35.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1796 | Train score: 0.9448 | Val loss: 0.2076 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:37.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2187 | Train score: 0.9448 | Val loss: 0.2087 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:38.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1744 | Train score: 0.9448 | Val loss: 0.2100 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:40.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1971 | Train score: 0.9387 | Val loss: 0.2111 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:10:41.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2082 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:43.146\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2232 | Train score: 0.9387 | Val loss: 0.1859 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:44.559\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2086 | Train score: 0.9387 | Val loss: 0.1848 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:45.973\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2207 | Train score: 0.9264 | Val loss: 0.1865 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:47.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1989 | Train score: 0.9387 | Val loss: 0.1879 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:48.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2176 | Train score: 0.9387 | Val loss: 0.1894 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:50.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2171 | Train score: 0.9325 | Val loss: 0.1906 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:51.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2066 | Train score: 0.9387 | Val loss: 0.1918 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:53.136\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2340 | Train score: 0.9325 | Val loss: 0.1938 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:54.561\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1876 | Train score: 0.9387 | Val loss: 0.1939 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:55.990\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2041 | Train score: 0.9387 | Val loss: 0.1928 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:10:57.604\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2205 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:10:58.951\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2183 | Train score: 0.9387 | Val loss: 0.2252 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:11:00.526\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2366 | Train score: 0.9387 | Val loss: 0.2265 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:11:01.988\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1911 | Train score: 0.9387 | Val loss: 0.2282 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:11:03.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2330 | Train score: 0.9325 | Val loss: 0.2278 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:11:04.737\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2331 | Train score: 0.9387 | Val loss: 0.2256 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:11:06.110\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2112 | Train score: 0.9387 | Val loss: 0.2241 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:11:07.676\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1921 | Train score: 0.9387 | Val loss: 0.2232 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:11:09.034\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2049 | Train score: 0.9387 | Val loss: 0.2225 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:11:10.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2130 | Train score: 0.9387 | Val loss: 0.2222 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:11:11.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2317 | Train score: 0.9325 | Val loss: 0.2213 | Val score: 0.9409\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.927         0.009           0.224          0.211        0.114       0.099         0.546        0.049    0.149   0.130         0.014        0.001\n",
      "MedPFNClassifier                0.938         0.015           0.400          0.316        0.271       0.212         0.626        0.106    0.314   0.238         1.583        0.012\n",
      "RandomForestClassifier          0.943         0.003           0.286          0.452        0.029       0.045         0.514        0.023    0.052   0.082         0.260        0.015\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.005        0.001\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.133        0.014\n",
      "TabForestPFNClassifier          0.942         0.006           0.286          0.452        0.043       0.073         0.521        0.037    0.074   0.123        15.770        0.247\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:11:43.851\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2169 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:11:45.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2210 | Train score: 0.9387 | Val loss: 0.2159 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:11:46.525\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2083 | Train score: 0.9387 | Val loss: 0.2199 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:11:47.866\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2162 | Train score: 0.9387 | Val loss: 0.2224 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:11:49.225\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2454 | Train score: 0.9387 | Val loss: 0.2245 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:11:50.572\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2344 | Train score: 0.9387 | Val loss: 0.2231 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:11:51.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2174 | Train score: 0.9387 | Val loss: 0.2213 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:11:53.457\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2057 | Train score: 0.9387 | Val loss: 0.2203 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:11:55.086\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2065 | Train score: 0.9387 | Val loss: 0.2188 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:11:56.465\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2032 | Train score: 0.9387 | Val loss: 0.2178 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:11:57.946\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1924 | Train score: 0.9387 | Val loss: 0.2174 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:11:59.566\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2214 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:00.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2312 | Train score: 0.9387 | Val loss: 0.2241 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:02.368\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2270 | Train score: 0.9387 | Val loss: 0.2237 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:03.780\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2267 | Train score: 0.9387 | Val loss: 0.2215 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:05.162\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2218 | Train score: 0.9387 | Val loss: 0.2179 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:06.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1979 | Train score: 0.9387 | Val loss: 0.2202 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:07.950\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2123 | Train score: 0.9387 | Val loss: 0.2229 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:09.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2052 | Train score: 0.9387 | Val loss: 0.2243 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:10.746\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1647 | Train score: 0.9325 | Val loss: 0.2274 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:12.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1952 | Train score: 0.9387 | Val loss: 0.2307 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:13.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2128 | Train score: 0.9264 | Val loss: 0.2341 | Val score: 0.9310\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:12:15.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2247 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:16.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2201 | Train score: 0.9387 | Val loss: 0.2539 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:17.865\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1567 | Train score: 0.9448 | Val loss: 0.2747 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:19.283\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1668 | Train score: 0.9571 | Val loss: 0.2925 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:20.671\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2584 | Train score: 0.9202 | Val loss: 0.2956 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:22.007\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2139 | Train score: 0.9325 | Val loss: 0.2943 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:23.374\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2250 | Train score: 0.9325 | Val loss: 0.2906 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:24.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1670 | Train score: 0.9509 | Val loss: 0.2870 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:26.094\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2259 | Train score: 0.9448 | Val loss: 0.2832 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:27.553\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1766 | Train score: 0.9448 | Val loss: 0.2796 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:29.042\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1850 | Train score: 0.9387 | Val loss: 0.2776 | Val score: 0.9212\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:12:30.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2218 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:32.281\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2277 | Train score: 0.9387 | Val loss: 0.2004 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:33.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2138 | Train score: 0.9387 | Val loss: 0.2018 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:35.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2201 | Train score: 0.9387 | Val loss: 0.1997 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:37.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2115 | Train score: 0.9325 | Val loss: 0.1986 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:38.882\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1769 | Train score: 0.9448 | Val loss: 0.1990 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:40.326\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2202 | Train score: 0.9448 | Val loss: 0.1998 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:41.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2020 | Train score: 0.9387 | Val loss: 0.1991 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:43.430\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2093 | Train score: 0.9325 | Val loss: 0.1985 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:44.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1881 | Train score: 0.9448 | Val loss: 0.2001 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:46.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1671 | Train score: 0.9448 | Val loss: 0.2053 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:12:47.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2144 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:49.486\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2272 | Train score: 0.9387 | Val loss: 0.1796 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:51.087\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2277 | Train score: 0.9325 | Val loss: 0.1774 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:52.649\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2017 | Train score: 0.9387 | Val loss: 0.1682 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:54.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2227 | Train score: 0.9387 | Val loss: 0.1721 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:55.737\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1924 | Train score: 0.9387 | Val loss: 0.1711 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:57.304\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2176 | Train score: 0.9387 | Val loss: 0.1710 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:12:58.717\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2078 | Train score: 0.9387 | Val loss: 0.1716 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:13:00.263\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2016 | Train score: 0.9387 | Val loss: 0.1728 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:13:01.652\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1929 | Train score: 0.9325 | Val loss: 0.1754 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:13:03.046\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2258 | Train score: 0.9387 | Val loss: 0.1776 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:13:04.673\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2140 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:13:06.259\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2230 | Train score: 0.9387 | Val loss: 0.2063 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:13:07.859\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2264 | Train score: 0.9325 | Val loss: 0.2102 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:13:09.291\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2178 | Train score: 0.9387 | Val loss: 0.2125 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:13:11.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2151 | Train score: 0.9325 | Val loss: 0.2132 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:13:12.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2094 | Train score: 0.9387 | Val loss: 0.2148 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:13:13.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1915 | Train score: 0.9387 | Val loss: 0.2170 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:13:15.448\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2227 | Train score: 0.9387 | Val loss: 0.2132 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:13:17.042\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1891 | Train score: 0.9387 | Val loss: 0.2145 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:13:18.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2084 | Train score: 0.9387 | Val loss: 0.2222 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:13:20.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1927 | Train score: 0.9387 | Val loss: 0.2237 | Val score: 0.9310\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:13:21.823\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2173 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:13:23.177\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2197 | Train score: 0.9387 | Val loss: 0.2192 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:13:24.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2025 | Train score: 0.9448 | Val loss: 0.2238 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:13:26.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1842 | Train score: 0.9509 | Val loss: 0.2279 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:13:27.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2129 | Train score: 0.9264 | Val loss: 0.2295 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:13:28.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1854 | Train score: 0.9325 | Val loss: 0.2310 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:13:30.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2212 | Train score: 0.9387 | Val loss: 0.2297 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:13:31.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2028 | Train score: 0.9325 | Val loss: 0.2288 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:13:33.100\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2130 | Train score: 0.9325 | Val loss: 0.2265 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:13:34.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2079 | Train score: 0.9387 | Val loss: 0.2259 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:13:35.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1651 | Train score: 0.9387 | Val loss: 0.2268 | Val score: 0.9310\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.934         0.005           0.238          0.216        0.057       0.049         0.523        0.026    0.092   0.079         0.015        0.002\n",
      "MedPFNClassifier                0.943         0.016           0.457          0.329        0.243       0.199         0.615        0.102    0.315   0.248         1.585        0.009\n",
      "RandomForestClassifier          0.940         0.002           0.000          0.000        0.000       0.000         0.500        0.001    0.000   0.000         0.250        0.010\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.005        0.000\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.141        0.030\n",
      "TabForestPFNClassifier          0.944         0.006           0.429          0.495        0.071       0.088         0.535        0.044    0.121   0.148        15.959        0.726\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:14:07.821\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2179 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:09.147\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2233 | Train score: 0.9387 | Val loss: 0.2293 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:10.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2464 | Train score: 0.9387 | Val loss: 0.2231 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:11.912\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2217 | Train score: 0.9387 | Val loss: 0.2196 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:13.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2074 | Train score: 0.9387 | Val loss: 0.2186 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:14.614\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2058 | Train score: 0.9387 | Val loss: 0.2188 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:15.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1897 | Train score: 0.9387 | Val loss: 0.2216 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:17.340\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2227 | Train score: 0.9387 | Val loss: 0.2244 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:18.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2077 | Train score: 0.9387 | Val loss: 0.2265 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:20.105\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2144 | Train score: 0.9387 | Val loss: 0.2273 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:21.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1861 | Train score: 0.9387 | Val loss: 0.2282 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:14:23.106\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2204 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:24.497\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2298 | Train score: 0.9387 | Val loss: 0.2136 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:25.891\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1946 | Train score: 0.9387 | Val loss: 0.2308 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:27.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2106 | Train score: 0.9387 | Val loss: 0.2365 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:28.705\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1939 | Train score: 0.9325 | Val loss: 0.2344 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:30.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2355 | Train score: 0.9080 | Val loss: 0.2288 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:31.496\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1954 | Train score: 0.9325 | Val loss: 0.2254 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:32.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1903 | Train score: 0.9387 | Val loss: 0.2226 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:34.290\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2062 | Train score: 0.9202 | Val loss: 0.2230 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:35.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2077 | Train score: 0.9387 | Val loss: 0.2240 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:37.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1992 | Train score: 0.9387 | Val loss: 0.2241 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:14:38.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2054 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:40.171\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2270 | Train score: 0.9387 | Val loss: 0.2222 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:41.701\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2296 | Train score: 0.9387 | Val loss: 0.2202 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:43.229\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2297 | Train score: 0.9387 | Val loss: 0.2135 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:44.755\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2195 | Train score: 0.9387 | Val loss: 0.1972 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:46.283\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2068 | Train score: 0.9387 | Val loss: 0.1844 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:47.851\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1783 | Train score: 0.9387 | Val loss: 0.1801 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:49.391\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2083 | Train score: 0.9448 | Val loss: 0.1797 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:50.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2334 | Train score: 0.9387 | Val loss: 0.1792 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:52.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2458 | Train score: 0.9325 | Val loss: 0.1807 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:54.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2095 | Train score: 0.9387 | Val loss: 0.1831 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:14:55.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2233 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:56.995\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2274 | Train score: 0.9387 | Val loss: 0.2126 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:58.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2164 | Train score: 0.9387 | Val loss: 0.2146 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:14:59.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2418 | Train score: 0.9387 | Val loss: 0.2152 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:01.073\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2110 | Train score: 0.9387 | Val loss: 0.2170 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:02.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2185 | Train score: 0.9387 | Val loss: 0.2183 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:03.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2109 | Train score: 0.9387 | Val loss: 0.2208 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:05.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2173 | Train score: 0.9448 | Val loss: 0.2219 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:06.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2050 | Train score: 0.9387 | Val loss: 0.2231 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:07.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1880 | Train score: 0.9387 | Val loss: 0.2263 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:09.417\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2178 | Train score: 0.9387 | Val loss: 0.2283 | Val score: 0.9310\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:15:10.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2140 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:12.611\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2253 | Train score: 0.9387 | Val loss: 0.1923 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:14.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2051 | Train score: 0.9448 | Val loss: 0.1933 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:15.660\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1908 | Train score: 0.9325 | Val loss: 0.1933 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:17.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2385 | Train score: 0.9264 | Val loss: 0.1893 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:18.680\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1649 | Train score: 0.9387 | Val loss: 0.1890 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:20.307\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1784 | Train score: 0.9387 | Val loss: 0.1901 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:21.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2031 | Train score: 0.9387 | Val loss: 0.1899 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:23.320\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1773 | Train score: 0.9387 | Val loss: 0.1916 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:24.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1916 | Train score: 0.9387 | Val loss: 0.1919 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:26.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2082 | Train score: 0.9387 | Val loss: 0.1943 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:15:27.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2213 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:29.362\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2266 | Train score: 0.9387 | Val loss: 0.2055 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:30.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2152 | Train score: 0.9387 | Val loss: 0.2052 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:32.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2106 | Train score: 0.9387 | Val loss: 0.2048 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:33.780\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2026 | Train score: 0.9387 | Val loss: 0.2047 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:35.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2156 | Train score: 0.9325 | Val loss: 0.2043 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:36.642\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2045 | Train score: 0.9387 | Val loss: 0.2038 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:38.018\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2193 | Train score: 0.9387 | Val loss: 0.2029 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:39.470\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2148 | Train score: 0.9387 | Val loss: 0.2024 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:40.891\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2212 | Train score: 0.9387 | Val loss: 0.2021 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:42.312\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2082 | Train score: 0.9387 | Val loss: 0.2022 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:15:43.959\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2244 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:45.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2144 | Train score: 0.9387 | Val loss: 0.2415 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:46.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2087 | Train score: 0.9387 | Val loss: 0.2484 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:48.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1998 | Train score: 0.9387 | Val loss: 0.2487 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:49.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2636 | Train score: 0.9325 | Val loss: 0.2424 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:51.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2143 | Train score: 0.9387 | Val loss: 0.2384 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:52.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1893 | Train score: 0.9387 | Val loss: 0.2374 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:53.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2114 | Train score: 0.9387 | Val loss: 0.2355 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:55.249\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1888 | Train score: 0.9387 | Val loss: 0.2350 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:56.679\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2015 | Train score: 0.9387 | Val loss: 0.2356 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:15:58.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1996 | Train score: 0.9448 | Val loss: 0.2361 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.926         0.010           0.162          0.188        0.071       0.088         0.526        0.045    0.098   0.119         0.013        0.001\n",
      "MedPFNClassifier                0.944         0.014           0.500          0.378        0.200       0.151         0.596        0.078    0.286   0.216         1.578        0.015\n",
      "RandomForestClassifier          0.942         0.002           0.143          0.350        0.014       0.035         0.507        0.017    0.026   0.064         0.244        0.008\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.004        0.000\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.140        0.028\n",
      "TabForestPFNClassifier          0.944         0.004           0.429          0.495        0.057       0.073         0.529        0.036    0.100   0.124        15.704        0.643\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:16:29.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2228 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:16:31.387\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2275 | Train score: 0.9387 | Val loss: 0.2240 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:16:32.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2096 | Train score: 0.9387 | Val loss: 0.2285 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:16:34.207\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2287 | Train score: 0.9387 | Val loss: 0.2246 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:16:35.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2230 | Train score: 0.9387 | Val loss: 0.2216 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:16:37.208\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2078 | Train score: 0.9387 | Val loss: 0.2219 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:16:38.613\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2137 | Train score: 0.9387 | Val loss: 0.2222 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:16:40.004\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1984 | Train score: 0.9387 | Val loss: 0.2237 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:16:41.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2110 | Train score: 0.9325 | Val loss: 0.2244 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:16:42.854\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2429 | Train score: 0.9387 | Val loss: 0.2247 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:16:44.245\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2081 | Train score: 0.9387 | Val loss: 0.2251 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:16:45.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2176 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:16:47.324\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2243 | Train score: 0.9387 | Val loss: 0.1999 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:16:48.700\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2226 | Train score: 0.9387 | Val loss: 0.1987 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:16:50.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2201 | Train score: 0.9325 | Val loss: 0.1998 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:16:51.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1981 | Train score: 0.9387 | Val loss: 0.2002 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:16:52.891\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2200 | Train score: 0.9387 | Val loss: 0.2016 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:16:54.261\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2170 | Train score: 0.9387 | Val loss: 0.2032 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:16:55.628\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2163 | Train score: 0.9387 | Val loss: 0.2054 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:16:56.988\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1964 | Train score: 0.9387 | Val loss: 0.2049 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:16:58.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2115 | Train score: 0.9387 | Val loss: 0.2030 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:16:59.942\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1991 | Train score: 0.9387 | Val loss: 0.2019 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:17:01.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2207 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:02.908\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2290 | Train score: 0.9387 | Val loss: 0.2043 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:04.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2166 | Train score: 0.9387 | Val loss: 0.2090 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:05.628\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1963 | Train score: 0.9387 | Val loss: 0.2213 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:06.968\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2141 | Train score: 0.9387 | Val loss: 0.2191 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:08.344\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2232 | Train score: 0.9325 | Val loss: 0.2130 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:09.743\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2552 | Train score: 0.9202 | Val loss: 0.2082 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:11.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1792 | Train score: 0.9387 | Val loss: 0.2059 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:12.441\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2300 | Train score: 0.9387 | Val loss: 0.2033 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:13.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1965 | Train score: 0.9387 | Val loss: 0.2015 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:15.174\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2313 | Train score: 0.9387 | Val loss: 0.2004 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:17:16.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2244 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:18.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2302 | Train score: 0.9387 | Val loss: 0.2143 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:19.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2120 | Train score: 0.9387 | Val loss: 0.2081 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:21.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2328 | Train score: 0.9387 | Val loss: 0.2094 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:22.475\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2190 | Train score: 0.9387 | Val loss: 0.2092 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:23.875\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2141 | Train score: 0.9387 | Val loss: 0.2091 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:25.279\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2151 | Train score: 0.9387 | Val loss: 0.2083 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:26.689\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2226 | Train score: 0.9387 | Val loss: 0.2071 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:28.083\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2176 | Train score: 0.9387 | Val loss: 0.2068 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:29.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1923 | Train score: 0.9387 | Val loss: 0.2072 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:31.040\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2076 | Train score: 0.9387 | Val loss: 0.2083 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:17:32.674\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2227 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:34.042\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2295 | Train score: 0.9387 | Val loss: 0.2215 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:35.469\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2190 | Train score: 0.9387 | Val loss: 0.2260 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:36.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2283 | Train score: 0.9387 | Val loss: 0.2279 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:38.277\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2055 | Train score: 0.9387 | Val loss: 0.2282 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:39.714\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2028 | Train score: 0.9387 | Val loss: 0.2317 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:41.102\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2560 | Train score: 0.9387 | Val loss: 0.2295 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:42.530\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2034 | Train score: 0.9387 | Val loss: 0.2290 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:43.945\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2096 | Train score: 0.9325 | Val loss: 0.2289 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:45.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1984 | Train score: 0.9387 | Val loss: 0.2296 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:46.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1848 | Train score: 0.9325 | Val loss: 0.2314 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:17:48.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2207 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:49.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2262 | Train score: 0.9387 | Val loss: 0.2003 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:51.153\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2121 | Train score: 0.9387 | Val loss: 0.2010 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:52.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2223 | Train score: 0.9325 | Val loss: 0.2077 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:53.881\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2376 | Train score: 0.9325 | Val loss: 0.2103 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:55.225\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1967 | Train score: 0.9387 | Val loss: 0.2116 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:56.572\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2275 | Train score: 0.9325 | Val loss: 0.2125 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:57.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1954 | Train score: 0.9448 | Val loss: 0.2143 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:17:59.298\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2024 | Train score: 0.9387 | Val loss: 0.2142 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:18:00.664\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2141 | Train score: 0.9387 | Val loss: 0.2141 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:18:02.019\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2231 | Train score: 0.9387 | Val loss: 0.2109 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:18:03.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2195 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:18:05.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2253 | Train score: 0.9387 | Val loss: 0.2250 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:18:06.402\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2272 | Train score: 0.9387 | Val loss: 0.2322 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:18:07.890\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2041 | Train score: 0.9387 | Val loss: 0.2378 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:18:09.281\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2211 | Train score: 0.9387 | Val loss: 0.2361 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:18:10.780\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2370 | Train score: 0.9448 | Val loss: 0.2283 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:18:12.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2128 | Train score: 0.9387 | Val loss: 0.2234 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:18:13.523\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2027 | Train score: 0.9387 | Val loss: 0.2233 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:18:14.904\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2000 | Train score: 0.9387 | Val loss: 0.2267 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:18:16.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2251 | Train score: 0.9387 | Val loss: 0.2274 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:18:17.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1901 | Train score: 0.9387 | Val loss: 0.2334 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.928         0.008           0.224          0.211        0.086       0.083         0.533        0.040    0.115   0.103         0.013        0.002\n",
      "MedPFNClassifier                0.933         0.005           0.286          0.193        0.100       0.076         0.543        0.036    0.141   0.096         1.587        0.013\n",
      "RandomForestClassifier          0.942         0.002           0.143          0.350        0.014       0.035         0.507        0.017    0.026   0.064         0.233        0.011\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.004        0.000\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.143        0.021\n",
      "TabForestPFNClassifier          0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000        15.330        0.289\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:18:49.445\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2222 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:18:50.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2301 | Train score: 0.9387 | Val loss: 0.2039 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:18:52.345\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2255 | Train score: 0.9387 | Val loss: 0.1980 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:18:53.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2367 | Train score: 0.9325 | Val loss: 0.1978 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:18:55.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2054 | Train score: 0.9387 | Val loss: 0.1984 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:18:56.799\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1926 | Train score: 0.9387 | Val loss: 0.1989 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:18:58.336\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1765 | Train score: 0.9387 | Val loss: 0.2053 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:18:59.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2331 | Train score: 0.9325 | Val loss: 0.2095 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:01.349\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1998 | Train score: 0.9325 | Val loss: 0.2116 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:02.871\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1864 | Train score: 0.9448 | Val loss: 0.2122 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:04.367\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2092 | Train score: 0.9325 | Val loss: 0.2119 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:19:05.994\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2213 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:07.555\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2291 | Train score: 0.9387 | Val loss: 0.1907 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:09.154\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2435 | Train score: 0.9387 | Val loss: 0.2037 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:10.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2231 | Train score: 0.9387 | Val loss: 0.2010 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:12.174\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2404 | Train score: 0.9387 | Val loss: 0.2028 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:13.821\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2229 | Train score: 0.9387 | Val loss: 0.2032 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:15.447\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2339 | Train score: 0.9387 | Val loss: 0.2057 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:17.110\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2148 | Train score: 0.9387 | Val loss: 0.2065 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:18.758\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2135 | Train score: 0.9387 | Val loss: 0.2052 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:20.347\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2207 | Train score: 0.9387 | Val loss: 0.2035 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:21.947\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2298 | Train score: 0.9387 | Val loss: 0.2020 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:19:23.567\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2226 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:24.918\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2273 | Train score: 0.9387 | Val loss: 0.2077 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:26.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2164 | Train score: 0.9387 | Val loss: 0.2106 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:27.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2431 | Train score: 0.9387 | Val loss: 0.2067 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:29.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2470 | Train score: 0.9325 | Val loss: 0.2040 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:30.471\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2126 | Train score: 0.9387 | Val loss: 0.2059 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:31.847\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2234 | Train score: 0.9387 | Val loss: 0.2089 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:33.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2176 | Train score: 0.9387 | Val loss: 0.2078 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:34.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2189 | Train score: 0.9387 | Val loss: 0.2061 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:36.225\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2243 | Train score: 0.9387 | Val loss: 0.2056 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:37.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2147 | Train score: 0.9387 | Val loss: 0.2053 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:19:39.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2248 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:40.957\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2308 | Train score: 0.9387 | Val loss: 0.2051 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:42.701\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2213 | Train score: 0.9387 | Val loss: 0.1821 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:44.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2683 | Train score: 0.9325 | Val loss: 0.1869 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:46.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2124 | Train score: 0.9448 | Val loss: 0.1898 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:47.743\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2100 | Train score: 0.9387 | Val loss: 0.1896 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:49.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2304 | Train score: 0.9387 | Val loss: 0.1902 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:51.030\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1957 | Train score: 0.9387 | Val loss: 0.1876 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:52.466\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2478 | Train score: 0.9387 | Val loss: 0.1884 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:54.188\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2131 | Train score: 0.9387 | Val loss: 0.1887 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:55.737\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2057 | Train score: 0.9387 | Val loss: 0.1874 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:19:57.357\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2273 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:19:58.912\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2167 | Train score: 0.9387 | Val loss: 0.2502 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:00.314\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2097 | Train score: 0.9448 | Val loss: 0.2561 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:01.756\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2170 | Train score: 0.9387 | Val loss: 0.2658 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:03.187\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2311 | Train score: 0.9325 | Val loss: 0.2660 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:04.586\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2312 | Train score: 0.9387 | Val loss: 0.2572 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:05.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2126 | Train score: 0.9448 | Val loss: 0.2461 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:07.529\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1964 | Train score: 0.9387 | Val loss: 0.2423 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:08.902\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2044 | Train score: 0.9387 | Val loss: 0.2426 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:10.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1894 | Train score: 0.9387 | Val loss: 0.2450 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:11.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1861 | Train score: 0.9387 | Val loss: 0.2510 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:20:13.371\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2174 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:14.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2307 | Train score: 0.9387 | Val loss: 0.1992 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:16.289\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2180 | Train score: 0.9387 | Val loss: 0.2013 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:17.716\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2324 | Train score: 0.9387 | Val loss: 0.2073 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:19.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2207 | Train score: 0.9387 | Val loss: 0.2032 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:20.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2108 | Train score: 0.9387 | Val loss: 0.1991 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:22.045\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1914 | Train score: 0.9387 | Val loss: 0.1975 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:23.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2102 | Train score: 0.9387 | Val loss: 0.1986 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:24.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2175 | Train score: 0.9387 | Val loss: 0.1995 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:26.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1889 | Train score: 0.9387 | Val loss: 0.2002 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:27.610\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2348 | Train score: 0.9448 | Val loss: 0.2003 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:20:29.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2285 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:30.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2186 | Train score: 0.9387 | Val loss: 0.2510 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:32.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1843 | Train score: 0.9387 | Val loss: 0.2735 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:33.607\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2057 | Train score: 0.9448 | Val loss: 0.2775 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:35.080\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2416 | Train score: 0.9387 | Val loss: 0.2679 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:36.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.3339 | Train score: 0.9202 | Val loss: 0.2638 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:38.146\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2305 | Train score: 0.9325 | Val loss: 0.2571 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:39.621\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1996 | Train score: 0.9509 | Val loss: 0.2472 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:41.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2462 | Train score: 0.9387 | Val loss: 0.2344 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:42.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1711 | Train score: 0.9509 | Val loss: 0.2327 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:20:43.926\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2280 | Train score: 0.9387 | Val loss: 0.2317 | Val score: 0.9310\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.932         0.014           0.512          0.333        0.157       0.073         0.569        0.037    0.215   0.086         0.013        0.002\n",
      "MedPFNClassifier                0.939         0.007           0.452          0.385        0.100       0.076         0.546        0.033    0.147   0.098         1.582        0.007\n",
      "RandomForestClassifier          0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.230        0.008\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.004        0.000\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.144        0.015\n",
      "TabForestPFNClassifier          0.942         0.002           0.143          0.350        0.014       0.035         0.507        0.017    0.026   0.064        16.304        0.850\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:21:15.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2237 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:17.304\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2309 | Train score: 0.9387 | Val loss: 0.2077 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:18.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2274 | Train score: 0.9387 | Val loss: 0.1988 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:20.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2293 | Train score: 0.9387 | Val loss: 0.2016 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:21.548\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2247 | Train score: 0.9387 | Val loss: 0.2045 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:22.899\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2165 | Train score: 0.9387 | Val loss: 0.2034 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:24.241\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2135 | Train score: 0.9387 | Val loss: 0.2032 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:25.601\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1930 | Train score: 0.9387 | Val loss: 0.2083 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:26.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2472 | Train score: 0.9387 | Val loss: 0.2069 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:28.348\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1921 | Train score: 0.9448 | Val loss: 0.2080 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:29.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2084 | Train score: 0.9325 | Val loss: 0.2049 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:21:31.308\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2235 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:32.666\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2306 | Train score: 0.9387 | Val loss: 0.2085 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:34.008\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2120 | Train score: 0.9387 | Val loss: 0.2140 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:35.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2089 | Train score: 0.9387 | Val loss: 0.2186 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:36.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2513 | Train score: 0.9387 | Val loss: 0.2151 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:38.117\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2254 | Train score: 0.9448 | Val loss: 0.2141 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:39.466\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2039 | Train score: 0.9325 | Val loss: 0.2160 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:40.829\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1906 | Train score: 0.9448 | Val loss: 0.2191 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:42.171\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1802 | Train score: 0.9448 | Val loss: 0.2249 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:43.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2169 | Train score: 0.9387 | Val loss: 0.2260 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:44.874\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2202 | Train score: 0.9387 | Val loss: 0.2269 | Val score: 0.9507\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:21:46.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2244 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:47.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2308 | Train score: 0.9387 | Val loss: 0.2175 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:49.397\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2268 | Train score: 0.9387 | Val loss: 0.2139 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:50.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2209 | Train score: 0.9387 | Val loss: 0.2180 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:52.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2428 | Train score: 0.9387 | Val loss: 0.2119 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:54.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2019 | Train score: 0.9387 | Val loss: 0.2111 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:55.918\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2222 | Train score: 0.9387 | Val loss: 0.2137 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:57.473\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1929 | Train score: 0.9387 | Val loss: 0.2217 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:21:59.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2003 | Train score: 0.9387 | Val loss: 0.2311 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:00.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1992 | Train score: 0.9387 | Val loss: 0.2427 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:02.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1703 | Train score: 0.9387 | Val loss: 0.2604 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:22:03.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2233 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:05.217\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2306 | Train score: 0.9387 | Val loss: 0.2105 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:06.601\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2238 | Train score: 0.9387 | Val loss: 0.2014 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:07.955\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1897 | Train score: 0.9387 | Val loss: 0.2113 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:09.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1748 | Train score: 0.9387 | Val loss: 0.2346 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:10.978\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2033 | Train score: 0.9448 | Val loss: 0.2379 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:12.329\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1946 | Train score: 0.9387 | Val loss: 0.2402 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:13.681\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1855 | Train score: 0.9509 | Val loss: 0.2367 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:15.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1996 | Train score: 0.9325 | Val loss: 0.2352 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:16.416\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2128 | Train score: 0.9325 | Val loss: 0.2365 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:17.806\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1770 | Train score: 0.9509 | Val loss: 0.2394 | Val score: 0.9261\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:22:19.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2240 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:20.798\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2290 | Train score: 0.9387 | Val loss: 0.2143 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:22.194\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2389 | Train score: 0.9387 | Val loss: 0.2161 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:23.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2176 | Train score: 0.9387 | Val loss: 0.2147 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:25.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2178 | Train score: 0.9387 | Val loss: 0.2121 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:26.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2357 | Train score: 0.9387 | Val loss: 0.2122 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:27.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2172 | Train score: 0.9387 | Val loss: 0.2117 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:29.438\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2205 | Train score: 0.9387 | Val loss: 0.2114 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:30.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2119 | Train score: 0.9387 | Val loss: 0.2118 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:32.258\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2074 | Train score: 0.9387 | Val loss: 0.2146 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:33.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1999 | Train score: 0.9387 | Val loss: 0.2196 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:22:35.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2228 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:36.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2305 | Train score: 0.9387 | Val loss: 0.2070 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:38.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2242 | Train score: 0.9387 | Val loss: 0.2103 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:39.801\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2405 | Train score: 0.9387 | Val loss: 0.2093 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:41.406\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2139 | Train score: 0.9387 | Val loss: 0.2116 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:42.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2378 | Train score: 0.9387 | Val loss: 0.2138 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:44.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2108 | Train score: 0.9387 | Val loss: 0.2135 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:45.880\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2069 | Train score: 0.9387 | Val loss: 0.2137 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:47.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2044 | Train score: 0.9387 | Val loss: 0.2139 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:48.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2150 | Train score: 0.9387 | Val loss: 0.2133 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:50.361\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1968 | Train score: 0.9387 | Val loss: 0.2139 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:22:51.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2230 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:53.372\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2291 | Train score: 0.9387 | Val loss: 0.2303 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:54.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2204 | Train score: 0.9387 | Val loss: 0.2352 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:56.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2029 | Train score: 0.9387 | Val loss: 0.2391 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:57.645\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2740 | Train score: 0.9325 | Val loss: 0.2326 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:22:59.201\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1921 | Train score: 0.9387 | Val loss: 0.2299 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:23:00.861\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2058 | Train score: 0.9387 | Val loss: 0.2290 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:23:02.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1924 | Train score: 0.9387 | Val loss: 0.2313 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:23:03.672\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1890 | Train score: 0.9387 | Val loss: 0.2363 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:23:05.213\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2017 | Train score: 0.9325 | Val loss: 0.2415 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:23:06.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2145 | Train score: 0.9325 | Val loss: 0.2445 | Val score: 0.9360\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.937         0.006           0.314          0.223        0.071       0.045         0.531        0.022    0.114   0.073         0.013        0.002\n",
      "MedPFNClassifier                0.941         0.004           0.500          0.378        0.100       0.076         0.547        0.037    0.157   0.109         1.596        0.022\n",
      "RandomForestClassifier          0.942         0.002           0.143          0.350        0.014       0.035         0.507        0.017    0.026   0.064         0.225        0.009\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.004        0.001\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.155        0.048\n",
      "TabForestPFNClassifier          0.943         0.003           0.381          0.364        0.086       0.083         0.541        0.040    0.135   0.126        15.764        0.718\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:23:38.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2193 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:23:39.714\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2260 | Train score: 0.9387 | Val loss: 0.2016 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:23:41.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2141 | Train score: 0.9387 | Val loss: 0.1986 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:23:42.438\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1983 | Train score: 0.9387 | Val loss: 0.2064 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:23:43.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2730 | Train score: 0.9325 | Val loss: 0.2065 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:23:45.181\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2186 | Train score: 0.9387 | Val loss: 0.2078 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:23:46.545\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2130 | Train score: 0.9509 | Val loss: 0.2070 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:23:47.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2060 | Train score: 0.9387 | Val loss: 0.2069 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:23:49.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2145 | Train score: 0.9448 | Val loss: 0.2063 | Val score: 0.9212\u001b[0m\n",
      "\u001b[32m2024-10-20 16:23:50.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1924 | Train score: 0.9387 | Val loss: 0.2047 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:23:52.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2046 | Train score: 0.9325 | Val loss: 0.2040 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:23:53.608\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2231 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:23:54.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2281 | Train score: 0.9387 | Val loss: 0.2224 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:23:56.498\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1942 | Train score: 0.9387 | Val loss: 0.2348 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:23:57.867\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2243 | Train score: 0.9448 | Val loss: 0.2342 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:23:59.263\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2108 | Train score: 0.9387 | Val loss: 0.2312 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:00.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2204 | Train score: 0.9448 | Val loss: 0.2279 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:02.060\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2364 | Train score: 0.9387 | Val loss: 0.2247 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:03.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1954 | Train score: 0.9448 | Val loss: 0.2236 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:04.995\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2088 | Train score: 0.9387 | Val loss: 0.2221 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:06.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2016 | Train score: 0.9387 | Val loss: 0.2217 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:07.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1981 | Train score: 0.9387 | Val loss: 0.2229 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:24:09.333\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2237 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:10.724\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2301 | Train score: 0.9387 | Val loss: 0.2036 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:12.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2156 | Train score: 0.9387 | Val loss: 0.2036 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:13.495\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2598 | Train score: 0.9264 | Val loss: 0.2011 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:14.893\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2061 | Train score: 0.9387 | Val loss: 0.2017 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:16.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2134 | Train score: 0.9387 | Val loss: 0.2019 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:17.663\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1939 | Train score: 0.9387 | Val loss: 0.2024 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:19.036\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2062 | Train score: 0.9387 | Val loss: 0.2032 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:20.456\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2032 | Train score: 0.9264 | Val loss: 0.2048 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:21.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2167 | Train score: 0.9264 | Val loss: 0.2055 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:23.177\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2131 | Train score: 0.9325 | Val loss: 0.2068 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:24:24.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2231 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:26.114\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2298 | Train score: 0.9387 | Val loss: 0.2122 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:27.465\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2067 | Train score: 0.9387 | Val loss: 0.2082 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:28.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1956 | Train score: 0.9325 | Val loss: 0.2103 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:30.198\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2606 | Train score: 0.9325 | Val loss: 0.2086 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:31.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2184 | Train score: 0.9325 | Val loss: 0.2062 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:32.965\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2120 | Train score: 0.9325 | Val loss: 0.2055 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:34.347\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2269 | Train score: 0.9387 | Val loss: 0.2052 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:35.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1787 | Train score: 0.9387 | Val loss: 0.2055 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:37.073\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2231 | Train score: 0.9387 | Val loss: 0.2052 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:38.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2127 | Train score: 0.9387 | Val loss: 0.2051 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:24:40.073\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2229 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:41.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2285 | Train score: 0.9387 | Val loss: 0.2110 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:42.899\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2200 | Train score: 0.9387 | Val loss: 0.2077 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:44.284\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2111 | Train score: 0.9387 | Val loss: 0.2101 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:45.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2081 | Train score: 0.9387 | Val loss: 0.2125 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:47.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2556 | Train score: 0.9387 | Val loss: 0.2127 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:48.585\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2169 | Train score: 0.9387 | Val loss: 0.2139 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:50.074\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2027 | Train score: 0.9387 | Val loss: 0.2139 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:51.544\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2180 | Train score: 0.9387 | Val loss: 0.2121 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:53.172\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2040 | Train score: 0.9387 | Val loss: 0.2102 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:54.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2062 | Train score: 0.9387 | Val loss: 0.2097 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:24:56.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2237 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:57.843\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2290 | Train score: 0.9387 | Val loss: 0.2090 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:24:59.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2059 | Train score: 0.9387 | Val loss: 0.2052 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:25:00.679\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2151 | Train score: 0.9387 | Val loss: 0.2044 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:25:02.075\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2035 | Train score: 0.9448 | Val loss: 0.2057 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:25:03.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2137 | Train score: 0.9325 | Val loss: 0.2084 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:25:04.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2149 | Train score: 0.9448 | Val loss: 0.2105 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:25:06.297\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2004 | Train score: 0.9387 | Val loss: 0.2115 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:25:07.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2059 | Train score: 0.9387 | Val loss: 0.2124 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:25:09.147\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1864 | Train score: 0.9387 | Val loss: 0.2129 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:25:10.551\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1919 | Train score: 0.9387 | Val loss: 0.2135 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:25:12.164\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2182 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:25:13.532\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2261 | Train score: 0.9387 | Val loss: 0.2076 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:25:14.932\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1980 | Train score: 0.9387 | Val loss: 0.2101 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:25:16.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2185 | Train score: 0.9325 | Val loss: 0.2104 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:25:17.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2219 | Train score: 0.9387 | Val loss: 0.2101 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:25:19.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1925 | Train score: 0.9387 | Val loss: 0.2102 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:25:20.599\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2023 | Train score: 0.9387 | Val loss: 0.2095 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:25:21.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2018 | Train score: 0.9387 | Val loss: 0.2084 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:25:23.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2031 | Train score: 0.9264 | Val loss: 0.2074 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:25:24.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2160 | Train score: 0.9387 | Val loss: 0.2058 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:25:26.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2061 | Train score: 0.9387 | Val loss: 0.2050 | Val score: 0.9409\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.928         0.007           0.214          0.140        0.100       0.076         0.540        0.033    0.129   0.087         0.011        0.002\n",
      "MedPFNClassifier                0.939         0.008           0.429          0.319        0.114       0.083         0.553        0.042    0.177   0.126         1.586        0.012\n",
      "RandomForestClassifier          0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.221        0.004\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.004        0.001\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.144        0.029\n",
      "TabForestPFNClassifier          0.943         0.003           0.429          0.495        0.043       0.049         0.521        0.025    0.078   0.090        15.341        0.360\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:25:57.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2234 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:25:59.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2270 | Train score: 0.9387 | Val loss: 0.2203 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:00.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2111 | Train score: 0.9387 | Val loss: 0.2257 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:02.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2102 | Train score: 0.9387 | Val loss: 0.2293 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:03.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1926 | Train score: 0.9448 | Val loss: 0.2335 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:04.943\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2589 | Train score: 0.9387 | Val loss: 0.2294 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:06.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2090 | Train score: 0.9387 | Val loss: 0.2270 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:07.833\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1803 | Train score: 0.9387 | Val loss: 0.2272 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:09.284\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1893 | Train score: 0.9387 | Val loss: 0.2279 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:10.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1834 | Train score: 0.9387 | Val loss: 0.2315 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:12.203\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2078 | Train score: 0.9387 | Val loss: 0.2363 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:26:13.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2244 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:15.166\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2308 | Train score: 0.9387 | Val loss: 0.2231 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:16.626\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2299 | Train score: 0.9387 | Val loss: 0.2212 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:18.012\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2263 | Train score: 0.9387 | Val loss: 0.2160 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:19.391\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2242 | Train score: 0.9387 | Val loss: 0.2093 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:20.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2195 | Train score: 0.9387 | Val loss: 0.2024 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:22.166\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2274 | Train score: 0.9387 | Val loss: 0.1984 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:23.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2103 | Train score: 0.9387 | Val loss: 0.1969 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:24.896\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2208 | Train score: 0.9387 | Val loss: 0.1962 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:26.361\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2255 | Train score: 0.9387 | Val loss: 0.1955 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:27.738\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2239 | Train score: 0.9448 | Val loss: 0.1950 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:26:29.337\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2240 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:30.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2305 | Train score: 0.9387 | Val loss: 0.2084 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:32.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2190 | Train score: 0.9387 | Val loss: 0.2028 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:33.415\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2245 | Train score: 0.9387 | Val loss: 0.2030 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:34.920\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2304 | Train score: 0.9387 | Val loss: 0.2049 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:36.293\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2313 | Train score: 0.9387 | Val loss: 0.2095 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:37.673\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2173 | Train score: 0.9387 | Val loss: 0.2104 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:39.041\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2154 | Train score: 0.9387 | Val loss: 0.2103 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:40.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2091 | Train score: 0.9387 | Val loss: 0.2099 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:41.873\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2283 | Train score: 0.9387 | Val loss: 0.2109 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:43.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2140 | Train score: 0.9387 | Val loss: 0.2121 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:26:44.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2237 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:46.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2301 | Train score: 0.9387 | Val loss: 0.2137 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:47.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2208 | Train score: 0.9387 | Val loss: 0.2121 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:48.904\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2012 | Train score: 0.9387 | Val loss: 0.2218 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:50.227\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2663 | Train score: 0.9387 | Val loss: 0.2188 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:51.591\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2056 | Train score: 0.9387 | Val loss: 0.2162 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:52.974\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1987 | Train score: 0.9387 | Val loss: 0.2147 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:54.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1916 | Train score: 0.9387 | Val loss: 0.2131 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:55.719\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2168 | Train score: 0.9387 | Val loss: 0.2110 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:57.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1952 | Train score: 0.9387 | Val loss: 0.2095 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:26:58.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2227 | Train score: 0.9387 | Val loss: 0.2076 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:27:00.047\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2243 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:01.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2306 | Train score: 0.9387 | Val loss: 0.2155 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:02.743\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2250 | Train score: 0.9387 | Val loss: 0.2095 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:04.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2192 | Train score: 0.9387 | Val loss: 0.2090 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:05.629\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2180 | Train score: 0.9387 | Val loss: 0.2098 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:07.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2134 | Train score: 0.9387 | Val loss: 0.2094 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:08.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2084 | Train score: 0.9387 | Val loss: 0.2100 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:09.902\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2173 | Train score: 0.9387 | Val loss: 0.2111 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:11.278\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1932 | Train score: 0.9387 | Val loss: 0.2129 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:12.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2194 | Train score: 0.9387 | Val loss: 0.2130 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:13.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1898 | Train score: 0.9387 | Val loss: 0.2145 | Val score: 0.9310\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:27:15.602\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2245 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:17.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2305 | Train score: 0.9387 | Val loss: 0.2205 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:18.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2242 | Train score: 0.9387 | Val loss: 0.2109 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:19.994\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2276 | Train score: 0.9387 | Val loss: 0.2086 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:21.353\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2227 | Train score: 0.9387 | Val loss: 0.2078 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:22.873\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2374 | Train score: 0.9325 | Val loss: 0.2088 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:24.391\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2105 | Train score: 0.9387 | Val loss: 0.2102 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:25.901\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2101 | Train score: 0.9387 | Val loss: 0.2117 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:27.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2203 | Train score: 0.9387 | Val loss: 0.2137 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:28.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2224 | Train score: 0.9387 | Val loss: 0.2162 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:30.626\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2429 | Train score: 0.9387 | Val loss: 0.2164 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:27:32.230\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2229 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:33.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2266 | Train score: 0.9387 | Val loss: 0.2230 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:35.206\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2095 | Train score: 0.9387 | Val loss: 0.2290 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:36.630\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1916 | Train score: 0.9387 | Val loss: 0.2377 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:38.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2044 | Train score: 0.9387 | Val loss: 0.2412 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:39.473\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2011 | Train score: 0.9387 | Val loss: 0.2368 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:40.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1713 | Train score: 0.9509 | Val loss: 0.2354 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:42.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2759 | Train score: 0.9202 | Val loss: 0.2350 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:43.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2180 | Train score: 0.9448 | Val loss: 0.2380 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:45.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2096 | Train score: 0.9325 | Val loss: 0.2372 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:27:46.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2066 | Train score: 0.9448 | Val loss: 0.2410 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.932         0.009           0.393          0.398        0.071       0.045         0.529        0.025    0.113   0.073         0.012        0.001\n",
      "MedPFNClassifier                0.936         0.005           0.071          0.175        0.014       0.035         0.504        0.018    0.024   0.058         1.596        0.022\n",
      "RandomForestClassifier          0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.213        0.005\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.004        0.001\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.135        0.016\n",
      "TabForestPFNClassifier          0.942         0.002           0.143          0.350        0.014       0.035         0.507        0.017    0.026   0.064        15.499        0.438\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:28:18.405\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2232 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:28:19.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2294 | Train score: 0.9387 | Val loss: 0.2065 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:28:21.123\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2249 | Train score: 0.9387 | Val loss: 0.1954 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:28:22.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2461 | Train score: 0.9387 | Val loss: 0.1991 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:28:23.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2169 | Train score: 0.9387 | Val loss: 0.2024 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:28:25.284\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2322 | Train score: 0.9387 | Val loss: 0.2064 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:28:26.762\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2238 | Train score: 0.9387 | Val loss: 0.2095 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:28:28.198\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2136 | Train score: 0.9387 | Val loss: 0.2075 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:28:29.542\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2209 | Train score: 0.9387 | Val loss: 0.2048 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:28:30.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2214 | Train score: 0.9387 | Val loss: 0.2024 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:28:32.375\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2142 | Train score: 0.9387 | Val loss: 0.1991 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:28:33.950\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2240 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:28:35.322\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2312 | Train score: 0.9387 | Val loss: 0.2246 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:28:36.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2303 | Train score: 0.9387 | Val loss: 0.2238 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:28:38.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2300 | Train score: 0.9387 | Val loss: 0.2218 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:28:39.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2269 | Train score: 0.9387 | Val loss: 0.2166 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:28:40.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2252 | Train score: 0.9387 | Val loss: 0.2158 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:28:42.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2347 | Train score: 0.9387 | Val loss: 0.2179 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:28:43.658\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2211 | Train score: 0.9387 | Val loss: 0.2179 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:28:45.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2296 | Train score: 0.9387 | Val loss: 0.2178 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:28:46.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2240 | Train score: 0.9387 | Val loss: 0.2175 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:28:47.905\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2233 | Train score: 0.9387 | Val loss: 0.2170 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:28:49.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2238 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:28:50.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2304 | Train score: 0.9387 | Val loss: 0.2112 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:28:52.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2200 | Train score: 0.9387 | Val loss: 0.2115 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:28:53.804\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1935 | Train score: 0.9387 | Val loss: 0.2210 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:28:55.185\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2277 | Train score: 0.9448 | Val loss: 0.2186 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:28:56.573\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1851 | Train score: 0.9387 | Val loss: 0.2178 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:28:57.987\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1939 | Train score: 0.9387 | Val loss: 0.2161 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:28:59.389\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1703 | Train score: 0.9387 | Val loss: 0.2174 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:00.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2365 | Train score: 0.9387 | Val loss: 0.2160 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:02.153\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1931 | Train score: 0.9387 | Val loss: 0.2156 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:03.643\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1960 | Train score: 0.9448 | Val loss: 0.2153 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:29:05.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2227 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:06.604\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2260 | Train score: 0.9387 | Val loss: 0.2215 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:08.030\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2293 | Train score: 0.9387 | Val loss: 0.2216 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:09.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2153 | Train score: 0.9387 | Val loss: 0.2227 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:10.870\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2343 | Train score: 0.9387 | Val loss: 0.2224 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:12.254\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1950 | Train score: 0.9387 | Val loss: 0.2238 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:13.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1929 | Train score: 0.9387 | Val loss: 0.2254 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:15.027\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2016 | Train score: 0.9387 | Val loss: 0.2282 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:16.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1978 | Train score: 0.9387 | Val loss: 0.2319 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:17.801\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1837 | Train score: 0.9387 | Val loss: 0.2366 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:19.187\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2060 | Train score: 0.9448 | Val loss: 0.2384 | Val score: 0.9310\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:29:20.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2226 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:22.106\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2212 | Train score: 0.9387 | Val loss: 0.2506 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:23.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1972 | Train score: 0.9387 | Val loss: 0.2960 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:24.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1800 | Train score: 0.9387 | Val loss: 0.3224 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:26.150\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2733 | Train score: 0.8834 | Val loss: 0.2993 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:27.523\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1720 | Train score: 0.9387 | Val loss: 0.2835 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:28.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2119 | Train score: 0.9264 | Val loss: 0.2717 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:30.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.3647 | Train score: 0.9018 | Val loss: 0.2570 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:31.754\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2333 | Train score: 0.9325 | Val loss: 0.2482 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:33.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1846 | Train score: 0.9387 | Val loss: 0.2439 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:34.436\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2312 | Train score: 0.9387 | Val loss: 0.2408 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:29:36.074\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2247 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:37.626\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2306 | Train score: 0.9387 | Val loss: 0.2244 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:39.100\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2294 | Train score: 0.9387 | Val loss: 0.2230 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:40.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2301 | Train score: 0.9387 | Val loss: 0.2220 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:42.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2278 | Train score: 0.9387 | Val loss: 0.2197 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:43.858\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2210 | Train score: 0.9387 | Val loss: 0.2163 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:45.251\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2289 | Train score: 0.9387 | Val loss: 0.2164 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:46.890\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2301 | Train score: 0.9387 | Val loss: 0.2180 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:48.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2243 | Train score: 0.9387 | Val loss: 0.2175 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:49.881\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2280 | Train score: 0.9387 | Val loss: 0.2173 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:51.493\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2214 | Train score: 0.9387 | Val loss: 0.2165 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:29:53.082\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2232 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:54.598\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2306 | Train score: 0.9387 | Val loss: 0.2125 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:55.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2251 | Train score: 0.9387 | Val loss: 0.2002 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:57.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2279 | Train score: 0.9387 | Val loss: 0.2066 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:29:59.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2402 | Train score: 0.9387 | Val loss: 0.2085 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:30:00.411\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2075 | Train score: 0.9387 | Val loss: 0.2086 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:30:01.956\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2134 | Train score: 0.9387 | Val loss: 0.2075 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:30:03.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2081 | Train score: 0.9387 | Val loss: 0.2074 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:30:04.861\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2410 | Train score: 0.9387 | Val loss: 0.2087 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:30:06.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1926 | Train score: 0.9387 | Val loss: 0.2104 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:30:07.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2067 | Train score: 0.9387 | Val loss: 0.2127 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.926         0.008           0.048          0.117        0.014       0.035         0.499        0.019    0.022   0.054         0.011        0.002\n",
      "MedPFNClassifier                0.939         0.008           0.107          0.262        0.043       0.105         0.519        0.052    0.061   0.150         1.592        0.009\n",
      "RandomForestClassifier          0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.208        0.003\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.004        0.001\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.147        0.030\n",
      "TabForestPFNClassifier          0.942         0.002           0.143          0.350        0.014       0.035         0.507        0.017    0.026   0.064        15.564        0.554\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:30:39.610\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2242 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:30:41.000\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2295 | Train score: 0.9387 | Val loss: 0.2143 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:30:42.319\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2217 | Train score: 0.9387 | Val loss: 0.2093 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:30:43.648\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2040 | Train score: 0.9387 | Val loss: 0.2228 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:30:45.001\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2019 | Train score: 0.9448 | Val loss: 0.2323 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:30:46.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2398 | Train score: 0.9325 | Val loss: 0.2315 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:30:47.689\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2373 | Train score: 0.9448 | Val loss: 0.2218 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:30:49.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2162 | Train score: 0.9387 | Val loss: 0.2126 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:30:50.371\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1974 | Train score: 0.9387 | Val loss: 0.2089 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:30:51.749\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2021 | Train score: 0.9387 | Val loss: 0.2084 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:30:53.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1954 | Train score: 0.9387 | Val loss: 0.2078 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:30:54.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2245 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:30:56.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2305 | Train score: 0.9387 | Val loss: 0.2245 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:30:57.620\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2300 | Train score: 0.9387 | Val loss: 0.2243 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:30:59.226\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2292 | Train score: 0.9387 | Val loss: 0.2245 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:00.856\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2299 | Train score: 0.9387 | Val loss: 0.2245 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:02.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2282 | Train score: 0.9387 | Val loss: 0.2249 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:03.966\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2198 | Train score: 0.9387 | Val loss: 0.2273 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:05.605\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2180 | Train score: 0.9387 | Val loss: 0.2374 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:07.241\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2352 | Train score: 0.9387 | Val loss: 0.2365 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:08.847\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2122 | Train score: 0.9387 | Val loss: 0.2352 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:10.428\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1967 | Train score: 0.9387 | Val loss: 0.2406 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:31:12.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2247 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:13.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2308 | Train score: 0.9387 | Val loss: 0.2249 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:15.375\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2306 | Train score: 0.9387 | Val loss: 0.2238 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:17.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2300 | Train score: 0.9387 | Val loss: 0.2230 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:18.470\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2300 | Train score: 0.9387 | Val loss: 0.2223 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:20.193\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2278 | Train score: 0.9387 | Val loss: 0.2211 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:21.990\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2251 | Train score: 0.9387 | Val loss: 0.2200 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:23.680\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2295 | Train score: 0.9387 | Val loss: 0.2194 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:25.230\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2279 | Train score: 0.9387 | Val loss: 0.2183 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:26.801\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2241 | Train score: 0.9387 | Val loss: 0.2162 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:28.345\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2181 | Train score: 0.9387 | Val loss: 0.2134 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:31:29.966\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2251 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:31.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2310 | Train score: 0.9387 | Val loss: 0.2238 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:32.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2304 | Train score: 0.9387 | Val loss: 0.2220 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:34.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2260 | Train score: 0.9387 | Val loss: 0.2162 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:35.583\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2247 | Train score: 0.9387 | Val loss: 0.2139 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:36.990\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2130 | Train score: 0.9387 | Val loss: 0.2213 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:38.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1961 | Train score: 0.9387 | Val loss: 0.2296 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:39.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2311 | Train score: 0.9387 | Val loss: 0.2317 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:41.210\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2215 | Train score: 0.9387 | Val loss: 0.2307 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:42.608\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1979 | Train score: 0.9387 | Val loss: 0.2299 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:43.985\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1946 | Train score: 0.9387 | Val loss: 0.2336 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:31:45.614\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2247 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:47.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2305 | Train score: 0.9387 | Val loss: 0.2226 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:48.671\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2256 | Train score: 0.9387 | Val loss: 0.2179 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:50.198\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2231 | Train score: 0.9387 | Val loss: 0.2137 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:51.829\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2131 | Train score: 0.9387 | Val loss: 0.2172 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:53.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2206 | Train score: 0.9387 | Val loss: 0.2179 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:55.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2109 | Train score: 0.9387 | Val loss: 0.2190 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:56.646\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2032 | Train score: 0.9387 | Val loss: 0.2197 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:31:58.440\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2408 | Train score: 0.9387 | Val loss: 0.2176 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:32:00.010\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2216 | Train score: 0.9387 | Val loss: 0.2163 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:32:01.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2070 | Train score: 0.9387 | Val loss: 0.2159 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:32:03.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2246 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:32:04.615\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2301 | Train score: 0.9387 | Val loss: 0.2243 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:32:06.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2300 | Train score: 0.9387 | Val loss: 0.2234 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:32:07.545\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2290 | Train score: 0.9387 | Val loss: 0.2230 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:32:09.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2238 | Train score: 0.9387 | Val loss: 0.2239 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:32:10.480\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2251 | Train score: 0.9387 | Val loss: 0.2245 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:32:11.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2202 | Train score: 0.9387 | Val loss: 0.2246 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:32:13.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2445 | Train score: 0.9387 | Val loss: 0.2237 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:32:14.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2105 | Train score: 0.9387 | Val loss: 0.2236 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:32:16.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2082 | Train score: 0.9387 | Val loss: 0.2249 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:32:17.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2157 | Train score: 0.9387 | Val loss: 0.2262 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:32:19.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2239 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:32:21.001\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2286 | Train score: 0.9387 | Val loss: 0.2362 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:32:22.630\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2163 | Train score: 0.9387 | Val loss: 0.2545 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:32:24.002\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2251 | Train score: 0.9509 | Val loss: 0.2468 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:32:25.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2240 | Train score: 0.9448 | Val loss: 0.2397 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:32:26.979\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2008 | Train score: 0.9387 | Val loss: 0.2386 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:32:28.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2220 | Train score: 0.9387 | Val loss: 0.2358 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:32:30.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2106 | Train score: 0.9387 | Val loss: 0.2363 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:32:31.480\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2013 | Train score: 0.9387 | Val loss: 0.2422 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:32:32.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2143 | Train score: 0.9387 | Val loss: 0.2490 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:32:34.390\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2115 | Train score: 0.9387 | Val loss: 0.2539 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.922         0.007           0.057          0.090        0.029       0.045         0.504        0.022    0.038   0.060         0.012        0.002\n",
      "MedPFNClassifier                0.939         0.003           0.167          0.199        0.043       0.049         0.519        0.022    0.068   0.078         1.616        0.026\n",
      "RandomForestClassifier          0.942         0.002           0.143          0.350        0.014       0.035         0.507        0.017    0.026   0.064         0.205        0.005\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.003        0.001\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.155        0.016\n",
      "TabForestPFNClassifier          0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000        16.342        0.953\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:33:06.166\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2223 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:07.498\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2291 | Train score: 0.9387 | Val loss: 0.2168 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:08.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2148 | Train score: 0.9387 | Val loss: 0.2222 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:10.173\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2451 | Train score: 0.9387 | Val loss: 0.2198 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:11.558\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2019 | Train score: 0.9387 | Val loss: 0.2206 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:12.902\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2133 | Train score: 0.9387 | Val loss: 0.2217 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:14.265\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2126 | Train score: 0.9387 | Val loss: 0.2215 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:15.602\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2193 | Train score: 0.9387 | Val loss: 0.2204 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:16.984\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2127 | Train score: 0.9387 | Val loss: 0.2195 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:18.345\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2076 | Train score: 0.9387 | Val loss: 0.2193 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:19.700\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1899 | Train score: 0.9387 | Val loss: 0.2211 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:33:21.297\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2243 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:22.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2300 | Train score: 0.9387 | Val loss: 0.2232 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:24.261\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2305 | Train score: 0.9387 | Val loss: 0.2231 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:25.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2273 | Train score: 0.9387 | Val loss: 0.2228 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:27.455\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2269 | Train score: 0.9387 | Val loss: 0.2227 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:29.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2194 | Train score: 0.9387 | Val loss: 0.2256 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:30.603\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2179 | Train score: 0.9387 | Val loss: 0.2294 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:32.196\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1936 | Train score: 0.9387 | Val loss: 0.2435 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:33.657\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2330 | Train score: 0.9387 | Val loss: 0.2538 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:35.078\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1926 | Train score: 0.9264 | Val loss: 0.2557 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:36.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2149 | Train score: 0.9448 | Val loss: 0.2480 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:33:38.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2245 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:39.802\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2306 | Train score: 0.9387 | Val loss: 0.2211 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:41.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2279 | Train score: 0.9387 | Val loss: 0.2123 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:42.758\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2237 | Train score: 0.9387 | Val loss: 0.1965 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:44.337\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2478 | Train score: 0.9387 | Val loss: 0.2070 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:45.915\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2245 | Train score: 0.9387 | Val loss: 0.2121 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:47.489\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2242 | Train score: 0.9387 | Val loss: 0.2139 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:49.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2206 | Train score: 0.9387 | Val loss: 0.2140 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:50.541\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2214 | Train score: 0.9387 | Val loss: 0.2129 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:52.122\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2184 | Train score: 0.9387 | Val loss: 0.2112 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:53.507\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2192 | Train score: 0.9387 | Val loss: 0.2087 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:33:55.110\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2245 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:56.570\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2303 | Train score: 0.9387 | Val loss: 0.2221 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:57.972\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2275 | Train score: 0.9387 | Val loss: 0.2169 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:33:59.367\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2162 | Train score: 0.9387 | Val loss: 0.2134 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:00.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2267 | Train score: 0.9387 | Val loss: 0.2137 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:02.182\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2018 | Train score: 0.9387 | Val loss: 0.2175 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:03.632\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2500 | Train score: 0.9387 | Val loss: 0.2178 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:05.030\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2099 | Train score: 0.9387 | Val loss: 0.2190 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:06.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2061 | Train score: 0.9387 | Val loss: 0.2215 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:07.902\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1892 | Train score: 0.9387 | Val loss: 0.2247 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:09.347\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2021 | Train score: 0.9387 | Val loss: 0.2259 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:34:10.957\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2241 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:12.541\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2297 | Train score: 0.9387 | Val loss: 0.2235 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:14.090\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2159 | Train score: 0.9387 | Val loss: 0.2283 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:15.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2001 | Train score: 0.9387 | Val loss: 0.2337 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:17.189\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1767 | Train score: 0.9448 | Val loss: 0.2392 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:18.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1825 | Train score: 0.9448 | Val loss: 0.2443 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:20.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2439 | Train score: 0.9448 | Val loss: 0.2408 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:22.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2199 | Train score: 0.9264 | Val loss: 0.2388 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:23.563\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2080 | Train score: 0.9448 | Val loss: 0.2380 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:25.099\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1733 | Train score: 0.9448 | Val loss: 0.2367 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:26.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2172 | Train score: 0.9387 | Val loss: 0.2357 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:34:28.306\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2244 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:29.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2301 | Train score: 0.9387 | Val loss: 0.2221 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:31.274\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2304 | Train score: 0.9387 | Val loss: 0.2231 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:32.833\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2297 | Train score: 0.9387 | Val loss: 0.2226 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:34.245\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2287 | Train score: 0.9387 | Val loss: 0.2212 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:35.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2298 | Train score: 0.9387 | Val loss: 0.2197 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:37.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2264 | Train score: 0.9387 | Val loss: 0.2161 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:38.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2218 | Train score: 0.9387 | Val loss: 0.2105 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:40.134\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2304 | Train score: 0.9387 | Val loss: 0.2104 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:41.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2285 | Train score: 0.9387 | Val loss: 0.2110 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:42.938\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2094 | Train score: 0.9387 | Val loss: 0.2099 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:34:44.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2238 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:46.107\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2303 | Train score: 0.9387 | Val loss: 0.2210 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:47.466\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2315 | Train score: 0.9387 | Val loss: 0.2220 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:48.851\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2285 | Train score: 0.9387 | Val loss: 0.2214 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:50.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2266 | Train score: 0.9387 | Val loss: 0.2192 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:51.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2244 | Train score: 0.9387 | Val loss: 0.2151 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:53.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2246 | Train score: 0.9387 | Val loss: 0.2108 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:54.545\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2040 | Train score: 0.9387 | Val loss: 0.2121 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:55.942\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2018 | Train score: 0.9387 | Val loss: 0.2145 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:57.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2179 | Train score: 0.9387 | Val loss: 0.2126 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:34:58.889\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2158 | Train score: 0.9387 | Val loss: 0.2098 | Val score: 0.9360\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.937         0.009           0.524          0.338        0.114       0.064         0.551        0.030    0.171   0.077         0.013        0.003\n",
      "MedPFNClassifier                0.933         0.009           0.190          0.243        0.086       0.099         0.536        0.047    0.115   0.134         1.605        0.016\n",
      "RandomForestClassifier          0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.203        0.004\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.004        0.001\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.156        0.019\n",
      "TabForestPFNClassifier          0.940         0.002           0.000          0.000        0.000       0.000         0.500        0.001    0.000   0.000        16.026        0.729\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:35:30.563\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2242 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:35:31.975\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2312 | Train score: 0.9387 | Val loss: 0.2234 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:35:33.399\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2294 | Train score: 0.9387 | Val loss: 0.2221 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:35:34.832\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2249 | Train score: 0.9387 | Val loss: 0.2207 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:35:36.336\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2115 | Train score: 0.9387 | Val loss: 0.2236 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:35:37.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2200 | Train score: 0.9387 | Val loss: 0.2257 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:35:39.241\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2371 | Train score: 0.9387 | Val loss: 0.2203 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:35:40.715\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2048 | Train score: 0.9387 | Val loss: 0.2212 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:35:42.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2440 | Train score: 0.9387 | Val loss: 0.2218 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:35:43.404\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2096 | Train score: 0.9387 | Val loss: 0.2220 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:35:44.851\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1996 | Train score: 0.9387 | Val loss: 0.2241 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:35:46.450\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2244 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:35:47.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2304 | Train score: 0.9387 | Val loss: 0.2234 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:35:49.115\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2287 | Train score: 0.9387 | Val loss: 0.2206 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:35:50.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2253 | Train score: 0.9387 | Val loss: 0.2182 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:35:51.846\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2088 | Train score: 0.9387 | Val loss: 0.2185 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:35:53.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2514 | Train score: 0.9387 | Val loss: 0.2182 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:35:54.553\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1996 | Train score: 0.9387 | Val loss: 0.2182 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:35:55.913\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2031 | Train score: 0.9387 | Val loss: 0.2183 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:35:57.268\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2223 | Train score: 0.9387 | Val loss: 0.2184 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:35:58.659\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2131 | Train score: 0.9387 | Val loss: 0.2185 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:00.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2037 | Train score: 0.9387 | Val loss: 0.2195 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:36:01.621\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2189 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:02.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2252 | Train score: 0.9387 | Val loss: 0.1934 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:04.319\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2074 | Train score: 0.9387 | Val loss: 0.1958 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:05.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2254 | Train score: 0.9387 | Val loss: 0.1981 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:07.225\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1919 | Train score: 0.9387 | Val loss: 0.1937 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:08.659\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1953 | Train score: 0.9387 | Val loss: 0.1889 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:10.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1762 | Train score: 0.9387 | Val loss: 0.1887 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:11.566\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1777 | Train score: 0.9387 | Val loss: 0.1951 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:13.001\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1812 | Train score: 0.9387 | Val loss: 0.2022 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:14.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1669 | Train score: 0.9448 | Val loss: 0.2086 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:15.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2191 | Train score: 0.9264 | Val loss: 0.2085 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:36:17.546\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2237 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:18.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2299 | Train score: 0.9387 | Val loss: 0.2208 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:20.274\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2279 | Train score: 0.9387 | Val loss: 0.2156 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:21.853\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2205 | Train score: 0.9387 | Val loss: 0.2077 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:23.384\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2119 | Train score: 0.9387 | Val loss: 0.2039 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:24.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2104 | Train score: 0.9387 | Val loss: 0.2030 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:26.450\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2012 | Train score: 0.9325 | Val loss: 0.2057 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:27.928\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2229 | Train score: 0.9387 | Val loss: 0.2042 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:29.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2220 | Train score: 0.9325 | Val loss: 0.2040 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:30.980\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1968 | Train score: 0.9387 | Val loss: 0.2084 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:32.498\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2413 | Train score: 0.9387 | Val loss: 0.2112 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:36:34.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2181 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:35.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2224 | Train score: 0.9387 | Val loss: 0.1954 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:36.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.1767 | Train score: 0.9387 | Val loss: 0.2016 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:38.337\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2218 | Train score: 0.9202 | Val loss: 0.2069 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:39.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2173 | Train score: 0.9325 | Val loss: 0.2043 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:41.163\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1647 | Train score: 0.9387 | Val loss: 0.2025 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:42.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1777 | Train score: 0.9387 | Val loss: 0.2005 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:43.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1875 | Train score: 0.9264 | Val loss: 0.1990 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:45.405\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1875 | Train score: 0.9448 | Val loss: 0.1981 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:46.824\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1714 | Train score: 0.9448 | Val loss: 0.1978 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:48.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1504 | Train score: 0.9448 | Val loss: 0.1985 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:36:49.859\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2244 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:51.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2306 | Train score: 0.9387 | Val loss: 0.2247 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:52.972\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2295 | Train score: 0.9387 | Val loss: 0.2225 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:54.549\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2260 | Train score: 0.9387 | Val loss: 0.2216 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:56.090\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2215 | Train score: 0.9387 | Val loss: 0.2211 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:57.645\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2073 | Train score: 0.9387 | Val loss: 0.2264 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:36:59.012\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2199 | Train score: 0.9387 | Val loss: 0.2276 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:37:00.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2060 | Train score: 0.9387 | Val loss: 0.2358 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:37:02.137\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2001 | Train score: 0.9448 | Val loss: 0.2461 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:37:03.720\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2398 | Train score: 0.9387 | Val loss: 0.2475 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:37:05.258\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1835 | Train score: 0.9387 | Val loss: 0.2505 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:37:06.866\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2249 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:37:08.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2305 | Train score: 0.9387 | Val loss: 0.2231 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:37:09.723\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2315 | Train score: 0.9387 | Val loss: 0.2217 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:37:11.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2282 | Train score: 0.9387 | Val loss: 0.2186 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:37:12.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2217 | Train score: 0.9387 | Val loss: 0.2141 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:37:14.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2140 | Train score: 0.9387 | Val loss: 0.2195 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:37:15.672\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2234 | Train score: 0.9387 | Val loss: 0.2317 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:37:17.099\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2036 | Train score: 0.9448 | Val loss: 0.2404 | Val score: 0.9113\u001b[0m\n",
      "\u001b[32m2024-10-20 16:37:18.535\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1972 | Train score: 0.9325 | Val loss: 0.2417 | Val score: 0.9163\u001b[0m\n",
      "\u001b[32m2024-10-20 16:37:19.973\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2766 | Train score: 0.9387 | Val loss: 0.2300 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:37:21.473\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2169 | Train score: 0.9387 | Val loss: 0.2240 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.935         0.008           0.290          0.228        0.129       0.116         0.557        0.058    0.177   0.154         0.011        0.001\n",
      "MedPFNClassifier                0.944         0.008           0.357          0.420        0.114       0.136         0.555        0.067    0.170   0.199         1.601        0.014\n",
      "RandomForestClassifier          0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.199        0.004\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.004        0.001\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.154        0.025\n",
      "TabForestPFNClassifier          0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000        15.784        0.545\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:37:53.177\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2247 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:37:54.642\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2308 | Train score: 0.9387 | Val loss: 0.2236 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:37:56.130\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2301 | Train score: 0.9387 | Val loss: 0.2204 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:37:57.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2291 | Train score: 0.9387 | Val loss: 0.2137 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:37:58.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2328 | Train score: 0.9387 | Val loss: 0.2155 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:00.480\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2268 | Train score: 0.9387 | Val loss: 0.2142 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:01.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2303 | Train score: 0.9387 | Val loss: 0.2124 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:03.449\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2277 | Train score: 0.9387 | Val loss: 0.2093 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:04.942\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2231 | Train score: 0.9387 | Val loss: 0.2049 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:06.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2179 | Train score: 0.9387 | Val loss: 0.2048 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:07.730\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2288 | Train score: 0.9325 | Val loss: 0.2046 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:38:09.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2247 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:10.696\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2306 | Train score: 0.9387 | Val loss: 0.2244 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:12.066\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2295 | Train score: 0.9387 | Val loss: 0.2242 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:13.450\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2263 | Train score: 0.9387 | Val loss: 0.2247 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:14.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2246 | Train score: 0.9387 | Val loss: 0.2277 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:16.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2144 | Train score: 0.9387 | Val loss: 0.2392 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:17.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2134 | Train score: 0.9387 | Val loss: 0.2532 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:18.990\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2032 | Train score: 0.9264 | Val loss: 0.2663 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:20.358\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2766 | Train score: 0.9325 | Val loss: 0.2502 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:21.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1849 | Train score: 0.9387 | Val loss: 0.2439 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:23.173\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2213 | Train score: 0.9387 | Val loss: 0.2386 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:38:24.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2042 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:26.180\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2202 | Train score: 0.9387 | Val loss: 0.1942 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:27.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2095 | Train score: 0.9387 | Val loss: 0.1801 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:29.199\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2074 | Train score: 0.9448 | Val loss: 0.1801 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:30.681\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1964 | Train score: 0.9387 | Val loss: 0.1836 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:32.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1788 | Train score: 0.9387 | Val loss: 0.1907 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:33.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2071 | Train score: 0.9448 | Val loss: 0.1963 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:34.932\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1488 | Train score: 0.9448 | Val loss: 0.2007 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:36.312\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1724 | Train score: 0.9264 | Val loss: 0.1997 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:37.657\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2134 | Train score: 0.9387 | Val loss: 0.1981 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:39.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1641 | Train score: 0.9387 | Val loss: 0.2002 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:38:40.823\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2244 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:42.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2304 | Train score: 0.9387 | Val loss: 0.2241 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:43.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2282 | Train score: 0.9387 | Val loss: 0.2221 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:45.059\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2247 | Train score: 0.9387 | Val loss: 0.2189 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:46.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2443 | Train score: 0.9387 | Val loss: 0.2181 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:47.959\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2152 | Train score: 0.9387 | Val loss: 0.2160 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:49.494\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2153 | Train score: 0.9387 | Val loss: 0.2128 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:50.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2229 | Train score: 0.9387 | Val loss: 0.2102 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:52.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2256 | Train score: 0.9387 | Val loss: 0.2096 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:53.823\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1896 | Train score: 0.9387 | Val loss: 0.2152 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:55.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2128 | Train score: 0.9387 | Val loss: 0.2079 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:38:56.853\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2184 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:58.204\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2277 | Train score: 0.9387 | Val loss: 0.1843 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:38:59.615\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2038 | Train score: 0.9387 | Val loss: 0.1631 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:00.983\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1740 | Train score: 0.9387 | Val loss: 0.1672 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:02.358\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2386 | Train score: 0.9264 | Val loss: 0.1764 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:03.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2521 | Train score: 0.9264 | Val loss: 0.1754 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:05.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2427 | Train score: 0.9202 | Val loss: 0.1702 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:06.478\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2085 | Train score: 0.9509 | Val loss: 0.1689 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:07.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2135 | Train score: 0.9387 | Val loss: 0.1680 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:09.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2260 | Train score: 0.9264 | Val loss: 0.1693 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:10.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1968 | Train score: 0.9387 | Val loss: 0.1698 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:39:12.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2245 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:13.758\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2306 | Train score: 0.9387 | Val loss: 0.2238 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:15.133\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2301 | Train score: 0.9387 | Val loss: 0.2231 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:16.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2290 | Train score: 0.9387 | Val loss: 0.2222 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:17.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2297 | Train score: 0.9387 | Val loss: 0.2213 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:19.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2294 | Train score: 0.9387 | Val loss: 0.2231 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:20.685\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2284 | Train score: 0.9387 | Val loss: 0.2211 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:22.048\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2275 | Train score: 0.9387 | Val loss: 0.2210 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:23.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2291 | Train score: 0.9387 | Val loss: 0.2199 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:24.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2284 | Train score: 0.9387 | Val loss: 0.2188 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:26.220\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2236 | Train score: 0.9387 | Val loss: 0.2168 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:39:27.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2247 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:29.352\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2312 | Train score: 0.9387 | Val loss: 0.2243 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:30.854\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2302 | Train score: 0.9387 | Val loss: 0.2232 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:32.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2315 | Train score: 0.9387 | Val loss: 0.2237 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:33.649\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2307 | Train score: 0.9387 | Val loss: 0.2232 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:35.129\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2304 | Train score: 0.9387 | Val loss: 0.2218 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:36.675\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2303 | Train score: 0.9387 | Val loss: 0.2197 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:38.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2262 | Train score: 0.9387 | Val loss: 0.2165 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:39.584\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2225 | Train score: 0.9387 | Val loss: 0.2204 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:41.083\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2228 | Train score: 0.9387 | Val loss: 0.2178 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:39:42.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2043 | Train score: 0.9387 | Val loss: 0.2148 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.928         0.007           0.071          0.113        0.029       0.045         0.507        0.022    0.041   0.065         0.011        0.001\n",
      "MedPFNClassifier                0.939         0.003           0.214          0.247        0.100       0.131         0.546        0.062    0.131   0.164         1.610        0.014\n",
      "RandomForestClassifier          0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.197        0.006\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.004        0.001\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.155        0.012\n",
      "TabForestPFNClassifier          0.939         0.004           0.036          0.087        0.014       0.035         0.506        0.014    0.020   0.050        15.568        0.347\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:40:14.285\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2241 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:15.629\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2307 | Train score: 0.9387 | Val loss: 0.2229 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:17.002\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2285 | Train score: 0.9387 | Val loss: 0.2192 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:18.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2250 | Train score: 0.9387 | Val loss: 0.2132 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:19.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2202 | Train score: 0.9387 | Val loss: 0.2113 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:21.100\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2134 | Train score: 0.9387 | Val loss: 0.2100 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:22.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2446 | Train score: 0.9264 | Val loss: 0.2108 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:23.904\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2045 | Train score: 0.9387 | Val loss: 0.2124 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:25.292\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2128 | Train score: 0.9387 | Val loss: 0.2140 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:26.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2396 | Train score: 0.9387 | Val loss: 0.2148 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:27.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2102 | Train score: 0.9387 | Val loss: 0.2149 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:40:29.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2248 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:31.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2309 | Train score: 0.9387 | Val loss: 0.2238 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:32.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2301 | Train score: 0.9387 | Val loss: 0.2233 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:33.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2296 | Train score: 0.9387 | Val loss: 0.2226 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:35.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2301 | Train score: 0.9387 | Val loss: 0.2214 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:36.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2227 | Train score: 0.9387 | Val loss: 0.2198 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:38.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2243 | Train score: 0.9387 | Val loss: 0.2184 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:39.494\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2288 | Train score: 0.9387 | Val loss: 0.2167 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:40.865\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2209 | Train score: 0.9387 | Val loss: 0.2160 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:42.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2163 | Train score: 0.9387 | Val loss: 0.2159 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:43.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2196 | Train score: 0.9387 | Val loss: 0.2151 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:40:45.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2187 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:46.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2276 | Train score: 0.9387 | Val loss: 0.1938 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:48.083\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2021 | Train score: 0.9387 | Val loss: 0.1927 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:49.555\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2075 | Train score: 0.9387 | Val loss: 0.2017 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:51.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2262 | Train score: 0.9264 | Val loss: 0.1991 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:52.390\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2194 | Train score: 0.9387 | Val loss: 0.1944 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:53.832\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1765 | Train score: 0.9387 | Val loss: 0.1960 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:55.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2157 | Train score: 0.9325 | Val loss: 0.1989 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:56.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1849 | Train score: 0.9387 | Val loss: 0.2027 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:58.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2027 | Train score: 0.9325 | Val loss: 0.2086 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:40:59.450\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1925 | Train score: 0.9387 | Val loss: 0.2111 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:41:01.046\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2241 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:02.374\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2299 | Train score: 0.9387 | Val loss: 0.2209 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:03.723\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2300 | Train score: 0.9387 | Val loss: 0.2185 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:05.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2230 | Train score: 0.9387 | Val loss: 0.2127 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:06.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2177 | Train score: 0.9387 | Val loss: 0.2067 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:07.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2018 | Train score: 0.9387 | Val loss: 0.2064 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:09.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2471 | Train score: 0.9264 | Val loss: 0.2077 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:10.436\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2112 | Train score: 0.9448 | Val loss: 0.2095 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:11.826\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2321 | Train score: 0.9387 | Val loss: 0.2111 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:13.175\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1898 | Train score: 0.9448 | Val loss: 0.2113 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:14.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2119 | Train score: 0.9325 | Val loss: 0.2129 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:41:16.115\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2234 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:17.455\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2271 | Train score: 0.9387 | Val loss: 0.2182 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:18.794\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2110 | Train score: 0.9387 | Val loss: 0.2158 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:20.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2063 | Train score: 0.9387 | Val loss: 0.2213 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:21.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2014 | Train score: 0.9387 | Val loss: 0.2281 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:22.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2178 | Train score: 0.9387 | Val loss: 0.2297 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:24.172\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1769 | Train score: 0.9448 | Val loss: 0.2313 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:25.535\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2277 | Train score: 0.9387 | Val loss: 0.2329 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:26.891\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1917 | Train score: 0.9448 | Val loss: 0.2313 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:28.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1747 | Train score: 0.9387 | Val loss: 0.2330 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:29.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1889 | Train score: 0.9448 | Val loss: 0.2291 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:41:31.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2245 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:32.495\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2305 | Train score: 0.9387 | Val loss: 0.2234 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:34.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2285 | Train score: 0.9387 | Val loss: 0.2219 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:35.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2262 | Train score: 0.9387 | Val loss: 0.2211 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:37.182\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2230 | Train score: 0.9387 | Val loss: 0.2214 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:38.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2431 | Train score: 0.9387 | Val loss: 0.2202 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:40.241\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2269 | Train score: 0.9387 | Val loss: 0.2195 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:41.570\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2236 | Train score: 0.9387 | Val loss: 0.2186 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:43.107\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2191 | Train score: 0.9387 | Val loss: 0.2177 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:44.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2293 | Train score: 0.9387 | Val loss: 0.2167 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:46.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2000 | Train score: 0.9387 | Val loss: 0.2175 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:41:47.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2243 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:49.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2303 | Train score: 0.9387 | Val loss: 0.2226 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:50.681\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2296 | Train score: 0.9387 | Val loss: 0.2184 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:52.147\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2268 | Train score: 0.9387 | Val loss: 0.2074 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:53.754\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2252 | Train score: 0.9387 | Val loss: 0.2018 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:55.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2466 | Train score: 0.9387 | Val loss: 0.2059 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:56.920\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2018 | Train score: 0.9387 | Val loss: 0.2045 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:41:58.410\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2323 | Train score: 0.9387 | Val loss: 0.2057 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:42:00.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2136 | Train score: 0.9387 | Val loss: 0.2041 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:42:01.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2189 | Train score: 0.9387 | Val loss: 0.2024 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:42:03.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2204 | Train score: 0.9387 | Val loss: 0.2015 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.929         0.011           0.207          0.174        0.071       0.045         0.527        0.020    0.101   0.066         0.011        0.001\n",
      "MedPFNClassifier                0.948         0.007           0.571          0.495        0.114       0.125         0.557        0.062    0.184   0.191         1.610        0.016\n",
      "RandomForestClassifier          0.940         0.002           0.000          0.000        0.000       0.000         0.500        0.001    0.000   0.000         0.193        0.005\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.004        0.001\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.157        0.018\n",
      "TabForestPFNClassifier          0.940         0.002           0.000          0.000        0.000       0.000         0.500        0.001    0.000   0.000        15.514        0.760\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:42:35.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2244 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:42:36.578\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2294 | Train score: 0.9387 | Val loss: 0.2197 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:42:37.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2264 | Train score: 0.9387 | Val loss: 0.2152 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:42:39.325\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2168 | Train score: 0.9387 | Val loss: 0.2120 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:42:40.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2159 | Train score: 0.9387 | Val loss: 0.2173 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:42:42.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2027 | Train score: 0.9387 | Val loss: 0.2348 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:42:43.416\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2565 | Train score: 0.9387 | Val loss: 0.2148 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:42:44.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2080 | Train score: 0.9387 | Val loss: 0.2069 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:42:46.090\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2011 | Train score: 0.9387 | Val loss: 0.2051 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:42:47.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2033 | Train score: 0.9387 | Val loss: 0.2040 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:42:48.965\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1900 | Train score: 0.9387 | Val loss: 0.2040 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:42:50.554\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2239 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:42:51.889\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2299 | Train score: 0.9387 | Val loss: 0.2200 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:42:53.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2278 | Train score: 0.9387 | Val loss: 0.2155 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:42:54.569\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2339 | Train score: 0.9387 | Val loss: 0.2149 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:42:55.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2187 | Train score: 0.9387 | Val loss: 0.2126 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:42:57.293\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2223 | Train score: 0.9387 | Val loss: 0.2109 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:42:58.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2205 | Train score: 0.9387 | Val loss: 0.2107 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:42:59.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1972 | Train score: 0.9387 | Val loss: 0.2101 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:01.335\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2314 | Train score: 0.9387 | Val loss: 0.2103 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:02.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1926 | Train score: 0.9387 | Val loss: 0.2112 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:04.016\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2218 | Train score: 0.9387 | Val loss: 0.2130 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:43:05.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2225 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:06.950\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2277 | Train score: 0.9387 | Val loss: 0.2203 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:08.279\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2045 | Train score: 0.9387 | Val loss: 0.2287 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:09.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1956 | Train score: 0.9448 | Val loss: 0.2377 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:10.991\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2024 | Train score: 0.9387 | Val loss: 0.2360 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:12.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2010 | Train score: 0.9387 | Val loss: 0.2350 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:13.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1964 | Train score: 0.9448 | Val loss: 0.2384 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:15.042\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1783 | Train score: 0.9387 | Val loss: 0.2414 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:16.429\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1914 | Train score: 0.9448 | Val loss: 0.2386 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:17.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1962 | Train score: 0.9264 | Val loss: 0.2303 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:19.106\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1937 | Train score: 0.9387 | Val loss: 0.2270 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:43:20.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2240 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:22.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2298 | Train score: 0.9387 | Val loss: 0.2227 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:23.397\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2275 | Train score: 0.9387 | Val loss: 0.2191 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:24.880\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2196 | Train score: 0.9387 | Val loss: 0.2183 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:26.211\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2208 | Train score: 0.9387 | Val loss: 0.2187 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:27.545\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2052 | Train score: 0.9387 | Val loss: 0.2232 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:29.010\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2115 | Train score: 0.9387 | Val loss: 0.2299 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:30.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1997 | Train score: 0.9387 | Val loss: 0.2389 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:31.701\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2132 | Train score: 0.9325 | Val loss: 0.2386 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:33.078\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1973 | Train score: 0.9264 | Val loss: 0.2388 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:34.437\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2078 | Train score: 0.9387 | Val loss: 0.2394 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:43:36.034\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2230 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:37.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2295 | Train score: 0.9387 | Val loss: 0.2085 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:38.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2126 | Train score: 0.9387 | Val loss: 0.2078 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:40.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.1897 | Train score: 0.9387 | Val loss: 0.2230 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:41.469\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2182 | Train score: 0.9325 | Val loss: 0.2368 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:42.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1789 | Train score: 0.9387 | Val loss: 0.2511 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:44.188\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2079 | Train score: 0.9387 | Val loss: 0.2537 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:45.579\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2041 | Train score: 0.9448 | Val loss: 0.2421 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:46.922\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1964 | Train score: 0.9387 | Val loss: 0.2360 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:48.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2421 | Train score: 0.9325 | Val loss: 0.2240 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:49.701\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2438 | Train score: 0.9325 | Val loss: 0.2163 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:43:51.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2246 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:52.584\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2303 | Train score: 0.9387 | Val loss: 0.2235 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:53.929\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2292 | Train score: 0.9387 | Val loss: 0.2224 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:55.277\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2301 | Train score: 0.9387 | Val loss: 0.2221 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:56.608\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2277 | Train score: 0.9387 | Val loss: 0.2216 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:57.965\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2243 | Train score: 0.9387 | Val loss: 0.2219 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:43:59.315\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2191 | Train score: 0.9387 | Val loss: 0.2250 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:44:00.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2395 | Train score: 0.9387 | Val loss: 0.2207 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:44:01.981\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2167 | Train score: 0.9387 | Val loss: 0.2186 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:44:03.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2213 | Train score: 0.9387 | Val loss: 0.2178 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:44:04.686\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2207 | Train score: 0.9387 | Val loss: 0.2175 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:44:06.278\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2243 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:44:07.607\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2304 | Train score: 0.9387 | Val loss: 0.2220 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:44:09.220\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2279 | Train score: 0.9387 | Val loss: 0.2166 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:44:10.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2272 | Train score: 0.9387 | Val loss: 0.2076 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:44:12.166\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2339 | Train score: 0.9387 | Val loss: 0.2117 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:44:13.689\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2188 | Train score: 0.9387 | Val loss: 0.2100 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:44:15.122\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2210 | Train score: 0.9387 | Val loss: 0.2070 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:44:16.677\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2207 | Train score: 0.9387 | Val loss: 0.2009 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:44:18.073\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2052 | Train score: 0.9387 | Val loss: 0.1943 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:44:19.791\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2001 | Train score: 0.9387 | Val loss: 0.1922 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:44:21.265\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1771 | Train score: 0.9387 | Val loss: 0.1985 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.926         0.010           0.167          0.148        0.086       0.083         0.533        0.038    0.107   0.097         0.011        0.001\n",
      "MedPFNClassifier                0.943         0.005           0.457          0.424        0.114       0.125         0.554        0.060    0.166   0.166         1.611        0.014\n",
      "RandomForestClassifier          0.938         0.006           0.000          0.000        0.000       0.000         0.499        0.003    0.000   0.000         0.189        0.003\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.004        0.001\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.183        0.047\n",
      "TabForestPFNClassifier          0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000        15.098        0.501\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:44:53.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2244 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:44:54.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2305 | Train score: 0.9387 | Val loss: 0.2221 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:44:56.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2286 | Train score: 0.9387 | Val loss: 0.2187 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:44:57.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2303 | Train score: 0.9387 | Val loss: 0.2168 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:44:58.976\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2267 | Train score: 0.9387 | Val loss: 0.2138 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:00.424\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2236 | Train score: 0.9387 | Val loss: 0.2091 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:01.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2164 | Train score: 0.9387 | Val loss: 0.2038 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:03.293\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2187 | Train score: 0.9387 | Val loss: 0.1983 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:04.718\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1894 | Train score: 0.9387 | Val loss: 0.1990 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:06.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1806 | Train score: 0.9387 | Val loss: 0.2039 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:07.574\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2305 | Train score: 0.9387 | Val loss: 0.2116 | Val score: 0.9261\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:45:09.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2239 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:10.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2304 | Train score: 0.9387 | Val loss: 0.2191 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:11.887\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2245 | Train score: 0.9387 | Val loss: 0.2148 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:13.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2261 | Train score: 0.9387 | Val loss: 0.2134 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:14.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2311 | Train score: 0.9387 | Val loss: 0.2137 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:16.002\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2299 | Train score: 0.9387 | Val loss: 0.2144 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:17.383\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2423 | Train score: 0.9387 | Val loss: 0.2160 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:18.918\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2157 | Train score: 0.9387 | Val loss: 0.2157 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:20.282\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2149 | Train score: 0.9387 | Val loss: 0.2144 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:21.728\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2269 | Train score: 0.9387 | Val loss: 0.2137 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:23.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2292 | Train score: 0.9387 | Val loss: 0.2137 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:45:24.790\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2243 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:26.110\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2283 | Train score: 0.9387 | Val loss: 0.2162 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:27.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2149 | Train score: 0.9387 | Val loss: 0.2233 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:28.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2163 | Train score: 0.9387 | Val loss: 0.2299 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:30.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2504 | Train score: 0.9325 | Val loss: 0.2296 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:31.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1792 | Train score: 0.9387 | Val loss: 0.2340 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:32.945\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2228 | Train score: 0.9387 | Val loss: 0.2347 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:34.333\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2196 | Train score: 0.9202 | Val loss: 0.2239 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:35.689\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2002 | Train score: 0.9387 | Val loss: 0.2192 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:37.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1861 | Train score: 0.9387 | Val loss: 0.2211 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:38.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1982 | Train score: 0.9387 | Val loss: 0.2225 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:45:40.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2242 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:41.753\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2305 | Train score: 0.9387 | Val loss: 0.2234 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:43.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2292 | Train score: 0.9387 | Val loss: 0.2223 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:45.093\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2218 | Train score: 0.9387 | Val loss: 0.2210 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:46.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2209 | Train score: 0.9387 | Val loss: 0.2211 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:48.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2265 | Train score: 0.9387 | Val loss: 0.2180 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:50.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2207 | Train score: 0.9387 | Val loss: 0.2153 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:51.530\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1887 | Train score: 0.9387 | Val loss: 0.2148 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:53.196\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2167 | Train score: 0.9387 | Val loss: 0.2172 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:54.666\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1992 | Train score: 0.9387 | Val loss: 0.2251 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:56.348\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2348 | Train score: 0.9387 | Val loss: 0.2288 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:45:57.932\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2209 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:45:59.281\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2272 | Train score: 0.9387 | Val loss: 0.2147 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:00.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2001 | Train score: 0.9387 | Val loss: 0.2174 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:02.030\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2202 | Train score: 0.9325 | Val loss: 0.2116 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:03.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1877 | Train score: 0.9387 | Val loss: 0.2132 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:04.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2126 | Train score: 0.9448 | Val loss: 0.2137 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:06.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2313 | Train score: 0.9325 | Val loss: 0.2152 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:07.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1802 | Train score: 0.9387 | Val loss: 0.2169 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:08.883\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2024 | Train score: 0.9387 | Val loss: 0.2186 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:10.266\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2056 | Train score: 0.9387 | Val loss: 0.2203 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:11.628\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1870 | Train score: 0.9387 | Val loss: 0.2223 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:46:13.251\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2240 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:14.831\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2296 | Train score: 0.9387 | Val loss: 0.2198 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:16.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2237 | Train score: 0.9387 | Val loss: 0.2171 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:17.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2208 | Train score: 0.9387 | Val loss: 0.2173 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:19.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2355 | Train score: 0.9387 | Val loss: 0.2176 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:20.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2303 | Train score: 0.9387 | Val loss: 0.2182 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:22.145\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2060 | Train score: 0.9387 | Val loss: 0.2211 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:23.555\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2089 | Train score: 0.9387 | Val loss: 0.2233 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:24.932\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2007 | Train score: 0.9387 | Val loss: 0.2244 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:26.329\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2042 | Train score: 0.9387 | Val loss: 0.2305 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:27.759\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1846 | Train score: 0.9387 | Val loss: 0.2429 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:46:29.372\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2239 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:30.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2299 | Train score: 0.9387 | Val loss: 0.2175 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:32.131\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2261 | Train score: 0.9387 | Val loss: 0.2104 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:33.514\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2205 | Train score: 0.9387 | Val loss: 0.2052 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:34.896\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2210 | Train score: 0.9387 | Val loss: 0.2048 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:36.271\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2036 | Train score: 0.9387 | Val loss: 0.2062 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:37.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2237 | Train score: 0.9387 | Val loss: 0.2033 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:39.045\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2336 | Train score: 0.9387 | Val loss: 0.1983 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:40.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2076 | Train score: 0.9387 | Val loss: 0.1949 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:41.851\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2099 | Train score: 0.9387 | Val loss: 0.1927 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:46:43.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2021 | Train score: 0.9387 | Val loss: 0.1927 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.932         0.008           0.286          0.117        0.086       0.035         0.535        0.021    0.132   0.054         0.010        0.002\n",
      "MedPFNClassifier                0.936         0.012           0.314          0.275        0.114       0.125         0.551        0.063    0.162   0.166         1.618        0.009\n",
      "RandomForestClassifier          0.940         0.002           0.000          0.000        0.000       0.000         0.500        0.001    0.000   0.000         0.190        0.004\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.004        0.001\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.171        0.015\n",
      "TabForestPFNClassifier          0.941         0.003           0.143          0.350        0.014       0.035         0.507        0.018    0.026   0.064        15.682        0.812\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:47:15.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2248 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:16.561\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2304 | Train score: 0.9387 | Val loss: 0.2235 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:17.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2298 | Train score: 0.9387 | Val loss: 0.2227 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:19.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2262 | Train score: 0.9387 | Val loss: 0.2225 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:20.578\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2252 | Train score: 0.9387 | Val loss: 0.2240 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:21.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2147 | Train score: 0.9387 | Val loss: 0.2270 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:23.267\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2222 | Train score: 0.9387 | Val loss: 0.2323 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:24.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2280 | Train score: 0.9387 | Val loss: 0.2300 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:25.988\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2249 | Train score: 0.9387 | Val loss: 0.2337 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:27.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2199 | Train score: 0.9387 | Val loss: 0.2332 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:28.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2284 | Train score: 0.9387 | Val loss: 0.2305 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:47:30.257\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2248 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:31.575\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2305 | Train score: 0.9387 | Val loss: 0.2241 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:32.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2284 | Train score: 0.9387 | Val loss: 0.2228 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:34.258\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2240 | Train score: 0.9387 | Val loss: 0.2236 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:35.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2116 | Train score: 0.9387 | Val loss: 0.2350 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:37.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2178 | Train score: 0.9387 | Val loss: 0.2361 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:38.466\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2063 | Train score: 0.9387 | Val loss: 0.2391 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:39.824\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1936 | Train score: 0.9387 | Val loss: 0.2433 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:41.154\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2310 | Train score: 0.9387 | Val loss: 0.2372 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:42.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2202 | Train score: 0.9387 | Val loss: 0.2317 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:43.881\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2096 | Train score: 0.9387 | Val loss: 0.2287 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:47:45.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2234 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:46.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2305 | Train score: 0.9387 | Val loss: 0.2214 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:48.514\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2285 | Train score: 0.9387 | Val loss: 0.2178 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:50.010\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2256 | Train score: 0.9387 | Val loss: 0.2118 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:51.359\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2196 | Train score: 0.9387 | Val loss: 0.2029 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:52.744\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2197 | Train score: 0.9387 | Val loss: 0.1972 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:54.249\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2023 | Train score: 0.9387 | Val loss: 0.1929 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:55.587\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2187 | Train score: 0.9387 | Val loss: 0.1924 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:57.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1974 | Train score: 0.9387 | Val loss: 0.1924 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:47:58.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2264 | Train score: 0.9387 | Val loss: 0.1932 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:00.140\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2102 | Train score: 0.9387 | Val loss: 0.1937 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:48:01.736\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2245 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:03.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2309 | Train score: 0.9387 | Val loss: 0.2260 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:04.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2279 | Train score: 0.9387 | Val loss: 0.2272 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:05.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2264 | Train score: 0.9387 | Val loss: 0.2308 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:07.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2201 | Train score: 0.9387 | Val loss: 0.2413 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:08.532\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2192 | Train score: 0.9387 | Val loss: 0.2533 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:09.867\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2181 | Train score: 0.9387 | Val loss: 0.2540 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:11.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2151 | Train score: 0.9387 | Val loss: 0.2489 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:12.659\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2182 | Train score: 0.9387 | Val loss: 0.2402 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:13.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2402 | Train score: 0.9387 | Val loss: 0.2335 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:15.389\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2038 | Train score: 0.9387 | Val loss: 0.2280 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:48:16.979\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2244 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:18.465\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2283 | Train score: 0.9387 | Val loss: 0.2206 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:19.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2223 | Train score: 0.9387 | Val loss: 0.2200 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:21.126\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2615 | Train score: 0.9387 | Val loss: 0.2205 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:22.594\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2082 | Train score: 0.9387 | Val loss: 0.2199 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:24.048\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2062 | Train score: 0.9387 | Val loss: 0.2191 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:25.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2027 | Train score: 0.9387 | Val loss: 0.2194 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:26.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2149 | Train score: 0.9387 | Val loss: 0.2202 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:28.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1968 | Train score: 0.9387 | Val loss: 0.2218 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:29.871\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2098 | Train score: 0.9387 | Val loss: 0.2227 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:31.241\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1914 | Train score: 0.9387 | Val loss: 0.2275 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:48:32.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2246 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:34.272\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2302 | Train score: 0.9387 | Val loss: 0.2217 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:35.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2287 | Train score: 0.9387 | Val loss: 0.2164 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:37.321\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2283 | Train score: 0.9387 | Val loss: 0.2128 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:38.744\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2324 | Train score: 0.9387 | Val loss: 0.2120 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:40.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2199 | Train score: 0.9387 | Val loss: 0.2090 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:41.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2266 | Train score: 0.9387 | Val loss: 0.2073 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:43.148\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2149 | Train score: 0.9387 | Val loss: 0.2051 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:44.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2191 | Train score: 0.9387 | Val loss: 0.2052 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:46.176\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2126 | Train score: 0.9387 | Val loss: 0.2050 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:47.615\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1970 | Train score: 0.9387 | Val loss: 0.2067 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:48:49.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2246 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:50.687\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2304 | Train score: 0.9387 | Val loss: 0.2223 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:52.061\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2267 | Train score: 0.9387 | Val loss: 0.2201 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:53.503\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2159 | Train score: 0.9387 | Val loss: 0.2186 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:54.882\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2268 | Train score: 0.9387 | Val loss: 0.2165 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:56.293\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2045 | Train score: 0.9387 | Val loss: 0.2162 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:57.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2408 | Train score: 0.9387 | Val loss: 0.2152 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:48:59.166\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2269 | Train score: 0.9387 | Val loss: 0.2159 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:49:00.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2262 | Train score: 0.9387 | Val loss: 0.2158 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:49:02.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2225 | Train score: 0.9387 | Val loss: 0.2165 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:49:03.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2231 | Train score: 0.9387 | Val loss: 0.2164 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.936         0.007           0.262          0.276        0.086       0.083         0.537        0.042    0.129   0.128         0.010        0.002\n",
      "MedPFNClassifier                0.939         0.003           0.122          0.194        0.086       0.136         0.539        0.062    0.101   0.159         1.620        0.013\n",
      "RandomForestClassifier          0.940         0.002           0.000          0.000        0.000       0.000         0.500        0.001    0.000   0.000         0.186        0.003\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.004        0.000\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.180        0.016\n",
      "TabForestPFNClassifier          0.940         0.002           0.000          0.000        0.000       0.000         0.500        0.001    0.000   0.000        15.403        0.497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:49:35.382\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2241 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:49:36.689\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2304 | Train score: 0.9387 | Val loss: 0.2210 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:49:38.282\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2238 | Train score: 0.9387 | Val loss: 0.2183 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:49:39.826\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2280 | Train score: 0.9387 | Val loss: 0.2175 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:49:41.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2090 | Train score: 0.9387 | Val loss: 0.2175 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:49:42.676\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2308 | Train score: 0.9387 | Val loss: 0.2154 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:49:44.194\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2158 | Train score: 0.9387 | Val loss: 0.2154 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:49:45.746\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2222 | Train score: 0.9387 | Val loss: 0.2156 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:49:47.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1942 | Train score: 0.9387 | Val loss: 0.2141 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:49:48.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2190 | Train score: 0.9387 | Val loss: 0.2135 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:49:50.345\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2059 | Train score: 0.9387 | Val loss: 0.2141 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:49:51.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2242 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:49:53.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2301 | Train score: 0.9387 | Val loss: 0.2220 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:49:54.673\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2270 | Train score: 0.9387 | Val loss: 0.2197 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:49:56.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2214 | Train score: 0.9387 | Val loss: 0.2181 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:49:57.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2193 | Train score: 0.9387 | Val loss: 0.2208 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:49:58.718\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2174 | Train score: 0.9387 | Val loss: 0.2278 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:00.140\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1989 | Train score: 0.9387 | Val loss: 0.2447 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:01.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2333 | Train score: 0.9325 | Val loss: 0.2462 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:02.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2293 | Train score: 0.9387 | Val loss: 0.2397 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:04.294\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2165 | Train score: 0.9387 | Val loss: 0.2365 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:05.663\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2201 | Train score: 0.9387 | Val loss: 0.2346 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:50:07.228\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2239 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:08.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2275 | Train score: 0.9387 | Val loss: 0.2187 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:10.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2184 | Train score: 0.9387 | Val loss: 0.2148 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:11.680\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2195 | Train score: 0.9387 | Val loss: 0.2125 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:13.216\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1948 | Train score: 0.9387 | Val loss: 0.2176 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:14.648\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2016 | Train score: 0.9325 | Val loss: 0.2249 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:16.121\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1596 | Train score: 0.9387 | Val loss: 0.2416 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:17.628\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2102 | Train score: 0.9387 | Val loss: 0.2414 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:19.099\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1980 | Train score: 0.9387 | Val loss: 0.2297 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:20.580\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2474 | Train score: 0.9202 | Val loss: 0.2199 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:21.938\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2336 | Train score: 0.9387 | Val loss: 0.2138 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:50:23.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2251 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:24.979\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2299 | Train score: 0.9387 | Val loss: 0.2235 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:26.561\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2289 | Train score: 0.9387 | Val loss: 0.2226 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:28.130\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2229 | Train score: 0.9387 | Val loss: 0.2231 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:29.675\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2407 | Train score: 0.9387 | Val loss: 0.2230 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:31.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2310 | Train score: 0.9387 | Val loss: 0.2227 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:32.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2224 | Train score: 0.9387 | Val loss: 0.2227 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:34.345\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2263 | Train score: 0.9387 | Val loss: 0.2230 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:35.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2247 | Train score: 0.9387 | Val loss: 0.2235 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:37.573\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2206 | Train score: 0.9387 | Val loss: 0.2257 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:38.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2205 | Train score: 0.9387 | Val loss: 0.2312 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:50:40.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2226 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:41.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2276 | Train score: 0.9387 | Val loss: 0.2149 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:43.236\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2205 | Train score: 0.9387 | Val loss: 0.2041 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:44.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2045 | Train score: 0.9387 | Val loss: 0.2034 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:45.946\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.1698 | Train score: 0.9387 | Val loss: 0.2182 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:47.287\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2024 | Train score: 0.9264 | Val loss: 0.2273 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:48.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2536 | Train score: 0.9325 | Val loss: 0.2251 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:50.040\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2245 | Train score: 0.9387 | Val loss: 0.2203 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:51.389\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1898 | Train score: 0.9264 | Val loss: 0.2140 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:52.749\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2038 | Train score: 0.9387 | Val loss: 0.2108 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:54.101\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1600 | Train score: 0.9387 | Val loss: 0.2128 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:50:55.702\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2248 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:57.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2310 | Train score: 0.9387 | Val loss: 0.2228 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:58.350\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2300 | Train score: 0.9387 | Val loss: 0.2208 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:50:59.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2294 | Train score: 0.9387 | Val loss: 0.2185 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:51:01.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2292 | Train score: 0.9387 | Val loss: 0.2168 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:51:02.397\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2245 | Train score: 0.9387 | Val loss: 0.2116 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:51:03.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2203 | Train score: 0.9387 | Val loss: 0.2053 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:51:05.078\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1980 | Train score: 0.9387 | Val loss: 0.2112 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:51:06.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2358 | Train score: 0.9325 | Val loss: 0.2114 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:51:07.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1860 | Train score: 0.9325 | Val loss: 0.2110 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:51:09.099\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1899 | Train score: 0.9387 | Val loss: 0.2136 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:51:10.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2244 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:51:12.013\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2301 | Train score: 0.9387 | Val loss: 0.2254 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:51:13.464\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2284 | Train score: 0.9387 | Val loss: 0.2273 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:51:14.889\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2207 | Train score: 0.9387 | Val loss: 0.2344 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:51:16.350\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2132 | Train score: 0.9387 | Val loss: 0.2386 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:51:17.841\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2103 | Train score: 0.9325 | Val loss: 0.2377 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:51:19.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2493 | Train score: 0.9325 | Val loss: 0.2333 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:51:20.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2140 | Train score: 0.9387 | Val loss: 0.2312 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:51:22.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1996 | Train score: 0.9387 | Val loss: 0.2307 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:51:23.596\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2288 | Train score: 0.9202 | Val loss: 0.2301 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:51:25.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2102 | Train score: 0.9387 | Val loss: 0.2311 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.935         0.010           0.190          0.301        0.057       0.090         0.524        0.046    0.088   0.139         0.011        0.001\n",
      "MedPFNClassifier                0.937         0.004           0.114          0.181        0.057       0.090         0.525        0.042    0.076   0.120         1.626        0.013\n",
      "RandomForestClassifier          0.937         0.004           0.000          0.000        0.000       0.000         0.498        0.002    0.000   0.000         0.183        0.004\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.004        0.001\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.181        0.016\n",
      "TabForestPFNClassifier          0.942         0.002           0.143          0.350        0.014       0.035         0.507        0.017    0.026   0.064        15.614        0.697\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:51:56.990\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2249 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:51:58.388\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2305 | Train score: 0.9387 | Val loss: 0.2231 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:51:59.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2283 | Train score: 0.9387 | Val loss: 0.2157 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:01.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2140 | Train score: 0.9387 | Val loss: 0.2242 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:02.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2131 | Train score: 0.9387 | Val loss: 0.2433 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:04.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2720 | Train score: 0.9387 | Val loss: 0.2192 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:05.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2266 | Train score: 0.9387 | Val loss: 0.2149 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:06.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2026 | Train score: 0.9387 | Val loss: 0.2140 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:08.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2065 | Train score: 0.9387 | Val loss: 0.2133 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:09.621\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2343 | Train score: 0.9387 | Val loss: 0.2127 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:10.986\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2260 | Train score: 0.9387 | Val loss: 0.2123 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:52:12.599\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2248 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:14.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2301 | Train score: 0.9387 | Val loss: 0.2232 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:15.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2307 | Train score: 0.9387 | Val loss: 0.2229 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:17.234\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2239 | Train score: 0.9387 | Val loss: 0.2223 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:18.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2296 | Train score: 0.9387 | Val loss: 0.2224 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:20.311\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2227 | Train score: 0.9387 | Val loss: 0.2244 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:21.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2259 | Train score: 0.9387 | Val loss: 0.2264 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:23.394\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2365 | Train score: 0.9387 | Val loss: 0.2261 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:25.083\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2179 | Train score: 0.9387 | Val loss: 0.2247 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:26.461\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2078 | Train score: 0.9387 | Val loss: 0.2274 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:27.988\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2185 | Train score: 0.9387 | Val loss: 0.2256 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:52:29.564\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2249 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:30.882\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2305 | Train score: 0.9387 | Val loss: 0.2220 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:32.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2298 | Train score: 0.9387 | Val loss: 0.2154 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:33.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2292 | Train score: 0.9387 | Val loss: 0.4328 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:34.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.4483 | Train score: 0.9387 | Val loss: 0.2074 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:36.322\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2198 | Train score: 0.9387 | Val loss: 0.2060 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:37.737\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2322 | Train score: 0.9387 | Val loss: 0.2052 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:39.149\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2212 | Train score: 0.9387 | Val loss: 0.2031 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:40.504\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2232 | Train score: 0.9387 | Val loss: 0.2001 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:41.861\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2264 | Train score: 0.9387 | Val loss: 0.1968 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:43.197\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2258 | Train score: 0.9387 | Val loss: 0.1932 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:52:44.798\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2249 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:46.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2312 | Train score: 0.9387 | Val loss: 0.2234 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:47.487\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2307 | Train score: 0.9387 | Val loss: 0.2226 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:48.859\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2307 | Train score: 0.9387 | Val loss: 0.2222 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:50.212\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2303 | Train score: 0.9387 | Val loss: 0.2218 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:51.603\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2308 | Train score: 0.9387 | Val loss: 0.2210 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:53.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2270 | Train score: 0.9387 | Val loss: 0.2172 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:54.402\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2252 | Train score: 0.9387 | Val loss: 0.2119 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:55.772\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2192 | Train score: 0.9387 | Val loss: 0.2100 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:57.115\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2227 | Train score: 0.9387 | Val loss: 0.2094 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:52:58.465\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2104 | Train score: 0.9387 | Val loss: 0.2113 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:53:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2246 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:01.371\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2310 | Train score: 0.9387 | Val loss: 0.2213 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:02.718\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2284 | Train score: 0.9387 | Val loss: 0.2167 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:04.036\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2224 | Train score: 0.9387 | Val loss: 0.2081 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:05.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2289 | Train score: 0.9387 | Val loss: 0.2023 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:06.745\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2007 | Train score: 0.9387 | Val loss: 0.1943 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:08.080\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2168 | Train score: 0.9387 | Val loss: 0.1911 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:09.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1847 | Train score: 0.9387 | Val loss: 0.1872 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:10.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1858 | Train score: 0.9387 | Val loss: 0.1908 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:12.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1527 | Train score: 0.9387 | Val loss: 0.1965 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:13.450\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2122 | Train score: 0.9264 | Val loss: 0.1959 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:53:15.046\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2238 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:16.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2299 | Train score: 0.9387 | Val loss: 0.2198 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:17.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2250 | Train score: 0.9387 | Val loss: 0.2152 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:19.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2243 | Train score: 0.9387 | Val loss: 0.2121 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:20.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2304 | Train score: 0.9387 | Val loss: 0.2104 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:21.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2235 | Train score: 0.9387 | Val loss: 0.2102 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:23.295\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2016 | Train score: 0.9387 | Val loss: 0.2098 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:24.624\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2160 | Train score: 0.9387 | Val loss: 0.2126 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:26.048\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2089 | Train score: 0.9387 | Val loss: 0.2158 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:27.396\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2108 | Train score: 0.9387 | Val loss: 0.2197 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:28.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2261 | Train score: 0.9387 | Val loss: 0.2176 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:53:30.337\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2251 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:31.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2305 | Train score: 0.9387 | Val loss: 0.2228 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:32.988\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2309 | Train score: 0.9387 | Val loss: 0.2214 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:34.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2261 | Train score: 0.9387 | Val loss: 0.2176 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:35.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2173 | Train score: 0.9387 | Val loss: 0.2270 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:36.996\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2326 | Train score: 0.9325 | Val loss: 0.2140 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:38.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2248 | Train score: 0.9448 | Val loss: 0.2090 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:39.745\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2040 | Train score: 0.9387 | Val loss: 0.2077 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:41.074\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2031 | Train score: 0.9387 | Val loss: 0.2092 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:42.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2223 | Train score: 0.9387 | Val loss: 0.2115 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:53:43.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2100 | Train score: 0.9387 | Val loss: 0.2113 | Val score: 0.9458\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.922         0.019           0.286          0.452        0.029       0.045         0.504        0.030    0.052   0.082         0.010        0.002\n",
      "MedPFNClassifier                0.937         0.003           0.122          0.194        0.086       0.136         0.538        0.063    0.101   0.159         1.629        0.024\n",
      "RandomForestClassifier          0.940         0.002           0.000          0.000        0.000       0.000         0.500        0.001    0.000   0.000         0.186        0.003\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.004        0.001\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.180        0.014\n",
      "TabForestPFNClassifier          0.934         0.014           0.026          0.064        0.029       0.070         0.510        0.025    0.027   0.067        15.191        0.638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:54:15.648\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2204 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:16.986\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2266 | Train score: 0.9387 | Val loss: 0.2138 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:18.367\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2169 | Train score: 0.9387 | Val loss: 0.2107 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:19.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2328 | Train score: 0.9387 | Val loss: 0.2056 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:21.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2079 | Train score: 0.9387 | Val loss: 0.2032 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:22.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2275 | Train score: 0.9387 | Val loss: 0.2024 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:23.980\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2147 | Train score: 0.9387 | Val loss: 0.2015 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:25.406\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1938 | Train score: 0.9387 | Val loss: 0.2002 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:26.830\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2183 | Train score: 0.9448 | Val loss: 0.2018 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:28.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2001 | Train score: 0.9387 | Val loss: 0.2036 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:29.676\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2086 | Train score: 0.9387 | Val loss: 0.2044 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:54:31.289\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2246 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:32.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2308 | Train score: 0.9387 | Val loss: 0.2220 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:33.990\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2302 | Train score: 0.9387 | Val loss: 0.2212 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:35.358\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2306 | Train score: 0.9387 | Val loss: 0.2209 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:36.745\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2282 | Train score: 0.9387 | Val loss: 0.2177 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:38.166\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2274 | Train score: 0.9387 | Val loss: 0.2089 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:39.666\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2190 | Train score: 0.9387 | Val loss: 0.1950 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:41.149\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2291 | Train score: 0.9387 | Val loss: 0.1956 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:42.705\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2334 | Train score: 0.9387 | Val loss: 0.2018 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:44.165\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2190 | Train score: 0.9387 | Val loss: 0.2054 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:45.546\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2241 | Train score: 0.9387 | Val loss: 0.2077 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:54:47.157\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2233 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:48.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2301 | Train score: 0.9387 | Val loss: 0.2191 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:50.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2260 | Train score: 0.9387 | Val loss: 0.2120 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:51.578\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2223 | Train score: 0.9387 | Val loss: 0.1975 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:53.125\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2141 | Train score: 0.9387 | Val loss: 0.1843 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:54.682\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2037 | Train score: 0.9387 | Val loss: 0.1757 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:56.239\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2460 | Train score: 0.9387 | Val loss: 0.1781 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:57.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2059 | Train score: 0.9387 | Val loss: 0.1793 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:54:59.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2558 | Train score: 0.9325 | Val loss: 0.1819 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:00.889\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1972 | Train score: 0.9387 | Val loss: 0.1823 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:02.400\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1993 | Train score: 0.9387 | Val loss: 0.1818 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:55:04.002\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2256 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:05.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2310 | Train score: 0.9387 | Val loss: 0.2247 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:06.833\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2281 | Train score: 0.9387 | Val loss: 0.2247 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:08.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2183 | Train score: 0.9387 | Val loss: 0.2283 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:09.746\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2453 | Train score: 0.9387 | Val loss: 0.2282 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:11.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2161 | Train score: 0.9387 | Val loss: 0.2272 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:12.738\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2082 | Train score: 0.9387 | Val loss: 0.2290 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:14.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2195 | Train score: 0.9387 | Val loss: 0.2285 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:15.754\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2107 | Train score: 0.9448 | Val loss: 0.2315 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:17.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2144 | Train score: 0.9325 | Val loss: 0.2461 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:18.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2529 | Train score: 0.9325 | Val loss: 0.2304 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:55:20.351\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2238 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:21.719\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2300 | Train score: 0.9387 | Val loss: 0.2198 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2307 | Train score: 0.9387 | Val loss: 0.2176 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:24.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2273 | Train score: 0.9387 | Val loss: 0.2127 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:25.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2218 | Train score: 0.9387 | Val loss: 0.2014 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:27.253\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2156 | Train score: 0.9387 | Val loss: 0.1870 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:28.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1828 | Train score: 0.9387 | Val loss: 0.1859 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:30.018\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1955 | Train score: 0.9387 | Val loss: 0.1839 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:31.419\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1833 | Train score: 0.9387 | Val loss: 0.1848 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:32.804\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2092 | Train score: 0.9387 | Val loss: 0.1830 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:34.219\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1884 | Train score: 0.9448 | Val loss: 0.1821 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:55:35.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2237 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:37.185\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2294 | Train score: 0.9387 | Val loss: 0.2178 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:38.612\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2276 | Train score: 0.9387 | Val loss: 0.2136 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:39.978\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2182 | Train score: 0.9387 | Val loss: 0.2088 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:41.367\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2065 | Train score: 0.9387 | Val loss: 0.2079 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:42.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1897 | Train score: 0.9387 | Val loss: 0.2186 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:44.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1936 | Train score: 0.9325 | Val loss: 0.2309 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:45.541\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2107 | Train score: 0.9325 | Val loss: 0.2255 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:46.893\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1833 | Train score: 0.9325 | Val loss: 0.2233 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:48.347\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2140 | Train score: 0.9448 | Val loss: 0.2188 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:49.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2554 | Train score: 0.9448 | Val loss: 0.2135 | Val score: 0.9261\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:55:51.344\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2245 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:52.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2300 | Train score: 0.9387 | Val loss: 0.2204 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:54.297\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2295 | Train score: 0.9387 | Val loss: 0.2164 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:55.680\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2244 | Train score: 0.9387 | Val loss: 0.2114 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:57.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2349 | Train score: 0.9387 | Val loss: 0.2121 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:58.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2213 | Train score: 0.9387 | Val loss: 0.2121 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:55:59.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2074 | Train score: 0.9387 | Val loss: 0.2092 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:56:01.267\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2134 | Train score: 0.9387 | Val loss: 0.2081 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:56:02.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2009 | Train score: 0.9387 | Val loss: 0.2060 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:56:04.094\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2220 | Train score: 0.9387 | Val loss: 0.2033 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:56:05.662\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2381 | Train score: 0.9387 | Val loss: 0.2071 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.934         0.011           0.071          0.175        0.014       0.035         0.503        0.019    0.024   0.058         0.010        0.002\n",
      "MedPFNClassifier                0.936         0.005           0.122          0.194        0.086       0.136         0.537        0.063    0.101   0.159         1.629        0.022\n",
      "RandomForestClassifier          0.940         0.002           0.000          0.000        0.000       0.000         0.500        0.001    0.000   0.000         0.184        0.004\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.004        0.001\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.181        0.015\n",
      "TabForestPFNClassifier          0.941         0.003           0.143          0.350        0.014       0.035         0.507        0.018    0.026   0.064        15.659        0.464\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:56:37.347\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2246 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:56:38.662\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2292 | Train score: 0.9387 | Val loss: 0.2251 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:56:40.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2215 | Train score: 0.9387 | Val loss: 0.2272 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:56:41.385\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2233 | Train score: 0.9387 | Val loss: 0.2270 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:56:42.728\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2025 | Train score: 0.9387 | Val loss: 0.2388 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:56:44.173\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2231 | Train score: 0.9387 | Val loss: 0.2434 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:56:45.558\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2135 | Train score: 0.9387 | Val loss: 0.2460 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:56:46.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2151 | Train score: 0.9387 | Val loss: 0.2483 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:56:48.298\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2140 | Train score: 0.9387 | Val loss: 0.2497 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:56:49.647\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2081 | Train score: 0.9387 | Val loss: 0.2472 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:56:50.975\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1837 | Train score: 0.9387 | Val loss: 0.2472 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:56:52.563\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2253 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:56:53.915\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2299 | Train score: 0.9387 | Val loss: 0.2269 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:56:55.281\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2251 | Train score: 0.9387 | Val loss: 0.2337 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:56:56.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2156 | Train score: 0.9387 | Val loss: 0.2437 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:56:58.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2196 | Train score: 0.9387 | Val loss: 0.2546 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:56:59.411\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1953 | Train score: 0.9387 | Val loss: 0.2721 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:00.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2288 | Train score: 0.9387 | Val loss: 0.2691 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:02.110\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1915 | Train score: 0.9387 | Val loss: 0.2720 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:03.560\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2076 | Train score: 0.9387 | Val loss: 0.2739 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:05.003\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1978 | Train score: 0.9387 | Val loss: 0.2758 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:06.461\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1843 | Train score: 0.9387 | Val loss: 0.2844 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:57:08.080\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2236 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:09.469\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2304 | Train score: 0.9387 | Val loss: 0.2359 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:10.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2316 | Train score: 0.9387 | Val loss: 0.2240 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:12.320\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2305 | Train score: 0.9387 | Val loss: 0.2242 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:13.723\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2302 | Train score: 0.9387 | Val loss: 0.2242 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:15.077\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2297 | Train score: 0.9387 | Val loss: 0.2239 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:16.480\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2292 | Train score: 0.9387 | Val loss: 0.2230 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:17.901\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2290 | Train score: 0.9387 | Val loss: 0.2217 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:19.307\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2241 | Train score: 0.9387 | Val loss: 0.2189 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:20.706\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2215 | Train score: 0.9387 | Val loss: 0.2143 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:22.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2148 | Train score: 0.9387 | Val loss: 0.2326 | Val score: 0.9310\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:57:23.716\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2246 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:25.083\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2303 | Train score: 0.9387 | Val loss: 0.2226 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:26.500\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2269 | Train score: 0.9387 | Val loss: 0.2204 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:27.918\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2154 | Train score: 0.9387 | Val loss: 0.2214 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:29.325\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2279 | Train score: 0.9387 | Val loss: 0.2220 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:30.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2120 | Train score: 0.9387 | Val loss: 0.2237 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:32.093\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2197 | Train score: 0.9387 | Val loss: 0.2251 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:33.461\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2380 | Train score: 0.9387 | Val loss: 0.2233 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:34.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2054 | Train score: 0.9387 | Val loss: 0.2226 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:36.351\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1992 | Train score: 0.9387 | Val loss: 0.2233 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:37.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2069 | Train score: 0.9387 | Val loss: 0.2247 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:57:39.445\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2241 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:40.801\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2301 | Train score: 0.9387 | Val loss: 0.2219 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:42.169\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2273 | Train score: 0.9387 | Val loss: 0.2142 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:43.560\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2315 | Train score: 0.9387 | Val loss: 0.2149 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:44.946\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2233 | Train score: 0.9387 | Val loss: 0.2126 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:46.352\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2189 | Train score: 0.9387 | Val loss: 0.2098 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:47.743\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2183 | Train score: 0.9387 | Val loss: 0.2066 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:49.110\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2194 | Train score: 0.9387 | Val loss: 0.2044 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:50.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2008 | Train score: 0.9387 | Val loss: 0.2051 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:51.895\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2216 | Train score: 0.9387 | Val loss: 0.2064 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:53.282\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1876 | Train score: 0.9387 | Val loss: 0.2116 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:57:54.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2258 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:56.261\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2303 | Train score: 0.9387 | Val loss: 0.2262 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:57.645\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2282 | Train score: 0.9387 | Val loss: 0.2290 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:57:59.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2299 | Train score: 0.9387 | Val loss: 0.2293 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:58:00.602\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2176 | Train score: 0.9387 | Val loss: 0.2341 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:58:02.003\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2221 | Train score: 0.9387 | Val loss: 0.2368 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:58:03.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2117 | Train score: 0.9387 | Val loss: 0.2397 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:58:04.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2176 | Train score: 0.9387 | Val loss: 0.2416 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:58:06.321\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2014 | Train score: 0.9387 | Val loss: 0.2448 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:58:07.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2096 | Train score: 0.9387 | Val loss: 0.2494 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:58:09.324\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2403 | Train score: 0.9387 | Val loss: 0.2416 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:58:10.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2242 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:58:12.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2298 | Train score: 0.9387 | Val loss: 0.2191 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:58:13.714\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2252 | Train score: 0.9387 | Val loss: 0.2145 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:58:15.129\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2119 | Train score: 0.9387 | Val loss: 0.2108 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:58:16.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2115 | Train score: 0.9387 | Val loss: 0.2117 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:58:17.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1903 | Train score: 0.9387 | Val loss: 0.2200 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 16:58:19.248\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2607 | Train score: 0.9264 | Val loss: 0.2168 | Val score: 0.9557\u001b[0m\n",
      "\u001b[32m2024-10-20 16:58:20.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1976 | Train score: 0.9264 | Val loss: 0.2180 | Val score: 0.9507\u001b[0m\n",
      "\u001b[32m2024-10-20 16:58:22.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2136 | Train score: 0.9448 | Val loss: 0.2161 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:58:23.499\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2254 | Train score: 0.9448 | Val loss: 0.2130 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 16:58:25.060\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2070 | Train score: 0.9387 | Val loss: 0.2101 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.931         0.006           0.071          0.113        0.029       0.045         0.508        0.021    0.041   0.065         0.010        0.002\n",
      "MedPFNClassifier                0.937         0.004           0.179          0.220        0.071       0.088         0.532        0.042    0.102   0.126         1.628        0.013\n",
      "RandomForestClassifier          0.940         0.002           0.000          0.000        0.000       0.000         0.500        0.001    0.000   0.000         0.183        0.002\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.004        0.000\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.159        0.014\n",
      "TabForestPFNClassifier          0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000        15.330        0.251\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:58:57.058\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2249 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:58:58.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2304 | Train score: 0.9387 | Val loss: 0.2224 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:58:59.753\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2315 | Train score: 0.9387 | Val loss: 0.2226 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:01.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2306 | Train score: 0.9387 | Val loss: 0.2225 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:02.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2279 | Train score: 0.9387 | Val loss: 0.2210 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:04.173\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2248 | Train score: 0.9387 | Val loss: 0.2193 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:05.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2228 | Train score: 0.9387 | Val loss: 0.2170 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:06.870\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2123 | Train score: 0.9387 | Val loss: 0.2156 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:08.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2137 | Train score: 0.9387 | Val loss: 0.2178 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:09.548\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2206 | Train score: 0.9387 | Val loss: 0.2180 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:10.882\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2027 | Train score: 0.9387 | Val loss: 0.2183 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:59:12.466\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2243 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:13.905\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2307 | Train score: 0.9387 | Val loss: 0.2231 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:15.352\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2318 | Train score: 0.9387 | Val loss: 0.2234 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:16.809\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2298 | Train score: 0.9387 | Val loss: 0.2231 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:18.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2298 | Train score: 0.9387 | Val loss: 0.2217 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:19.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2270 | Train score: 0.9387 | Val loss: 0.2177 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:21.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2234 | Train score: 0.9387 | Val loss: 0.2170 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:22.714\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2226 | Train score: 0.9387 | Val loss: 0.2168 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:24.234\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2201 | Train score: 0.9387 | Val loss: 0.2182 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:25.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2023 | Train score: 0.9387 | Val loss: 0.2316 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:27.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2500 | Train score: 0.9387 | Val loss: 0.2216 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:59:28.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2245 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:30.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2308 | Train score: 0.9387 | Val loss: 0.2265 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:31.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2297 | Train score: 0.9387 | Val loss: 0.2236 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:32.851\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2281 | Train score: 0.9387 | Val loss: 0.2235 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:34.175\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2272 | Train score: 0.9387 | Val loss: 0.2237 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:35.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2277 | Train score: 0.9387 | Val loss: 0.2248 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:37.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2186 | Train score: 0.9387 | Val loss: 0.2282 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:38.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2159 | Train score: 0.9387 | Val loss: 0.2317 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:39.851\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2164 | Train score: 0.9387 | Val loss: 0.2323 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:41.346\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2170 | Train score: 0.9387 | Val loss: 0.2266 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:42.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2315 | Train score: 0.9387 | Val loss: 0.2202 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:59:44.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2246 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:45.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2302 | Train score: 0.9387 | Val loss: 0.2244 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:47.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2304 | Train score: 0.9387 | Val loss: 0.2226 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:48.405\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2285 | Train score: 0.9387 | Val loss: 0.2213 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:49.756\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2312 | Train score: 0.9387 | Val loss: 0.2206 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:51.106\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2287 | Train score: 0.9387 | Val loss: 0.2182 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:52.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2286 | Train score: 0.9387 | Val loss: 0.2135 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:53.852\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2207 | Train score: 0.9387 | Val loss: 0.2064 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:55.226\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2287 | Train score: 0.9387 | Val loss: 0.2022 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:56.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2227 | Train score: 0.9387 | Val loss: 0.1994 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 16:59:58.074\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2086 | Train score: 0.9387 | Val loss: 0.1948 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 16:59:59.685\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2245 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:01.066\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2307 | Train score: 0.9387 | Val loss: 0.2236 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:02.436\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2299 | Train score: 0.9387 | Val loss: 0.2221 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:03.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2293 | Train score: 0.9387 | Val loss: 0.2213 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:05.357\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2252 | Train score: 0.9387 | Val loss: 0.2193 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:06.762\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2281 | Train score: 0.9387 | Val loss: 0.2165 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:08.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2245 | Train score: 0.9387 | Val loss: 0.2126 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:09.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2352 | Train score: 0.9387 | Val loss: 0.2127 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:10.987\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2290 | Train score: 0.9387 | Val loss: 0.2113 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:12.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2274 | Train score: 0.9387 | Val loss: 0.2114 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:13.881\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2245 | Train score: 0.9387 | Val loss: 0.2116 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:00:15.480\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2249 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:16.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2294 | Train score: 0.9387 | Val loss: 0.2232 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:18.360\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2276 | Train score: 0.9387 | Val loss: 0.2218 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:19.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2204 | Train score: 0.9387 | Val loss: 0.2230 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:21.276\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2111 | Train score: 0.9387 | Val loss: 0.2295 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:22.743\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2291 | Train score: 0.9387 | Val loss: 0.2319 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:24.175\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2072 | Train score: 0.9387 | Val loss: 0.2320 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:25.662\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2009 | Train score: 0.9387 | Val loss: 0.2345 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:27.174\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1980 | Train score: 0.9387 | Val loss: 0.2504 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:28.677\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2403 | Train score: 0.9387 | Val loss: 0.2471 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:30.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2282 | Train score: 0.9448 | Val loss: 0.2413 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:00:31.714\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2244 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:33.162\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2305 | Train score: 0.9387 | Val loss: 0.2219 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:34.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2302 | Train score: 0.9387 | Val loss: 0.2168 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:36.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2269 | Train score: 0.9387 | Val loss: 0.2097 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:37.592\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2239 | Train score: 0.9387 | Val loss: 0.2031 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:39.030\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2187 | Train score: 0.9387 | Val loss: 0.1977 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:40.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2345 | Train score: 0.9387 | Val loss: 0.1956 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:41.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2070 | Train score: 0.9387 | Val loss: 0.1926 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:43.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2391 | Train score: 0.9387 | Val loss: 0.1938 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:44.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2305 | Train score: 0.9387 | Val loss: 0.1956 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:00:46.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2173 | Train score: 0.9387 | Val loss: 0.1952 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.935         0.009           0.190          0.301        0.057       0.090         0.524        0.046    0.088   0.139         0.010        0.002\n",
      "MedPFNClassifier                0.930         0.008           0.048          0.117        0.014       0.035         0.501        0.018    0.022   0.054         1.634        0.014\n",
      "RandomForestClassifier          0.940         0.002           0.000          0.000        0.000       0.000         0.500        0.001    0.000   0.000         0.184        0.003\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.003        0.000\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.175        0.015\n",
      "TabForestPFNClassifier          0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000        15.563        0.396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:01:18.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2241 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:01:19.727\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2306 | Train score: 0.9387 | Val loss: 0.2225 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:01:21.059\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2292 | Train score: 0.9387 | Val loss: 0.2182 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:01:22.401\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2236 | Train score: 0.9387 | Val loss: 0.2119 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:01:23.745\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2174 | Train score: 0.9387 | Val loss: 0.2106 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:01:25.080\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1853 | Train score: 0.9387 | Val loss: 0.2199 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:01:26.410\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2028 | Train score: 0.9387 | Val loss: 0.2311 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 17:01:27.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2438 | Train score: 0.9325 | Val loss: 0.2304 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:01:29.264\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1594 | Train score: 0.9387 | Val loss: 0.2467 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:01:30.596\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1966 | Train score: 0.9387 | Val loss: 0.2257 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:01:31.922\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2188 | Train score: 0.9387 | Val loss: 0.2195 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:01:33.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2243 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:01:34.870\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2293 | Train score: 0.9387 | Val loss: 0.2206 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:01:36.226\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2270 | Train score: 0.9387 | Val loss: 0.2172 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:01:37.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2166 | Train score: 0.9387 | Val loss: 0.2186 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:01:39.148\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2242 | Train score: 0.9387 | Val loss: 0.2197 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:01:40.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1921 | Train score: 0.9387 | Val loss: 0.2333 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 17:01:41.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2134 | Train score: 0.9325 | Val loss: 0.2357 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 17:01:43.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1935 | Train score: 0.9387 | Val loss: 0.2291 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 17:01:44.644\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2119 | Train score: 0.9509 | Val loss: 0.2283 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 17:01:46.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1883 | Train score: 0.9509 | Val loss: 0.2297 | Val score: 0.9360\u001b[0m\n",
      "\u001b[32m2024-10-20 17:01:47.539\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2467 | Train score: 0.9202 | Val loss: 0.2289 | Val score: 0.9360\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:01:49.101\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2246 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:01:50.474\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2316 | Train score: 0.9387 | Val loss: 0.2244 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:01:51.846\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2310 | Train score: 0.9387 | Val loss: 0.2239 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:01:53.204\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2310 | Train score: 0.9387 | Val loss: 0.2235 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:01:54.572\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2300 | Train score: 0.9387 | Val loss: 0.2235 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:01:55.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2294 | Train score: 0.9387 | Val loss: 0.2228 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:01:57.300\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2266 | Train score: 0.9387 | Val loss: 0.2250 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:01:58.658\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2322 | Train score: 0.9387 | Val loss: 0.2220 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:00.034\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2245 | Train score: 0.9387 | Val loss: 0.2216 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:01.401\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2238 | Train score: 0.9387 | Val loss: 0.2206 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:02.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2198 | Train score: 0.9387 | Val loss: 0.2197 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:02:04.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2249 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:05.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2311 | Train score: 0.9387 | Val loss: 0.2235 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:07.276\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2291 | Train score: 0.9387 | Val loss: 0.2221 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:08.629\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2281 | Train score: 0.9387 | Val loss: 0.2200 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:10.187\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2232 | Train score: 0.9387 | Val loss: 0.2172 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:11.700\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2234 | Train score: 0.9387 | Val loss: 0.2161 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:13.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2283 | Train score: 0.9387 | Val loss: 0.2166 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:14.402\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1961 | Train score: 0.9387 | Val loss: 0.2195 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:15.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2246 | Train score: 0.9387 | Val loss: 0.2152 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:17.293\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2371 | Train score: 0.9387 | Val loss: 0.2129 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:18.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1994 | Train score: 0.9387 | Val loss: 0.2144 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:02:20.213\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2252 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:21.592\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2307 | Train score: 0.9387 | Val loss: 0.2242 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:22.965\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2301 | Train score: 0.9387 | Val loss: 0.2230 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:24.345\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2278 | Train score: 0.9387 | Val loss: 0.2219 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:25.728\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2304 | Train score: 0.9387 | Val loss: 0.2207 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:27.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2282 | Train score: 0.9387 | Val loss: 0.2177 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:28.504\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2274 | Train score: 0.9387 | Val loss: 0.2131 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:29.902\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2200 | Train score: 0.9387 | Val loss: 0.2057 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:31.253\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2137 | Train score: 0.9387 | Val loss: 0.2022 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:32.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2424 | Train score: 0.9387 | Val loss: 0.2015 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:33.994\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2130 | Train score: 0.9387 | Val loss: 0.2031 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:02:35.604\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2254 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:36.954\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2288 | Train score: 0.9387 | Val loss: 0.2263 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:38.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2287 | Train score: 0.9387 | Val loss: 0.2266 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:39.753\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2244 | Train score: 0.9387 | Val loss: 0.2286 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:41.101\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2048 | Train score: 0.9387 | Val loss: 0.2364 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:42.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2078 | Train score: 0.9387 | Val loss: 0.2445 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:43.917\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2049 | Train score: 0.9387 | Val loss: 0.2530 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:45.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1892 | Train score: 0.9387 | Val loss: 0.2635 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:46.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1988 | Train score: 0.9387 | Val loss: 0.2654 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:47.973\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1998 | Train score: 0.9387 | Val loss: 0.2542 | Val score: 0.9458\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:49.350\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2050 | Train score: 0.9264 | Val loss: 0.2439 | Val score: 0.9458\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:02:50.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2237 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:52.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2303 | Train score: 0.9387 | Val loss: 0.2212 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:54.007\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2308 | Train score: 0.9387 | Val loss: 0.2172 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:55.566\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2264 | Train score: 0.9387 | Val loss: 0.2100 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:57.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2317 | Train score: 0.9387 | Val loss: 0.2063 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:02:58.714\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2239 | Train score: 0.9387 | Val loss: 0.2018 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:03:00.264\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2156 | Train score: 0.9387 | Val loss: 0.1955 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:03:01.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2220 | Train score: 0.9387 | Val loss: 0.1916 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:03:03.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2206 | Train score: 0.9387 | Val loss: 0.1901 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:03:04.761\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2261 | Train score: 0.9387 | Val loss: 0.1918 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:03:06.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1984 | Train score: 0.9387 | Val loss: 0.1931 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.934         0.012           0.286          0.452        0.029       0.045         0.510        0.026    0.052   0.082         0.011        0.001\n",
      "MedPFNClassifier                0.935         0.008           0.048          0.117        0.014       0.035         0.504        0.017    0.022   0.054         1.623        0.009\n",
      "RandomForestClassifier          0.938         0.003           0.000          0.000        0.000       0.000         0.499        0.002    0.000   0.000         0.184        0.002\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.003        0.000\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.170        0.013\n",
      "TabForestPFNClassifier          0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000        15.360        0.490\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:03:38.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2240 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:03:39.429\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2303 | Train score: 0.9387 | Val loss: 0.2226 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:03:40.806\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2296 | Train score: 0.9387 | Val loss: 0.2215 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:03:42.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2283 | Train score: 0.9387 | Val loss: 0.2190 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:03:43.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2271 | Train score: 0.9387 | Val loss: 0.2160 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:03:45.239\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2322 | Train score: 0.9387 | Val loss: 0.2158 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:03:46.601\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2194 | Train score: 0.9387 | Val loss: 0.2142 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:03:48.145\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2209 | Train score: 0.9387 | Val loss: 0.2129 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:03:49.728\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2332 | Train score: 0.9387 | Val loss: 0.2130 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:03:51.066\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2039 | Train score: 0.9387 | Val loss: 0.2140 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:03:52.450\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2352 | Train score: 0.9387 | Val loss: 0.2145 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:03:54.030\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2247 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:03:55.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2299 | Train score: 0.9387 | Val loss: 0.2219 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:03:56.728\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2372 | Train score: 0.9387 | Val loss: 0.2227 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:03:58.058\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2298 | Train score: 0.9387 | Val loss: 0.2227 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:03:59.646\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2298 | Train score: 0.9387 | Val loss: 0.2226 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:01.320\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2314 | Train score: 0.9387 | Val loss: 0.2227 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:02.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2274 | Train score: 0.9387 | Val loss: 0.2225 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:04.028\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2292 | Train score: 0.9387 | Val loss: 0.2227 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:05.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2282 | Train score: 0.9387 | Val loss: 0.2219 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:06.994\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2304 | Train score: 0.9387 | Val loss: 0.2226 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:08.360\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2285 | Train score: 0.9387 | Val loss: 0.2215 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:04:09.946\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2243 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:11.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2303 | Train score: 0.9387 | Val loss: 0.2215 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:12.762\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2301 | Train score: 0.9387 | Val loss: 0.2197 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:14.174\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2300 | Train score: 0.9387 | Val loss: 0.2188 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:15.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2262 | Train score: 0.9387 | Val loss: 0.2150 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:16.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2174 | Train score: 0.9387 | Val loss: 0.2053 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:18.358\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2217 | Train score: 0.9387 | Val loss: 0.1981 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:19.693\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2198 | Train score: 0.9387 | Val loss: 0.2025 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:21.090\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2215 | Train score: 0.9387 | Val loss: 0.2046 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:22.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2077 | Train score: 0.9387 | Val loss: 0.2054 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:23.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2119 | Train score: 0.9387 | Val loss: 0.2064 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:04:25.449\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2248 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:26.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2306 | Train score: 0.9387 | Val loss: 0.2241 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:28.110\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2316 | Train score: 0.9387 | Val loss: 0.2231 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:29.457\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2300 | Train score: 0.9387 | Val loss: 0.2223 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:30.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2293 | Train score: 0.9387 | Val loss: 0.2190 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:32.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2267 | Train score: 0.9387 | Val loss: 0.2099 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:33.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2210 | Train score: 0.9387 | Val loss: 0.1997 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:34.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2329 | Train score: 0.9387 | Val loss: 0.1973 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:36.148\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2221 | Train score: 0.9387 | Val loss: 0.1941 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:37.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2265 | Train score: 0.9387 | Val loss: 0.1946 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:38.839\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2171 | Train score: 0.9387 | Val loss: 0.1948 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:04:40.425\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2243 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:41.951\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2295 | Train score: 0.9387 | Val loss: 0.2219 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:43.478\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2270 | Train score: 0.9387 | Val loss: 0.2200 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:44.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2200 | Train score: 0.9387 | Val loss: 0.2205 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:46.175\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2023 | Train score: 0.9387 | Val loss: 0.2268 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:47.546\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1931 | Train score: 0.9387 | Val loss: 0.2396 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:48.874\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1711 | Train score: 0.9387 | Val loss: 0.2667 | Val score: 0.9310\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:50.230\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1856 | Train score: 0.9448 | Val loss: 0.2831 | Val score: 0.9261\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:51.563\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1816 | Train score: 0.9448 | Val loss: 0.2915 | Val score: 0.9163\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:52.894\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1950 | Train score: 0.9448 | Val loss: 0.2911 | Val score: 0.9015\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:54.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1843 | Train score: 0.9325 | Val loss: 0.2787 | Val score: 0.9212\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:04:55.865\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2244 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:57.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2302 | Train score: 0.9387 | Val loss: 0.2206 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:04:58.702\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2293 | Train score: 0.9387 | Val loss: 0.2171 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:05:00.123\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2256 | Train score: 0.9387 | Val loss: 0.2120 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:05:01.504\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2208 | Train score: 0.9387 | Val loss: 0.2061 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:05:02.893\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2219 | Train score: 0.9387 | Val loss: 0.2031 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:05:04.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2049 | Train score: 0.9387 | Val loss: 0.2019 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:05:05.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2333 | Train score: 0.9387 | Val loss: 0.2036 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:05:06.994\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2065 | Train score: 0.9387 | Val loss: 0.2068 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:05:08.419\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1924 | Train score: 0.9387 | Val loss: 0.2092 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:05:09.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2038 | Train score: 0.9387 | Val loss: 0.2084 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:05:11.377\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2249 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:05:12.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2308 | Train score: 0.9387 | Val loss: 0.2252 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:05:14.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2313 | Train score: 0.9387 | Val loss: 0.2242 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:05:15.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2296 | Train score: 0.9387 | Val loss: 0.2240 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:05:16.869\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2299 | Train score: 0.9387 | Val loss: 0.2230 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:05:18.226\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2277 | Train score: 0.9387 | Val loss: 0.2185 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:05:19.573\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2181 | Train score: 0.9387 | Val loss: 0.2363 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:05:20.919\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2196 | Train score: 0.9387 | Val loss: 0.2366 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:05:22.332\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2163 | Train score: 0.9387 | Val loss: 0.2346 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:05:23.723\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2279 | Train score: 0.9387 | Val loss: 0.2250 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:05:25.079\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2045 | Train score: 0.9387 | Val loss: 0.2223 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.932         0.008           0.271          0.212        0.071       0.045         0.529        0.023    0.110   0.071         0.011        0.003\n",
      "MedPFNClassifier                0.926         0.009           0.041          0.065        0.029       0.045         0.505        0.016    0.034   0.053         1.626        0.011\n",
      "RandomForestClassifier          0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.186        0.003\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.003        0.000\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.183        0.016\n",
      "TabForestPFNClassifier          0.940         0.002           0.000          0.000        0.000       0.000         0.500        0.001    0.000   0.000        15.236        0.325\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:05:57.171\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2241 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:05:58.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2307 | Train score: 0.9387 | Val loss: 0.2227 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:00.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2293 | Train score: 0.9387 | Val loss: 0.2216 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:01.718\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2285 | Train score: 0.9387 | Val loss: 0.2189 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:03.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2276 | Train score: 0.9387 | Val loss: 0.2155 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:04.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2232 | Train score: 0.9387 | Val loss: 0.2126 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:06.171\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2172 | Train score: 0.9387 | Val loss: 0.2106 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:07.586\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2087 | Train score: 0.9387 | Val loss: 0.2124 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:09.195\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1941 | Train score: 0.9387 | Val loss: 0.2172 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:10.983\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2095 | Train score: 0.9387 | Val loss: 0.2218 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:12.573\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2136 | Train score: 0.9387 | Val loss: 0.2179 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:06:14.175\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2247 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:15.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2306 | Train score: 0.9387 | Val loss: 0.2237 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:17.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2292 | Train score: 0.9387 | Val loss: 0.2228 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:18.614\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2274 | Train score: 0.9387 | Val loss: 0.2211 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:19.988\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2287 | Train score: 0.9387 | Val loss: 0.2201 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:21.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2247 | Train score: 0.9387 | Val loss: 0.2166 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:22.942\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2227 | Train score: 0.9387 | Val loss: 0.2143 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:24.498\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.1935 | Train score: 0.9387 | Val loss: 0.2257 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:26.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2032 | Train score: 0.9448 | Val loss: 0.2443 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:27.578\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2316 | Train score: 0.9325 | Val loss: 0.2366 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:29.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2172 | Train score: 0.9387 | Val loss: 0.2359 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:06:30.699\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2247 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:32.185\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2294 | Train score: 0.9387 | Val loss: 0.2228 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:33.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2300 | Train score: 0.9387 | Val loss: 0.2232 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:35.076\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2257 | Train score: 0.9387 | Val loss: 0.2216 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:36.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2185 | Train score: 0.9387 | Val loss: 0.2216 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:37.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2208 | Train score: 0.9387 | Val loss: 0.2225 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:39.320\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2255 | Train score: 0.9387 | Val loss: 0.2229 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:40.693\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2029 | Train score: 0.9387 | Val loss: 0.2246 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:42.100\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2166 | Train score: 0.9387 | Val loss: 0.2248 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:43.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2020 | Train score: 0.9387 | Val loss: 0.2262 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:45.011\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1761 | Train score: 0.9387 | Val loss: 0.2311 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:06:46.611\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2248 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:47.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2302 | Train score: 0.9387 | Val loss: 0.2219 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:49.346\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2242 | Train score: 0.9387 | Val loss: 0.2178 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:50.718\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2172 | Train score: 0.9387 | Val loss: 0.2162 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:52.103\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2346 | Train score: 0.9387 | Val loss: 0.2153 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:53.470\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2164 | Train score: 0.9387 | Val loss: 0.2151 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:54.857\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2254 | Train score: 0.9387 | Val loss: 0.2154 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:56.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2133 | Train score: 0.9387 | Val loss: 0.2136 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:57.648\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2017 | Train score: 0.9387 | Val loss: 0.2144 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:06:59.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2106 | Train score: 0.9387 | Val loss: 0.2188 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:00.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2019 | Train score: 0.9387 | Val loss: 0.2237 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:07:02.214\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2246 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:03.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2307 | Train score: 0.9387 | Val loss: 0.2224 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:05.124\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2284 | Train score: 0.9387 | Val loss: 0.2192 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:06.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2265 | Train score: 0.9387 | Val loss: 0.2146 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:07.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2238 | Train score: 0.9387 | Val loss: 0.2090 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:09.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2178 | Train score: 0.9387 | Val loss: 0.2055 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:10.610\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2195 | Train score: 0.9387 | Val loss: 0.2066 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:12.177\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2216 | Train score: 0.9387 | Val loss: 0.2073 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:13.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2171 | Train score: 0.9387 | Val loss: 0.2083 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:15.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.1923 | Train score: 0.9387 | Val loss: 0.2069 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:16.430\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2078 | Train score: 0.9387 | Val loss: 0.2115 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:07:18.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2250 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:19.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2301 | Train score: 0.9387 | Val loss: 0.2236 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:20.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2336 | Train score: 0.9387 | Val loss: 0.2239 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:22.210\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2289 | Train score: 0.9387 | Val loss: 0.2234 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:23.615\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2274 | Train score: 0.9387 | Val loss: 0.2228 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:24.989\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2260 | Train score: 0.9387 | Val loss: 0.2218 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:26.360\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2239 | Train score: 0.9387 | Val loss: 0.2214 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:27.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2177 | Train score: 0.9387 | Val loss: 0.2206 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:29.129\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2146 | Train score: 0.9387 | Val loss: 0.2195 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:30.492\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2105 | Train score: 0.9387 | Val loss: 0.2212 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:31.856\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1916 | Train score: 0.9387 | Val loss: 0.2344 | Val score: 0.9261\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:07:33.469\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2248 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:34.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2307 | Train score: 0.9387 | Val loss: 0.2233 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:36.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2288 | Train score: 0.9387 | Val loss: 0.2211 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:37.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2311 | Train score: 0.9387 | Val loss: 0.2204 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:39.292\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2240 | Train score: 0.9387 | Val loss: 0.2185 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:40.672\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2326 | Train score: 0.9387 | Val loss: 0.2183 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:42.087\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2181 | Train score: 0.9387 | Val loss: 0.2173 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:43.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2218 | Train score: 0.9387 | Val loss: 0.2177 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:44.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.1992 | Train score: 0.9387 | Val loss: 0.2205 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:46.295\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2083 | Train score: 0.9387 | Val loss: 0.2254 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:07:47.736\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2080 | Train score: 0.9387 | Val loss: 0.2319 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.933         0.003           0.238          0.151        0.100       0.076         0.543        0.034    0.137   0.096         0.009        0.001\n",
      "MedPFNClassifier                0.927         0.007           0.048          0.075        0.029       0.045         0.506        0.018    0.036   0.056         1.677        0.090\n",
      "RandomForestClassifier          0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.184        0.004\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.003        0.001\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.164        0.017\n",
      "TabForestPFNClassifier          0.940         0.002           0.000          0.000        0.000       0.000         0.500        0.001    0.000   0.000        15.739        0.503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Using a Transformer with 6.48 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:08:19.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2242 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:08:20.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2297 | Train score: 0.9387 | Val loss: 0.2229 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:08:22.153\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2283 | Train score: 0.9387 | Val loss: 0.2221 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:08:23.507\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2269 | Train score: 0.9387 | Val loss: 0.2212 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:08:24.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2096 | Train score: 0.9387 | Val loss: 0.2227 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:08:26.181\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.1962 | Train score: 0.9387 | Val loss: 0.2292 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:08:27.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2368 | Train score: 0.9387 | Val loss: 0.2298 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:08:28.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2350 | Train score: 0.9387 | Val loss: 0.2257 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:08:30.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2462 | Train score: 0.9387 | Val loss: 0.2205 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:08:31.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2003 | Train score: 0.9387 | Val loss: 0.2188 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:08:32.962\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2056 | Train score: 0.9387 | Val loss: 0.2188 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:08:34.558\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2237 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:08:35.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2304 | Train score: 0.9387 | Val loss: 0.2217 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:08:37.213\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2307 | Train score: 0.9387 | Val loss: 0.2216 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:08:38.578\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2288 | Train score: 0.9387 | Val loss: 0.2202 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:08:39.945\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2280 | Train score: 0.9387 | Val loss: 0.2151 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:08:41.357\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2235 | Train score: 0.9387 | Val loss: 0.2051 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:08:42.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2078 | Train score: 0.9387 | Val loss: 0.2145 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:08:44.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2194 | Train score: 0.9264 | Val loss: 0.2117 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:08:45.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2416 | Train score: 0.9325 | Val loss: 0.2071 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:08:47.080\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2200 | Train score: 0.9387 | Val loss: 0.2085 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:08:48.507\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2255 | Train score: 0.9387 | Val loss: 0.2096 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:08:50.122\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2244 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:08:51.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2307 | Train score: 0.9387 | Val loss: 0.2235 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:08:52.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2309 | Train score: 0.9387 | Val loss: 0.2221 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:08:54.211\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2297 | Train score: 0.9387 | Val loss: 0.2201 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:08:55.586\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2286 | Train score: 0.9387 | Val loss: 0.2167 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:08:56.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2282 | Train score: 0.9387 | Val loss: 0.2115 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:08:58.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2239 | Train score: 0.9387 | Val loss: 0.2023 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:08:59.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2433 | Train score: 0.9387 | Val loss: 0.2054 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:01.348\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2264 | Train score: 0.9387 | Val loss: 0.2074 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:02.718\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2241 | Train score: 0.9387 | Val loss: 0.2072 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:04.080\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2236 | Train score: 0.9387 | Val loss: 0.2056 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:09:05.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2247 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:07.060\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2304 | Train score: 0.9387 | Val loss: 0.2214 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:08.418\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2282 | Train score: 0.9387 | Val loss: 0.2178 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:09.874\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2299 | Train score: 0.9387 | Val loss: 0.2165 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:11.225\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2226 | Train score: 0.9387 | Val loss: 0.2153 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:12.605\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2133 | Train score: 0.9387 | Val loss: 0.2187 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:13.974\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2439 | Train score: 0.9387 | Val loss: 0.2161 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:15.368\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2331 | Train score: 0.9387 | Val loss: 0.2153 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:16.825\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2120 | Train score: 0.9387 | Val loss: 0.2153 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:18.293\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2123 | Train score: 0.9387 | Val loss: 0.2147 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:19.652\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2106 | Train score: 0.9387 | Val loss: 0.2135 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:09:21.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2244 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:22.701\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2300 | Train score: 0.9387 | Val loss: 0.2218 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:24.054\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2279 | Train score: 0.9387 | Val loss: 0.2179 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:25.432\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2203 | Train score: 0.9387 | Val loss: 0.2132 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:26.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2052 | Train score: 0.9387 | Val loss: 0.2102 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:28.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2021 | Train score: 0.9387 | Val loss: 0.2099 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:29.630\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2560 | Train score: 0.9387 | Val loss: 0.2121 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:31.030\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2223 | Train score: 0.9387 | Val loss: 0.2110 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:32.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2096 | Train score: 0.9387 | Val loss: 0.2111 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:33.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2532 | Train score: 0.9387 | Val loss: 0.2107 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:35.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2063 | Train score: 0.9387 | Val loss: 0.2091 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:09:36.823\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2242 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:38.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2297 | Train score: 0.9387 | Val loss: 0.2222 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:39.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2282 | Train score: 0.9387 | Val loss: 0.2205 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:41.028\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2199 | Train score: 0.9387 | Val loss: 0.2180 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:42.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2275 | Train score: 0.9387 | Val loss: 0.2175 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:44.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2192 | Train score: 0.9387 | Val loss: 0.2184 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:45.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.1998 | Train score: 0.9387 | Val loss: 0.2269 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:47.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2603 | Train score: 0.9325 | Val loss: 0.2218 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:48.529\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2130 | Train score: 0.9387 | Val loss: 0.2195 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:49.923\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2082 | Train score: 0.9387 | Val loss: 0.2181 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:51.373\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.1953 | Train score: 0.9387 | Val loss: 0.2190 | Val score: 0.9409\u001b[0m\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenna\\Desktop\\MT\\TabPFN-medical\\tabularbench\\models\\foundation\\foundation_transformer.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path_to_weights))\n",
      "\u001b[32m2024-10-20 17:09:52.973\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEpoch 000 | Train loss: -.---- | Train score: -.---- | Val loss: 0.2250 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:54.478\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 001 | Train loss: 0.2320 | Train score: 0.9387 | Val loss: 0.2239 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:55.887\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 002 | Train loss: 0.2296 | Train score: 0.9387 | Val loss: 0.2236 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:57.387\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 003 | Train loss: 0.2304 | Train score: 0.9387 | Val loss: 0.2232 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:09:58.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 004 | Train loss: 0.2287 | Train score: 0.9387 | Val loss: 0.2215 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:10:00.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 005 | Train loss: 0.2299 | Train score: 0.9387 | Val loss: 0.2224 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:10:01.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 006 | Train loss: 0.2247 | Train score: 0.9387 | Val loss: 0.2202 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:10:03.561\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 007 | Train loss: 0.2323 | Train score: 0.9387 | Val loss: 0.2203 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:10:05.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 008 | Train loss: 0.2251 | Train score: 0.9387 | Val loss: 0.2190 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:10:06.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 009 | Train loss: 0.2258 | Train score: 0.9387 | Val loss: 0.2190 | Val score: 0.9409\u001b[0m\n",
      "\u001b[32m2024-10-20 17:10:08.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtabularbench.core.trainer_finetune\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEpoch 010 | Train loss: 0.2265 | Train score: 0.9387 | Val loss: 0.2154 | Val score: 0.9409\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " feature_select_shift_10step \n",
      "                         accuracy mean  accuracy std  precision mean  precision std  recall mean  recall std  roc_auc mean  roc_auc std  f1 mean  f1 std  runtime mean  runtime std\n",
      "XGBClassifier                   0.934         0.010           0.286          0.278        0.114       0.099         0.550        0.049    0.159   0.140         0.010        0.001\n",
      "MedPFNClassifier                0.926         0.013           0.107          0.169        0.043       0.049         0.512        0.020    0.056   0.066         1.615        0.013\n",
      "RandomForestClassifier          0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.183        0.006\n",
      "LogisticRegression              0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         0.004        0.001\n",
      "TabPFNClassifier                0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000         2.174        0.010\n",
      "TabForestPFNClassifier          0.941         0.000           0.000          0.000        0.000       0.000         0.500        0.000    0.000   0.000        15.486        0.523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\master4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "save = True\n",
    "sampling = None\n",
    "cv = 7\n",
    "best_delete = 0\n",
    "strat_split = True\n",
    "n_optim = 1000\n",
    "cat_optim = 10\n",
    "ft_epochs = 10\n",
    "ft_lr = 1e-7\n",
    "max_s = 1024\n",
    "max_q = 128\n",
    "max_samples = 1024\n",
    "no_pre_process = False\n",
    "multi_decoder = \"permutation\"\n",
    "N_ens = 7\n",
    "seed = 42\n",
    "overwrite = True\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\", \"f1\"]\n",
    "dir_path = os.path.abspath(os.getcwd())\n",
    "run_name = \"medium_net_mlp_balance_bsplit_baseline\" ## NO PERM, VERY ROBUST\n",
    "run_name = \"medium_mlp_var_balance_05weight_anova\" ### BEST!!!!!! ####\n",
    "#run_name = \"medium_mlp_balance_lownoisefixparam_anova\"\n",
    "#run_name = \"medium_mlp_0.5static_balance_anova\"\n",
    "#run_name = \"large_mlp_fullbalance\"\n",
    "#run_name = \"small_net_mlp_var_imbalance_05weight\" ## WORKS VERY WELL WITH PERM\n",
    "#run_name = \"small_net_mlp_varbalance_weight_batchunisplit\"\n",
    "#run_name = \"small_net_mlp_variable_balance_weight\"\n",
    "#run_name = \"small_net_mlp_balance_minevalup\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "filename = \"model\"\n",
    "models = [\n",
    "    #CatBoostOptim(n_optim=cat_optim),\n",
    "    XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "    MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "    RandomForestClassifier(),\n",
    "    #XGBoostOptim(n_optim=n_optim),\n",
    "    LogisticRegression(max_iter=1000), \n",
    "    TabPFNClassifier(device='cpu', N_ensemble_configurations=5, no_preprocess_mode=no_pre_process),\n",
    "    TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\", max_epochs=ft_epochs)\n",
    "]\n",
    "\n",
    "#for reducer in [AnovaSelect(), NonZeroSelect(), MeanSelect(), StdSelect(), MaxSelect(), PCASelect()]:\n",
    "reducer = AnovaSelect()\n",
    "#for reduce_data in [top_anova, top_non_zero, top_mean, top_std, top_max, pca_reduce]:\n",
    "    #data = reduce_data(all_data, labels, 100)\n",
    "    #print(all_data.shape)\n",
    "for best_delete in range(0,510,10):\n",
    "    #reducer.k = 100\n",
    "    #reducer = None\n",
    "    results_mean = pd.DataFrame(np.zeros((len(models), len(metrics)+1)),\n",
    "                                index=[m.__class__.__name__ for m in models],\n",
    "                                columns=metrics+[\"runtime\"])\n",
    "    results_std = pd.DataFrame(np.zeros((len(models), len(metrics)+1)), \n",
    "                               index=[m.__class__.__name__ for m in models],\n",
    "                               columns=metrics+[\"runtime\"])\n",
    "    \n",
    "    for ii, model in enumerate(models):\n",
    "        results_mean.iloc[ii,:], results_std.iloc[ii,:] = cross_validate_sample(\n",
    "            model, all_data, labels, metrics, strat_split, cv, sampling, \n",
    "            reducer, max_samples, seed=seed, overwrite=overwrite, n_best_delete=best_delete)\n",
    "\n",
    "    results_mean = results_mean.add_suffix(\" mean\")\n",
    "    results_std = results_std.add_suffix(\" std\")\n",
    "    results_full = pd.concat((results_mean, results_std), axis=1)\n",
    "    cols = results_full.columns.tolist()\n",
    "    new_cols = []\n",
    "    for i in range(int(len(cols)/2)):\n",
    "        new_cols.append(cols[i])\n",
    "        new_cols.append(cols[i+int(len(cols)/2)])\n",
    "    results_full = results_full[new_cols]\n",
    "    red_name = \"feature_select_shift_10step\"\n",
    "    if save:\n",
    "        directory = f\"results/{red_name}\"\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        save_path = f'results/{red_name}/{best_delete}_cv{cv}_mxsamp{max_samples}_sd{seed}_ovrw{int(overwrite)}'\n",
    "        results_full.to_csv(save_path)\n",
    "    print(\"\\n\", \"\\n\", red_name, \"\\n\", results_full.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbafc58-eff4-4b74-b585-52c0a8cf6234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1da53d28-57c8-48ba-a1e1-3080b58d3553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2400x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ws = 20\n",
    "rocs = []\n",
    "f1s = []\n",
    "for i in range(0,510,10):\n",
    "    results = pd.read_csv(f'results/feature_select_shift_10step/{i}_cv{7}_mxsamp{1024}_sd{42}_ovrw{1}')\n",
    "    rocs.append(results[\"roc_auc mean\"].values)\n",
    "    f1s.append(results[\"f1 mean\"].values)\n",
    "rocs = np.array(rocs)\n",
    "f1s = np.array(f1s)\n",
    "labels = results.iloc[:,0].values\n",
    "plt.figure(figsize=(12,3), dpi=200)\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "colors = []\n",
    "for cl in range(rocs.shape[1]):\n",
    "    ax1.plot(np.arange(rocs.shape[0]-ws+1)*10, moving_average(rocs[:,cl], ws), label=labels[cl])\n",
    "    ax2.plot(np.arange(rocs.shape[0]-ws+1)*10, moving_average(f1s[:,cl], ws), label=labels[cl])\n",
    "plt.suptitle(\"ROC AUC and F1-Score shifting feature selection\")\n",
    "#ax1.set_title(\"ROC AUC\")\n",
    "ax1.set_ylabel(\"ROC AUC\")\n",
    "#ax2.set_title(\"F1-Score\")\n",
    "ax2.set_xlabel(\"feature shift\")\n",
    "ax2.set_ylabel(\"F1\")\n",
    "ax1.legend(fontsize=7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b66f80d-b4bc-4c06-900c-038a3b16b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/data_all.csv\"\n",
    "all_data, labels = get_microbiome(path)\n",
    "all_data = remove_zero_features(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229c531a-d6f8-464b-bb73-476293131bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.count_nonzero(all_data, axis=0)\n",
    "counts = 1-counts/all_data.shape[0]\n",
    "means = np.mean(all_data, axis=0)\n",
    "plt.scatter(counts, means, s=1)\n",
    "plt.show()\n",
    "plt.scatter(counts, np.max(all_data, axis=0), s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216c8ae-5da2-4b93-bbde-e5bd0009e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3d2554-f847-48c5-9566-e04693fa8eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.abspath(os.getcwd())\n",
    "run_name = \"medium_net_mlp_balance_bsplit_baseline\"\n",
    "run_name = \"medium_net_mlp_balance_bsplit_baseline_longer\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "filename = \"model\"\n",
    "for sampling in [None]:#, undersample]:\n",
    "    cv = 5\n",
    "    strat_split = True\n",
    "    n_optim = 1000\n",
    "    cat_optim = 10\n",
    "    ft_epochs = 10\n",
    "    ft_lr = 1e-8\n",
    "    max_s = 1024\n",
    "    max_q = 128\n",
    "    max_samples = None\n",
    "    no_pre_process = False\n",
    "    multi_decoder = None\n",
    "    N_ens = 5\n",
    "    metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\"]\n",
    "    models = [\n",
    "        #CatBoostOptim(n_optim=cat_optim),\n",
    "        #MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder, ft_epochs=ft_epochs, ft_lr=ft_lr,\n",
    "        #                 max_s=max_s, max_q=max_q, no_preprocess_mode=no_pre_process),\n",
    "        MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=N_ens, multiclass_decoder=multi_decoder,  no_preprocess_mode=no_pre_process),\n",
    "        #MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=5, no_preprocess_mode=True),\n",
    "        XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "        XGBoostOptim(n_optim=n_optim),\n",
    "        LogisticRegression(max_iter=500), \n",
    "        TabPFNClassifier(device='cpu', N_ensemble_configurations=5, no_preprocess_mode=no_pre_process),\n",
    "        TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\", max_epochs=ft_epochs)\n",
    "    ]\n",
    "    results_mean = pd.DataFrame(np.zeros((len(models), len(metrics)+1)),\n",
    "                                index=[m.__class__.__name__ for m in models],\n",
    "                                columns=metrics+[\"runtime\"])\n",
    "    results_std = pd.DataFrame(np.zeros((len(models), len(metrics)+1)), \n",
    "                               index=[m.__class__.__name__ for m in models],\n",
    "                               columns=metrics+[\"runtime\"])\n",
    "    \n",
    "    for ii, model in enumerate(models):\n",
    "        results_mean.iloc[ii,:], results_std.iloc[ii,:] = cross_validate_sample(model, data, labels, metrics, strat_split, cv, sampling, max_samples)\n",
    "    #results_sorted = results.sort_values(\"roc_auc\")\n",
    "    #print(results_sorted)\n",
    "    print(results_mean)\n",
    "    print(results_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04052993-69b3-4c87-9bf8-163357aa9fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "for m in metrics + \"runtime\":\n",
    "    cols.append(m)\n",
    "    cols.append(m+\" std\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc23d5f-155a-4e3d-b696-1fd6439a0cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dir_path = os.path.abspath(os.getcwd())\n",
    "run_name = \"medium_net_mlp_balance_bsplit_baseline\"\n",
    "path = dir_path + f\"/logs/trainrun_{run_name}\"\n",
    "filename = \"model\"\n",
    "#model, config = load_model(path, filename, device=\"cpu\", eval_positions=None, verbose=0)\n",
    "#pred_model = TabPFNClassifier(model[2], config, device=\"cpu\", N_ensemble_configurations=5, no_preprocess_mode=False)\n",
    "for sampling in [None]:\n",
    "    cv = 3\n",
    "    strat_split = True\n",
    "    n_optim = 10\n",
    "    ft_epochs = 10\n",
    "    max_samples = None\n",
    "    metrics = metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\"]\n",
    "    models = [\n",
    "        RandomForestClassifier()\n",
    "        #CatBoostOptim(n_optim=n_optim),\n",
    "        #pred_model,\n",
    "        #MedPFNClassifier(base_path=path, filename=filename, device='cpu', N_ensemble_configurations=3, no_preprocess_mode=True),\n",
    "        #XGBClassifier(n_estimators=5, max_depth=5, learning_rate=1, objective='binary:logistic'),\n",
    "        #XGBoostOptim(n_optim=n_optim),\n",
    "        #LogisticRegression(max_iter=500), \n",
    "        #TabPFNClassifier(device='cpu', N_ensemble_configurations=3, no_preprocess_mode=True),\n",
    "        #TabForestPFNClassifier(\"saved_models/tabforest/mix600k/tabforestpfn.pt\", \"saved_models/tabforest/mix600k/config_run.yaml\", max_epochs=ft_epochs)\n",
    "    ]\n",
    "    results = pd.DataFrame(np.zeros((len(models), len(metrics)+1)), \n",
    "                           index=[m.__class__.__name__ for m in models],\n",
    "                          columns=metrics+[\"runtime\"])\n",
    "    \n",
    "    for ii, model in enumerate(models):\n",
    "        results.iloc[ii,:] = cross_validate_sample(model, data, labels, metrics, strat_split, cv, sampling, max_samples)\n",
    "    results_sorted = results.sort_values(\"roc_auc\")\n",
    "    \n",
    "    print(results_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5983e6a1-a7ce-4694-b775-e83498cd3be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819f6306-686d-42f1-829c-303890202785",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac970c40-6003-4c24-b454-014ba31f6b26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
